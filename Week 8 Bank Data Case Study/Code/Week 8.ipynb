{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Bank Data Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data\n",
    "\n",
    "In this section we read in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>C</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.989780</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>AU</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636365</td>\n",
       "      <td>2.857144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630686</td>\n",
       "      <td>7.464411</td>\n",
       "      <td>C</td>\n",
       "      <td>4.145098</td>\n",
       "      <td>9.191265</td>\n",
       "      <td>2.436402</td>\n",
       "      <td>2.483921</td>\n",
       "      <td>2.301630</td>\n",
       "      <td>...</td>\n",
       "      <td>6.822439</td>\n",
       "      <td>3.549938</td>\n",
       "      <td>0.598896</td>\n",
       "      <td>AF</td>\n",
       "      <td>1.672658</td>\n",
       "      <td>3.239542</td>\n",
       "      <td>1.957825</td>\n",
       "      <td>0</td>\n",
       "      <td>1.925763</td>\n",
       "      <td>1.739389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943877</td>\n",
       "      <td>5.310079</td>\n",
       "      <td>C</td>\n",
       "      <td>4.410969</td>\n",
       "      <td>5.326159</td>\n",
       "      <td>3.979592</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>...</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>2.477596</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>AE</td>\n",
       "      <td>1.773709</td>\n",
       "      <td>3.922193</td>\n",
       "      <td>1.120468</td>\n",
       "      <td>2</td>\n",
       "      <td>0.883118</td>\n",
       "      <td>1.176472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.797415</td>\n",
       "      <td>8.304757</td>\n",
       "      <td>C</td>\n",
       "      <td>4.225930</td>\n",
       "      <td>11.627438</td>\n",
       "      <td>2.097700</td>\n",
       "      <td>1.987549</td>\n",
       "      <td>0.171947</td>\n",
       "      <td>...</td>\n",
       "      <td>7.018256</td>\n",
       "      <td>1.812795</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>CJ</td>\n",
       "      <td>1.415230</td>\n",
       "      <td>2.954381</td>\n",
       "      <td>1.990847</td>\n",
       "      <td>1</td>\n",
       "      <td>1.677108</td>\n",
       "      <td>1.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630686</td>\n",
       "      <td>7.464411</td>\n",
       "      <td>C</td>\n",
       "      <td>4.145098</td>\n",
       "      <td>8.742359</td>\n",
       "      <td>2.436402</td>\n",
       "      <td>2.483921</td>\n",
       "      <td>1.496569</td>\n",
       "      <td>...</td>\n",
       "      <td>6.822439</td>\n",
       "      <td>3.549938</td>\n",
       "      <td>0.919812</td>\n",
       "      <td>Z</td>\n",
       "      <td>1.672658</td>\n",
       "      <td>3.239542</td>\n",
       "      <td>2.030373</td>\n",
       "      <td>0</td>\n",
       "      <td>1.925763</td>\n",
       "      <td>1.739389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target        v1        v2 v3        v4         v5        v6        v7  \\\n",
       "0   3       1  1.335739  8.727474  C  3.921026   7.915266  2.599278  3.176895   \n",
       "1   4       1  1.630686  7.464411  C  4.145098   9.191265  2.436402  2.483921   \n",
       "2   5       1  0.943877  5.310079  C  4.410969   5.326159  3.979592  3.928571   \n",
       "3   6       1  0.797415  8.304757  C  4.225930  11.627438  2.097700  1.987549   \n",
       "4   8       1  1.630686  7.464411  C  4.145098   8.742359  2.436402  2.483921   \n",
       "\n",
       "         v8  ...      v122      v123      v124  v125      v126      v127  \\\n",
       "0  0.012941  ...  8.000000  1.989780  0.035754    AU  1.804126  3.113719   \n",
       "1  2.301630  ...  6.822439  3.549938  0.598896    AF  1.672658  3.239542   \n",
       "2  0.019645  ...  9.333333  2.477596  0.013452    AE  1.773709  3.922193   \n",
       "3  0.171947  ...  7.018256  1.812795  0.002267    CJ  1.415230  2.954381   \n",
       "4  1.496569  ...  6.822439  3.549938  0.919812     Z  1.672658  3.239542   \n",
       "\n",
       "       v128  v129      v130      v131  \n",
       "0  2.024285     0  0.636365  2.857144  \n",
       "1  1.957825     0  1.925763  1.739389  \n",
       "2  1.120468     2  0.883118  1.176472  \n",
       "3  1.990847     1  1.677108  1.034483  \n",
       "4  2.030373     0  1.925763  1.739389  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../../case_8.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obvious issues like parsing errors or missings. Lets see what we have for data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114321 entries, 0 to 114320\n",
      "Data columns (total 133 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   ID      int64  \n",
      " 1   target  int64  \n",
      " 2   v1      float64\n",
      " 3   v2      float64\n",
      " 4   v3      object \n",
      " 5   v4      float64\n",
      " 6   v5      float64\n",
      " 7   v6      float64\n",
      " 8   v7      float64\n",
      " 9   v8      float64\n",
      " 10  v9      float64\n",
      " 11  v10     float64\n",
      " 12  v11     float64\n",
      " 13  v12     float64\n",
      " 14  v13     float64\n",
      " 15  v14     float64\n",
      " 16  v15     float64\n",
      " 17  v16     float64\n",
      " 18  v17     float64\n",
      " 19  v18     float64\n",
      " 20  v19     float64\n",
      " 21  v20     float64\n",
      " 22  v21     float64\n",
      " 23  v22     object \n",
      " 24  v23     float64\n",
      " 25  v24     object \n",
      " 26  v25     float64\n",
      " 27  v26     float64\n",
      " 28  v27     float64\n",
      " 29  v28     float64\n",
      " 30  v29     float64\n",
      " 31  v30     object \n",
      " 32  v31     object \n",
      " 33  v32     float64\n",
      " 34  v33     float64\n",
      " 35  v34     float64\n",
      " 36  v35     float64\n",
      " 37  v36     float64\n",
      " 38  v37     float64\n",
      " 39  v38     int64  \n",
      " 40  v39     float64\n",
      " 41  v40     float64\n",
      " 42  v41     float64\n",
      " 43  v42     float64\n",
      " 44  v43     float64\n",
      " 45  v44     float64\n",
      " 46  v45     float64\n",
      " 47  v46     float64\n",
      " 48  v47     object \n",
      " 49  v48     float64\n",
      " 50  v49     float64\n",
      " 51  v50     float64\n",
      " 52  v51     float64\n",
      " 53  v52     object \n",
      " 54  v53     float64\n",
      " 55  v54     float64\n",
      " 56  v55     float64\n",
      " 57  v56     object \n",
      " 58  v57     float64\n",
      " 59  v58     float64\n",
      " 60  v59     float64\n",
      " 61  v60     float64\n",
      " 62  v61     float64\n",
      " 63  v62     int64  \n",
      " 64  v63     float64\n",
      " 65  v64     float64\n",
      " 66  v65     float64\n",
      " 67  v66     object \n",
      " 68  v67     float64\n",
      " 69  v68     float64\n",
      " 70  v69     float64\n",
      " 71  v70     float64\n",
      " 72  v71     object \n",
      " 73  v72     int64  \n",
      " 74  v73     float64\n",
      " 75  v74     object \n",
      " 76  v75     object \n",
      " 77  v76     float64\n",
      " 78  v77     float64\n",
      " 79  v78     float64\n",
      " 80  v79     object \n",
      " 81  v80     float64\n",
      " 82  v81     float64\n",
      " 83  v82     float64\n",
      " 84  v83     float64\n",
      " 85  v84     float64\n",
      " 86  v85     float64\n",
      " 87  v86     float64\n",
      " 88  v87     float64\n",
      " 89  v88     float64\n",
      " 90  v89     float64\n",
      " 91  v90     float64\n",
      " 92  v91     object \n",
      " 93  v92     float64\n",
      " 94  v93     float64\n",
      " 95  v94     float64\n",
      " 96  v95     float64\n",
      " 97  v96     float64\n",
      " 98  v97     float64\n",
      " 99  v98     float64\n",
      " 100 v99     float64\n",
      " 101 v100    float64\n",
      " 102 v101    float64\n",
      " 103 v102    float64\n",
      " 104 v103    float64\n",
      " 105 v104    float64\n",
      " 106 v105    float64\n",
      " 107 v106    float64\n",
      " 108 v107    object \n",
      " 109 v108    float64\n",
      " 110 v109    float64\n",
      " 111 v110    object \n",
      " 112 v111    float64\n",
      " 113 v112    object \n",
      " 114 v113    object \n",
      " 115 v114    float64\n",
      " 116 v115    float64\n",
      " 117 v116    float64\n",
      " 118 v117    float64\n",
      " 119 v118    float64\n",
      " 120 v119    float64\n",
      " 121 v120    float64\n",
      " 122 v121    float64\n",
      " 123 v122    float64\n",
      " 124 v123    float64\n",
      " 125 v124    float64\n",
      " 126 v125    object \n",
      " 127 v126    float64\n",
      " 128 v127    float64\n",
      " 129 v128    float64\n",
      " 130 v129    int64  \n",
      " 131 v130    float64\n",
      " 132 v131    float64\n",
      "dtypes: float64(108), int64(6), object(19)\n",
      "memory usage: 116.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see mostly floats. There are some object data types we should probably recast. We have 114K observations, plenty to work with.  No variable names, as expected. He said there are no missings but lets check anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine. He told the truth. How about that target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    87021\n",
      "0    27300\n",
      "Name: target, dtype: int64\n",
      "0.2388\n"
     ]
    }
   ],
   "source": [
    "counts = df.target.value_counts()\n",
    "print(counts)\n",
    "print(round(counts[0]/sum(counts),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is binary and a little unbalanced, but not terrible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>v11</th>\n",
       "      <th>v12</th>\n",
       "      <th>v13</th>\n",
       "      <th>v14</th>\n",
       "      <th>v15</th>\n",
       "      <th>v16</th>\n",
       "      <th>v17</th>\n",
       "      <th>v18</th>\n",
       "      <th>v19</th>\n",
       "      <th>v20</th>\n",
       "      <th>v21</th>\n",
       "      <th>v23</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>v29</th>\n",
       "      <th>v32</th>\n",
       "      <th>v33</th>\n",
       "      <th>v34</th>\n",
       "      <th>v35</th>\n",
       "      <th>v36</th>\n",
       "      <th>v37</th>\n",
       "      <th>v38</th>\n",
       "      <th>v39</th>\n",
       "      <th>v40</th>\n",
       "      <th>v41</th>\n",
       "      <th>v42</th>\n",
       "      <th>v43</th>\n",
       "      <th>v44</th>\n",
       "      <th>v45</th>\n",
       "      <th>v46</th>\n",
       "      <th>v48</th>\n",
       "      <th>v49</th>\n",
       "      <th>v50</th>\n",
       "      <th>v51</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "      <th>v61</th>\n",
       "      <th>v62</th>\n",
       "      <th>v63</th>\n",
       "      <th>v64</th>\n",
       "      <th>v65</th>\n",
       "      <th>v67</th>\n",
       "      <th>v68</th>\n",
       "      <th>v69</th>\n",
       "      <th>v70</th>\n",
       "      <th>v72</th>\n",
       "      <th>v73</th>\n",
       "      <th>v76</th>\n",
       "      <th>v77</th>\n",
       "      <th>v78</th>\n",
       "      <th>v80</th>\n",
       "      <th>v81</th>\n",
       "      <th>v82</th>\n",
       "      <th>v83</th>\n",
       "      <th>v84</th>\n",
       "      <th>v85</th>\n",
       "      <th>v86</th>\n",
       "      <th>v87</th>\n",
       "      <th>v88</th>\n",
       "      <th>v89</th>\n",
       "      <th>v90</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "      <th>v100</th>\n",
       "      <th>v101</th>\n",
       "      <th>v102</th>\n",
       "      <th>v103</th>\n",
       "      <th>v104</th>\n",
       "      <th>v105</th>\n",
       "      <th>v106</th>\n",
       "      <th>v108</th>\n",
       "      <th>v109</th>\n",
       "      <th>v111</th>\n",
       "      <th>v114</th>\n",
       "      <th>v115</th>\n",
       "      <th>v116</th>\n",
       "      <th>v117</th>\n",
       "      <th>v118</th>\n",
       "      <th>v119</th>\n",
       "      <th>v120</th>\n",
       "      <th>v121</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "      <td>114321.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>114228.92823</td>\n",
       "      <td>0.76120</td>\n",
       "      <td>1.63069</td>\n",
       "      <td>7.46441</td>\n",
       "      <td>4.14510</td>\n",
       "      <td>8.74236</td>\n",
       "      <td>2.43640</td>\n",
       "      <td>2.48392</td>\n",
       "      <td>1.49657</td>\n",
       "      <td>9.03186</td>\n",
       "      <td>1.88305</td>\n",
       "      <td>15.44741</td>\n",
       "      <td>6.88130</td>\n",
       "      <td>3.79840</td>\n",
       "      <td>12.09428</td>\n",
       "      <td>2.08091</td>\n",
       "      <td>4.92322</td>\n",
       "      <td>3.83227</td>\n",
       "      <td>0.84105</td>\n",
       "      <td>0.22230</td>\n",
       "      <td>17.77359</td>\n",
       "      <td>7.02974</td>\n",
       "      <td>1.09309</td>\n",
       "      <td>1.69813</td>\n",
       "      <td>1.87603</td>\n",
       "      <td>2.74345</td>\n",
       "      <td>5.09333</td>\n",
       "      <td>8.20642</td>\n",
       "      <td>1.62215</td>\n",
       "      <td>2.16163</td>\n",
       "      <td>6.40624</td>\n",
       "      <td>8.12239</td>\n",
       "      <td>13.37560</td>\n",
       "      <td>0.74147</td>\n",
       "      <td>0.09093</td>\n",
       "      <td>1.23718</td>\n",
       "      <td>10.46593</td>\n",
       "      <td>7.18255</td>\n",
       "      <td>12.92497</td>\n",
       "      <td>2.21660</td>\n",
       "      <td>10.79517</td>\n",
       "      <td>9.14223</td>\n",
       "      <td>1.63053</td>\n",
       "      <td>12.53802</td>\n",
       "      <td>8.01655</td>\n",
       "      <td>1.50426</td>\n",
       "      <td>7.19816</td>\n",
       "      <td>15.71130</td>\n",
       "      <td>1.25386</td>\n",
       "      <td>1.55956</td>\n",
       "      <td>4.07783</td>\n",
       "      <td>7.70165</td>\n",
       "      <td>10.58794</td>\n",
       "      <td>1.71429</td>\n",
       "      <td>14.58303</td>\n",
       "      <td>1.03069</td>\n",
       "      <td>1.68733</td>\n",
       "      <td>6.34371</td>\n",
       "      <td>15.84756</td>\n",
       "      <td>9.28728</td>\n",
       "      <td>17.56412</td>\n",
       "      <td>9.44934</td>\n",
       "      <td>12.26996</td>\n",
       "      <td>1.43177</td>\n",
       "      <td>2.43330</td>\n",
       "      <td>2.40506</td>\n",
       "      <td>7.30737</td>\n",
       "      <td>13.33448</td>\n",
       "      <td>2.20970</td>\n",
       "      <td>7.28717</td>\n",
       "      <td>6.20836</td>\n",
       "      <td>2.17381</td>\n",
       "      <td>1.60796</td>\n",
       "      <td>2.82225</td>\n",
       "      <td>1.22018</td>\n",
       "      <td>10.18022</td>\n",
       "      <td>1.92418</td>\n",
       "      <td>1.51843</td>\n",
       "      <td>0.96691</td>\n",
       "      <td>0.58237</td>\n",
       "      <td>5.47518</td>\n",
       "      <td>3.85288</td>\n",
       "      <td>0.66576</td>\n",
       "      <td>6.45795</td>\n",
       "      <td>7.62255</td>\n",
       "      <td>7.66762</td>\n",
       "      <td>1.25072</td>\n",
       "      <td>12.09162</td>\n",
       "      <td>6.86641</td>\n",
       "      <td>2.89029</td>\n",
       "      <td>5.29672</td>\n",
       "      <td>2.64283</td>\n",
       "      <td>1.08105</td>\n",
       "      <td>11.79136</td>\n",
       "      <td>2.15262</td>\n",
       "      <td>4.18128</td>\n",
       "      <td>3.36531</td>\n",
       "      <td>13.57445</td>\n",
       "      <td>10.54805</td>\n",
       "      <td>2.29122</td>\n",
       "      <td>8.30386</td>\n",
       "      <td>8.36465</td>\n",
       "      <td>3.16897</td>\n",
       "      <td>1.29122</td>\n",
       "      <td>2.73760</td>\n",
       "      <td>6.82244</td>\n",
       "      <td>3.54994</td>\n",
       "      <td>0.91981</td>\n",
       "      <td>1.67266</td>\n",
       "      <td>3.23954</td>\n",
       "      <td>2.03037</td>\n",
       "      <td>0.31014</td>\n",
       "      <td>1.92576</td>\n",
       "      <td>1.73939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>65934.48736</td>\n",
       "      <td>0.42635</td>\n",
       "      <td>0.81326</td>\n",
       "      <td>2.22504</td>\n",
       "      <td>0.86266</td>\n",
       "      <td>1.54344</td>\n",
       "      <td>0.45061</td>\n",
       "      <td>0.44271</td>\n",
       "      <td>2.10979</td>\n",
       "      <td>1.44954</td>\n",
       "      <td>1.39347</td>\n",
       "      <td>0.59338</td>\n",
       "      <td>0.92415</td>\n",
       "      <td>0.88317</td>\n",
       "      <td>1.44392</td>\n",
       "      <td>0.55045</td>\n",
       "      <td>1.34464</td>\n",
       "      <td>1.43607</td>\n",
       "      <td>0.46286</td>\n",
       "      <td>0.12868</td>\n",
       "      <td>0.86743</td>\n",
       "      <td>1.06940</td>\n",
       "      <td>2.98732</td>\n",
       "      <td>2.24158</td>\n",
       "      <td>0.41398</td>\n",
       "      <td>0.62666</td>\n",
       "      <td>2.01131</td>\n",
       "      <td>0.96545</td>\n",
       "      <td>0.42324</td>\n",
       "      <td>0.73970</td>\n",
       "      <td>2.02420</td>\n",
       "      <td>1.00628</td>\n",
       "      <td>1.78573</td>\n",
       "      <td>0.40657</td>\n",
       "      <td>0.58348</td>\n",
       "      <td>1.77108</td>\n",
       "      <td>3.16764</td>\n",
       "      <td>0.75443</td>\n",
       "      <td>0.74880</td>\n",
       "      <td>0.48667</td>\n",
       "      <td>1.58586</td>\n",
       "      <td>1.55058</td>\n",
       "      <td>2.19532</td>\n",
       "      <td>1.64993</td>\n",
       "      <td>0.67797</td>\n",
       "      <td>1.16789</td>\n",
       "      <td>1.87306</td>\n",
       "      <td>0.60036</td>\n",
       "      <td>1.75460</td>\n",
       "      <td>0.62668</td>\n",
       "      <td>0.50925</td>\n",
       "      <td>5.13806</td>\n",
       "      <td>1.55640</td>\n",
       "      <td>0.40378</td>\n",
       "      <td>1.59344</td>\n",
       "      <td>0.69624</td>\n",
       "      <td>2.24951</td>\n",
       "      <td>1.89742</td>\n",
       "      <td>1.41050</td>\n",
       "      <td>0.84371</td>\n",
       "      <td>1.71983</td>\n",
       "      <td>1.42670</td>\n",
       "      <td>1.75436</td>\n",
       "      <td>0.92227</td>\n",
       "      <td>0.59981</td>\n",
       "      <td>1.03956</td>\n",
       "      <td>0.94339</td>\n",
       "      <td>1.38423</td>\n",
       "      <td>0.80726</td>\n",
       "      <td>1.68567</td>\n",
       "      <td>2.78821</td>\n",
       "      <td>0.79785</td>\n",
       "      <td>0.70691</td>\n",
       "      <td>1.06186</td>\n",
       "      <td>0.34985</td>\n",
       "      <td>2.27357</td>\n",
       "      <td>0.78753</td>\n",
       "      <td>2.13245</td>\n",
       "      <td>0.13438</td>\n",
       "      <td>0.18040</td>\n",
       "      <td>1.23201</td>\n",
       "      <td>0.64216</td>\n",
       "      <td>0.19835</td>\n",
       "      <td>0.84155</td>\n",
       "      <td>1.44498</td>\n",
       "      <td>1.76276</td>\n",
       "      <td>0.34655</td>\n",
       "      <td>5.17341</td>\n",
       "      <td>1.76901</td>\n",
       "      <td>1.35412</td>\n",
       "      <td>0.92291</td>\n",
       "      <td>0.66527</td>\n",
       "      <td>1.70317</td>\n",
       "      <td>2.21935</td>\n",
       "      <td>0.69222</td>\n",
       "      <td>2.81395</td>\n",
       "      <td>1.11715</td>\n",
       "      <td>2.61288</td>\n",
       "      <td>1.42744</td>\n",
       "      <td>0.50340</td>\n",
       "      <td>2.74269</td>\n",
       "      <td>1.50358</td>\n",
       "      <td>3.16360</td>\n",
       "      <td>0.55455</td>\n",
       "      <td>1.01860</td>\n",
       "      <td>1.34870</td>\n",
       "      <td>1.94343</td>\n",
       "      <td>1.59155</td>\n",
       "      <td>0.37791</td>\n",
       "      <td>1.22123</td>\n",
       "      <td>0.81434</td>\n",
       "      <td>0.69326</td>\n",
       "      <td>0.94964</td>\n",
       "      <td>0.85182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>1.51678</td>\n",
       "      <td>0.10618</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.04104</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.06935</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.01306</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05306</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.65930</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>1.50136</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.42709</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.87240</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.02237</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.01914</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>-0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57280.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.34615</td>\n",
       "      <td>6.57577</td>\n",
       "      <td>4.06870</td>\n",
       "      <td>8.39409</td>\n",
       "      <td>2.34097</td>\n",
       "      <td>2.37659</td>\n",
       "      <td>0.26531</td>\n",
       "      <td>8.81356</td>\n",
       "      <td>1.05033</td>\n",
       "      <td>15.39823</td>\n",
       "      <td>6.32262</td>\n",
       "      <td>3.46409</td>\n",
       "      <td>11.25602</td>\n",
       "      <td>1.90569</td>\n",
       "      <td>4.70588</td>\n",
       "      <td>3.37983</td>\n",
       "      <td>0.71949</td>\n",
       "      <td>0.19241</td>\n",
       "      <td>17.77359</td>\n",
       "      <td>6.41875</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.27448</td>\n",
       "      <td>1.75553</td>\n",
       "      <td>2.56647</td>\n",
       "      <td>4.74236</td>\n",
       "      <td>8.11437</td>\n",
       "      <td>1.49129</td>\n",
       "      <td>1.83074</td>\n",
       "      <td>5.05580</td>\n",
       "      <td>7.89615</td>\n",
       "      <td>13.09935</td>\n",
       "      <td>0.59072</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.30522</td>\n",
       "      <td>8.41039</td>\n",
       "      <td>7.06762</td>\n",
       "      <td>12.81317</td>\n",
       "      <td>2.06897</td>\n",
       "      <td>10.54256</td>\n",
       "      <td>8.88634</td>\n",
       "      <td>0.25630</td>\n",
       "      <td>12.15693</td>\n",
       "      <td>7.91400</td>\n",
       "      <td>0.65879</td>\n",
       "      <td>6.83727</td>\n",
       "      <td>15.66772</td>\n",
       "      <td>0.20819</td>\n",
       "      <td>1.27660</td>\n",
       "      <td>3.97665</td>\n",
       "      <td>4.06136</td>\n",
       "      <td>10.21680</td>\n",
       "      <td>1.59960</td>\n",
       "      <td>14.58303</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.27094</td>\n",
       "      <td>5.85294</td>\n",
       "      <td>15.84756</td>\n",
       "      <td>9.16798</td>\n",
       "      <td>17.56412</td>\n",
       "      <td>9.27007</td>\n",
       "      <td>12.08748</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.23612</td>\n",
       "      <td>2.06003</td>\n",
       "      <td>7.19023</td>\n",
       "      <td>13.15175</td>\n",
       "      <td>1.95122</td>\n",
       "      <td>7.20499</td>\n",
       "      <td>3.59298</td>\n",
       "      <td>1.81535</td>\n",
       "      <td>1.31804</td>\n",
       "      <td>2.44395</td>\n",
       "      <td>1.10841</td>\n",
       "      <td>9.55184</td>\n",
       "      <td>1.62351</td>\n",
       "      <td>0.22454</td>\n",
       "      <td>0.94962</td>\n",
       "      <td>0.51793</td>\n",
       "      <td>5.13587</td>\n",
       "      <td>3.65008</td>\n",
       "      <td>0.59461</td>\n",
       "      <td>6.36225</td>\n",
       "      <td>7.18954</td>\n",
       "      <td>7.29994</td>\n",
       "      <td>1.17271</td>\n",
       "      <td>12.09162</td>\n",
       "      <td>6.34055</td>\n",
       "      <td>2.32905</td>\n",
       "      <td>4.98815</td>\n",
       "      <td>2.43202</td>\n",
       "      <td>0.17355</td>\n",
       "      <td>11.70203</td>\n",
       "      <td>1.85201</td>\n",
       "      <td>2.72122</td>\n",
       "      <td>2.92386</td>\n",
       "      <td>11.99667</td>\n",
       "      <td>10.26667</td>\n",
       "      <td>2.13904</td>\n",
       "      <td>7.60400</td>\n",
       "      <td>7.86517</td>\n",
       "      <td>1.16943</td>\n",
       "      <td>1.05263</td>\n",
       "      <td>2.28261</td>\n",
       "      <td>6.51961</td>\n",
       "      <td>2.57105</td>\n",
       "      <td>0.08471</td>\n",
       "      <td>1.57097</td>\n",
       "      <td>2.76250</td>\n",
       "      <td>1.68126</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.44948</td>\n",
       "      <td>1.46341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>114189.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.63069</td>\n",
       "      <td>7.46441</td>\n",
       "      <td>4.14510</td>\n",
       "      <td>8.74236</td>\n",
       "      <td>2.43640</td>\n",
       "      <td>2.48392</td>\n",
       "      <td>1.49657</td>\n",
       "      <td>9.03186</td>\n",
       "      <td>1.31291</td>\n",
       "      <td>15.44741</td>\n",
       "      <td>6.61324</td>\n",
       "      <td>3.79840</td>\n",
       "      <td>11.96783</td>\n",
       "      <td>2.08091</td>\n",
       "      <td>4.92322</td>\n",
       "      <td>3.83227</td>\n",
       "      <td>0.84105</td>\n",
       "      <td>0.22230</td>\n",
       "      <td>17.77359</td>\n",
       "      <td>7.03937</td>\n",
       "      <td>0.33059</td>\n",
       "      <td>1.69813</td>\n",
       "      <td>1.87603</td>\n",
       "      <td>2.74345</td>\n",
       "      <td>5.09333</td>\n",
       "      <td>8.20642</td>\n",
       "      <td>1.62215</td>\n",
       "      <td>2.16163</td>\n",
       "      <td>6.53443</td>\n",
       "      <td>8.12239</td>\n",
       "      <td>13.37560</td>\n",
       "      <td>0.74147</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.23718</td>\n",
       "      <td>10.33934</td>\n",
       "      <td>7.18255</td>\n",
       "      <td>12.92497</td>\n",
       "      <td>2.21660</td>\n",
       "      <td>10.79517</td>\n",
       "      <td>9.14223</td>\n",
       "      <td>1.63053</td>\n",
       "      <td>12.53802</td>\n",
       "      <td>8.01655</td>\n",
       "      <td>1.21194</td>\n",
       "      <td>7.19816</td>\n",
       "      <td>15.71130</td>\n",
       "      <td>1.25386</td>\n",
       "      <td>1.55956</td>\n",
       "      <td>4.07783</td>\n",
       "      <td>7.70165</td>\n",
       "      <td>10.58794</td>\n",
       "      <td>1.71429</td>\n",
       "      <td>14.58303</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.68733</td>\n",
       "      <td>6.34371</td>\n",
       "      <td>15.84756</td>\n",
       "      <td>9.28728</td>\n",
       "      <td>17.56412</td>\n",
       "      <td>9.44934</td>\n",
       "      <td>12.26996</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.43330</td>\n",
       "      <td>2.40506</td>\n",
       "      <td>7.30737</td>\n",
       "      <td>13.33448</td>\n",
       "      <td>2.20970</td>\n",
       "      <td>7.28717</td>\n",
       "      <td>6.20836</td>\n",
       "      <td>2.17381</td>\n",
       "      <td>1.60796</td>\n",
       "      <td>2.82225</td>\n",
       "      <td>1.22018</td>\n",
       "      <td>10.18022</td>\n",
       "      <td>1.92418</td>\n",
       "      <td>1.51843</td>\n",
       "      <td>0.96691</td>\n",
       "      <td>0.58237</td>\n",
       "      <td>5.47518</td>\n",
       "      <td>3.85288</td>\n",
       "      <td>0.66576</td>\n",
       "      <td>6.45795</td>\n",
       "      <td>7.62255</td>\n",
       "      <td>7.66762</td>\n",
       "      <td>1.25072</td>\n",
       "      <td>12.09162</td>\n",
       "      <td>6.86641</td>\n",
       "      <td>2.89029</td>\n",
       "      <td>5.29672</td>\n",
       "      <td>2.64283</td>\n",
       "      <td>1.08105</td>\n",
       "      <td>11.79136</td>\n",
       "      <td>2.15262</td>\n",
       "      <td>4.18128</td>\n",
       "      <td>3.36531</td>\n",
       "      <td>14.03888</td>\n",
       "      <td>10.54805</td>\n",
       "      <td>2.29122</td>\n",
       "      <td>8.30386</td>\n",
       "      <td>8.36465</td>\n",
       "      <td>3.16897</td>\n",
       "      <td>1.29122</td>\n",
       "      <td>2.73760</td>\n",
       "      <td>6.82244</td>\n",
       "      <td>3.54994</td>\n",
       "      <td>0.91981</td>\n",
       "      <td>1.67266</td>\n",
       "      <td>3.23954</td>\n",
       "      <td>2.03037</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.92576</td>\n",
       "      <td>1.73939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>171206.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.63069</td>\n",
       "      <td>7.55150</td>\n",
       "      <td>4.34023</td>\n",
       "      <td>8.92480</td>\n",
       "      <td>2.48470</td>\n",
       "      <td>2.52845</td>\n",
       "      <td>1.49657</td>\n",
       "      <td>9.30233</td>\n",
       "      <td>2.10066</td>\n",
       "      <td>15.59390</td>\n",
       "      <td>7.01940</td>\n",
       "      <td>3.79840</td>\n",
       "      <td>12.71577</td>\n",
       "      <td>2.08091</td>\n",
       "      <td>5.14286</td>\n",
       "      <td>3.83227</td>\n",
       "      <td>0.84105</td>\n",
       "      <td>0.22230</td>\n",
       "      <td>18.15460</td>\n",
       "      <td>7.66652</td>\n",
       "      <td>1.09309</td>\n",
       "      <td>1.69813</td>\n",
       "      <td>1.89891</td>\n",
       "      <td>2.77910</td>\n",
       "      <td>5.33034</td>\n",
       "      <td>8.47939</td>\n",
       "      <td>1.62215</td>\n",
       "      <td>2.16163</td>\n",
       "      <td>7.70145</td>\n",
       "      <td>8.25076</td>\n",
       "      <td>14.32492</td>\n",
       "      <td>0.74147</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.23718</td>\n",
       "      <td>12.76246</td>\n",
       "      <td>7.34477</td>\n",
       "      <td>13.04965</td>\n",
       "      <td>2.23749</td>\n",
       "      <td>11.02210</td>\n",
       "      <td>9.41516</td>\n",
       "      <td>1.63053</td>\n",
       "      <td>12.67463</td>\n",
       "      <td>8.13559</td>\n",
       "      <td>2.00572</td>\n",
       "      <td>7.41788</td>\n",
       "      <td>15.87156</td>\n",
       "      <td>1.25386</td>\n",
       "      <td>1.55956</td>\n",
       "      <td>4.15366</td>\n",
       "      <td>7.70165</td>\n",
       "      <td>10.83954</td>\n",
       "      <td>1.73502</td>\n",
       "      <td>15.31291</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.68733</td>\n",
       "      <td>6.38440</td>\n",
       "      <td>16.47085</td>\n",
       "      <td>9.46899</td>\n",
       "      <td>18.43750</td>\n",
       "      <td>9.73384</td>\n",
       "      <td>12.91660</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.43665</td>\n",
       "      <td>2.40506</td>\n",
       "      <td>7.55221</td>\n",
       "      <td>13.55932</td>\n",
       "      <td>2.24359</td>\n",
       "      <td>7.82301</td>\n",
       "      <td>6.20836</td>\n",
       "      <td>2.17381</td>\n",
       "      <td>1.60796</td>\n",
       "      <td>2.82225</td>\n",
       "      <td>1.22018</td>\n",
       "      <td>10.43359</td>\n",
       "      <td>1.92418</td>\n",
       "      <td>1.51843</td>\n",
       "      <td>0.99010</td>\n",
       "      <td>0.58237</td>\n",
       "      <td>5.47518</td>\n",
       "      <td>3.85288</td>\n",
       "      <td>0.66576</td>\n",
       "      <td>6.66900</td>\n",
       "      <td>7.71084</td>\n",
       "      <td>8.00612</td>\n",
       "      <td>1.30167</td>\n",
       "      <td>15.69721</td>\n",
       "      <td>6.93119</td>\n",
       "      <td>2.89029</td>\n",
       "      <td>5.29672</td>\n",
       "      <td>2.64283</td>\n",
       "      <td>1.08105</td>\n",
       "      <td>12.44363</td>\n",
       "      <td>2.15262</td>\n",
       "      <td>4.18128</td>\n",
       "      <td>3.36531</td>\n",
       "      <td>15.37219</td>\n",
       "      <td>10.71895</td>\n",
       "      <td>2.31017</td>\n",
       "      <td>8.64537</td>\n",
       "      <td>8.41772</td>\n",
       "      <td>3.16897</td>\n",
       "      <td>1.29122</td>\n",
       "      <td>2.73760</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>3.54994</td>\n",
       "      <td>0.91981</td>\n",
       "      <td>1.67266</td>\n",
       "      <td>3.23954</td>\n",
       "      <td>2.03037</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.92576</td>\n",
       "      <td>1.73939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>228713.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>18.53392</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>18.71055</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>19.29605</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>19.84819</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>17.56098</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>19.91553</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>19.83168</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>18.84696</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>19.81631</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>15.97351</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>17.56098</td>\n",
       "      <td>19.84275</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>6.30577</td>\n",
       "      <td>8.92384</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>19.01631</td>\n",
       "      <td>9.07054</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>19.05880</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>18.77525</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>10.39427</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>19.68607</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>15.63161</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID       target           v1           v2           v4  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean  114228.92823      0.76120      1.63069      7.46441      4.14510   \n",
       "std    65934.48736      0.42635      0.81326      2.22504      0.86266   \n",
       "min        3.00000      0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%    57280.00000      1.00000      1.34615      6.57577      4.06870   \n",
       "50%   114189.00000      1.00000      1.63069      7.46441      4.14510   \n",
       "75%   171206.00000      1.00000      1.63069      7.55150      4.34023   \n",
       "max   228713.00000      1.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "                v5           v6           v7           v8           v9  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       8.74236      2.43640      2.48392      1.49657      9.03186   \n",
       "std        1.54344      0.45061      0.44271      2.10979      1.44954   \n",
       "min       -0.00000     -0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        8.39409      2.34097      2.37659      0.26531      8.81356   \n",
       "50%        8.74236      2.43640      2.48392      1.49657      9.03186   \n",
       "75%        8.92480      2.48470      2.52845      1.49657      9.30233   \n",
       "max       20.00000     20.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "               v10          v11          v12          v13          v14  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       1.88305     15.44741      6.88130      3.79840     12.09428   \n",
       "std        1.39347      0.59338      0.92415      0.88317      1.44392   \n",
       "min       -0.00000     -0.00000      0.00000     -0.00000     -0.00000   \n",
       "25%        1.05033     15.39823      6.32262      3.46409     11.25602   \n",
       "50%        1.31291     15.44741      6.61324      3.79840     11.96783   \n",
       "75%        2.10066     15.59390      7.01940      3.79840     12.71577   \n",
       "max       18.53392     20.00000     18.71055     20.00000     20.00000   \n",
       "\n",
       "               v15          v16          v17          v18          v19  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       2.08091      4.92322      3.83227      0.84105      0.22230   \n",
       "std        0.55045      1.34464      1.43607      0.46286      0.12868   \n",
       "min       -0.00000     -0.00000     -0.00000      0.00000     -0.00000   \n",
       "25%        1.90569      4.70588      3.37983      0.71949      0.19241   \n",
       "50%        2.08091      4.92322      3.83227      0.84105      0.22230   \n",
       "75%        2.08091      5.14286      3.83227      0.84105      0.22230   \n",
       "max       20.00000     20.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "               v20          v21          v23          v25          v26  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean      17.77359      7.02974      1.09309      1.69813      1.87603   \n",
       "std        0.86743      1.06940      2.98732      2.24158      0.41398   \n",
       "min        1.51678      0.10618     -0.00000      0.04104     -0.00000   \n",
       "25%       17.77359      6.41875      0.00000      0.27448      1.75553   \n",
       "50%       17.77359      7.03937      0.33059      1.69813      1.87603   \n",
       "75%       18.15460      7.66652      1.09309      1.69813      1.89891   \n",
       "max       20.00000     19.29605     20.00000     20.00000     20.00000   \n",
       "\n",
       "               v27          v28          v29          v32          v33  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       2.74345      5.09333      8.20642      1.62215      2.16163   \n",
       "std        0.62666      2.01131      0.96545      0.42324      0.73970   \n",
       "min       -0.00000     -0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        2.56647      4.74236      8.11437      1.49129      1.83074   \n",
       "50%        2.74345      5.09333      8.20642      1.62215      2.16163   \n",
       "75%        2.77910      5.33034      8.47939      1.62215      2.16163   \n",
       "max       20.00000     19.84819     20.00000     17.56098     20.00000   \n",
       "\n",
       "               v34          v35          v36          v37          v38  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       6.40624      8.12239     13.37560      0.74147      0.09093   \n",
       "std        2.02420      1.00628      1.78573      0.40657      0.58348   \n",
       "min       -0.00000     -0.00000     -0.00000     -0.00000      0.00000   \n",
       "25%        5.05580      7.89615     13.09935      0.59072      0.00000   \n",
       "50%        6.53443      8.12239     13.37560      0.74147      0.00000   \n",
       "75%        7.70145      8.25076     14.32492      0.74147      0.00000   \n",
       "max       20.00000     20.00000     20.00000     20.00000     12.00000   \n",
       "\n",
       "               v39          v40          v41          v42          v43  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       1.23718     10.46593      7.18255     12.92497      2.21660   \n",
       "std        1.77108      3.16764      0.75443      0.74880      0.48667   \n",
       "min       -0.00000      0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        0.30522      8.41039      7.06762     12.81317      2.06897   \n",
       "50%        1.23718     10.33934      7.18255     12.92497      2.21660   \n",
       "75%        1.23718     12.76246      7.34477     13.04965      2.23749   \n",
       "max       19.91553     20.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "               v44          v45          v46          v48          v49  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean      10.79517      9.14223      1.63053     12.53802      8.01655   \n",
       "std        1.58586      1.55058      2.19532      1.64993      0.67797   \n",
       "min       -0.00000     -0.00000      0.06935     -0.00000     -0.00000   \n",
       "25%       10.54256      8.88634      0.25630     12.15693      7.91400   \n",
       "50%       10.79517      9.14223      1.63053     12.53802      8.01655   \n",
       "75%       11.02210      9.41516      1.63053     12.67463      8.13559   \n",
       "max       19.83168     20.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "               v50          v51          v53          v54          v55  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       1.50426      7.19816     15.71130      1.25386      1.55956   \n",
       "std        1.16789      1.87306      0.60036      1.75460      0.62668   \n",
       "min       -0.00000     -0.00000     -0.00000      0.01306     -0.00000   \n",
       "25%        0.65879      6.83727     15.66772      0.20819      1.27660   \n",
       "50%        1.21194      7.19816     15.71130      1.25386      1.55956   \n",
       "75%        2.00572      7.41788     15.87156      1.25386      1.55956   \n",
       "max       20.00000     20.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "               v57          v58          v59          v60          v61  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       4.07783      7.70165     10.58794      1.71429     14.58303   \n",
       "std        0.50925      5.13806      1.55640      0.40378      1.59344   \n",
       "min       -0.00000     -0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        3.97665      4.06136     10.21680      1.59960     14.58303   \n",
       "50%        4.07783      7.70165     10.58794      1.71429     14.58303   \n",
       "75%        4.15366      7.70165     10.83954      1.73502     15.31291   \n",
       "max       20.00000     20.00000     20.00000     20.00000     18.84696   \n",
       "\n",
       "               v62          v63          v64          v65          v67  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       1.03069      1.68733      6.34371     15.84756      9.28728   \n",
       "std        0.69624      2.24951      1.89742      1.41050      0.84371   \n",
       "min        0.00000      0.05306     -0.00000      0.65930     -0.00000   \n",
       "25%        1.00000      0.27094      5.85294     15.84756      9.16798   \n",
       "50%        1.00000      1.68733      6.34371     15.84756      9.28728   \n",
       "75%        1.00000      1.68733      6.38440     16.47085      9.46899   \n",
       "max        7.00000     20.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "               v68          v69          v70          v72          v73  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean      17.56412      9.44934     12.26996      1.43177      2.43330   \n",
       "std        1.71983      1.42670      1.75436      0.92227      0.59981   \n",
       "min        1.50136     -0.00000      0.42709      0.00000     -0.00000   \n",
       "25%       17.56412      9.27007     12.08748      1.00000      2.23612   \n",
       "50%       17.56412      9.44934     12.26996      1.00000      2.43330   \n",
       "75%       18.43750      9.73384     12.91660      2.00000      2.43665   \n",
       "max       20.00000     20.00000     19.81631     12.00000     20.00000   \n",
       "\n",
       "               v76          v77          v78          v80          v81  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       2.40506      7.30737     13.33448      2.20970      7.28717   \n",
       "std        1.03956      0.94339      1.38423      0.80726      1.68567   \n",
       "min       -0.00000     -0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        2.06003      7.19023     13.15175      1.95122      7.20499   \n",
       "50%        2.40506      7.30737     13.33448      2.20970      7.28717   \n",
       "75%        2.40506      7.55221     13.55932      2.24359      7.82301   \n",
       "max       20.00000     15.97351     20.00000     20.00000     20.00000   \n",
       "\n",
       "               v82          v83          v84          v85          v86  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       6.20836      2.17381      1.60796      2.82225      1.22018   \n",
       "std        2.78821      0.79785      0.70691      1.06186      0.34985   \n",
       "min       -0.00000     -0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        3.59298      1.81535      1.31804      2.44395      1.10841   \n",
       "50%        6.20836      2.17381      1.60796      2.82225      1.22018   \n",
       "75%        6.20836      2.17381      1.60796      2.82225      1.22018   \n",
       "max       20.00000     20.00000     20.00000     20.00000     17.56098   \n",
       "\n",
       "               v87          v88          v89          v90          v92  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean      10.18022      1.92418      1.51843      0.96691      0.58237   \n",
       "std        2.27357      0.78753      2.13245      0.13438      0.18040   \n",
       "min        0.87240     -0.00000      0.02237     -0.00000      0.00000   \n",
       "25%        9.55184      1.62351      0.22454      0.94962      0.51793   \n",
       "50%       10.18022      1.92418      1.51843      0.96691      0.58237   \n",
       "75%       10.43359      1.92418      1.51843      0.99010      0.58237   \n",
       "max       19.84275     20.00000     20.00000      6.30577      8.92384   \n",
       "\n",
       "               v93          v94          v95          v96          v97  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       5.47518      3.85288      0.66576      6.45795      7.62255   \n",
       "std        1.23201      0.64216      0.19835      0.84155      1.44498   \n",
       "min        0.00000     -0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        5.13587      3.65008      0.59461      6.36225      7.18954   \n",
       "50%        5.47518      3.85288      0.66576      6.45795      7.62255   \n",
       "75%        5.47518      3.85288      0.66576      6.66900      7.71084   \n",
       "max       20.00000     19.01631      9.07054     20.00000     20.00000   \n",
       "\n",
       "               v98          v99         v100         v101         v102  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       7.66762      1.25072     12.09162      6.86641      2.89029   \n",
       "std        1.76276      0.34655      5.17341      1.76901      1.35412   \n",
       "min       -0.00000     -0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        7.29994      1.17271     12.09162      6.34055      2.32905   \n",
       "50%        7.66762      1.25072     12.09162      6.86641      2.89029   \n",
       "75%        8.00612      1.30167     15.69721      6.93119      2.89029   \n",
       "max       19.05880     20.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "              v103         v104         v105         v106         v108  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       5.29672      2.64283      1.08105     11.79136      2.15262   \n",
       "std        0.92291      0.66527      1.70317      2.21935      0.69222   \n",
       "min       -0.00000     -0.00000      0.00009     -0.00000      0.00000   \n",
       "25%        4.98815      2.43202      0.17355     11.70203      1.85201   \n",
       "50%        5.29672      2.64283      1.08105     11.79136      2.15262   \n",
       "75%        5.29672      2.64283      1.08105     12.44363      2.15262   \n",
       "max       18.77525     20.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "              v109         v111         v114         v115         v116  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       4.18128      3.36531     13.57445     10.54805      2.29122   \n",
       "std        2.81395      1.11715      2.61288      1.42744      0.50340   \n",
       "min       -0.00000     -0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        2.72122      2.92386     11.99667     10.26667      2.13904   \n",
       "50%        4.18128      3.36531     14.03888     10.54805      2.29122   \n",
       "75%        4.18128      3.36531     15.37219     10.71895      2.31017   \n",
       "max       20.00000     20.00000     20.00000     20.00000     20.00000   \n",
       "\n",
       "              v117         v118         v119         v120         v121  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       8.30386      8.36465      3.16897      1.29122      2.73760   \n",
       "std        2.74269      1.50358      3.16360      0.55455      1.01860   \n",
       "min       -0.00000     -0.00000     -0.00000     -0.00000     -0.00000   \n",
       "25%        7.60400      7.86517      1.16943      1.05263      2.28261   \n",
       "50%        8.30386      8.36465      3.16897      1.29122      2.73760   \n",
       "75%        8.64537      8.41772      3.16897      1.29122      2.73760   \n",
       "max       20.00000     20.00000     20.00000     10.39427     20.00000   \n",
       "\n",
       "              v122         v123         v124         v126         v127  \\\n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000 114321.00000   \n",
       "mean       6.82244      3.54994      0.91981      1.67266      3.23954   \n",
       "std        1.34870      1.94343      1.59155      0.37791      1.22123   \n",
       "min       -0.00000      0.01914     -0.00000     -0.00000     -0.00000   \n",
       "25%        6.51961      2.57105      0.08471      1.57097      2.76250   \n",
       "50%        6.82244      3.54994      0.91981      1.67266      3.23954   \n",
       "75%        7.00000      3.54994      0.91981      1.67266      3.23954   \n",
       "max       20.00000     19.68607     20.00000     15.63161     20.00000   \n",
       "\n",
       "              v128         v129         v130         v131  \n",
       "count 114321.00000 114321.00000 114321.00000 114321.00000  \n",
       "mean       2.03037      0.31014      1.92576      1.73939  \n",
       "std        0.81434      0.69326      0.94964      0.85182  \n",
       "min        0.00000      0.00000     -0.00000     -0.00000  \n",
       "25%        1.68126      0.00000      1.44948      1.46341  \n",
       "50%        2.03037      0.00000      1.92576      1.73939  \n",
       "75%        2.03037      0.00000      1.92576      1.73939  \n",
       "max       20.00000     11.00000     20.00000     20.00000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v3</th>\n",
       "      <th>v22</th>\n",
       "      <th>v24</th>\n",
       "      <th>v30</th>\n",
       "      <th>v31</th>\n",
       "      <th>v47</th>\n",
       "      <th>v52</th>\n",
       "      <th>v56</th>\n",
       "      <th>v66</th>\n",
       "      <th>v71</th>\n",
       "      <th>v74</th>\n",
       "      <th>v75</th>\n",
       "      <th>v79</th>\n",
       "      <th>v91</th>\n",
       "      <th>v107</th>\n",
       "      <th>v110</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>18210</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>36</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>C</td>\n",
       "      <td>AGDF</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>J</td>\n",
       "      <td>BW</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>G</td>\n",
       "      <td>BM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>114041</td>\n",
       "      <td>2886</td>\n",
       "      <td>55177</td>\n",
       "      <td>92288</td>\n",
       "      <td>91804</td>\n",
       "      <td>55425</td>\n",
       "      <td>11106</td>\n",
       "      <td>18233</td>\n",
       "      <td>70353</td>\n",
       "      <td>75094</td>\n",
       "      <td>113560</td>\n",
       "      <td>75087</td>\n",
       "      <td>34561</td>\n",
       "      <td>27082</td>\n",
       "      <td>27082</td>\n",
       "      <td>55688</td>\n",
       "      <td>22053</td>\n",
       "      <td>71556</td>\n",
       "      <td>5836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            v3     v22     v24     v30     v31     v47     v52     v56  \\\n",
       "count   114321  114321  114321  114321  114321  114321  114321  114321   \n",
       "unique       3   18210       5       7       3      10      12     122   \n",
       "top          C    AGDF       E       C       A       C       J      BW   \n",
       "freq    114041    2886   55177   92288   91804   55425   11106   18233   \n",
       "\n",
       "           v66     v71     v74     v75     v79     v91    v107    v110  \\\n",
       "count   114321  114321  114321  114321  114321  114321  114321  114321   \n",
       "unique       3       9       3       4      18       7       7       3   \n",
       "top          A       F       B       D       C       A       E       A   \n",
       "freq     70353   75094  113560   75087   34561   27082   27082   55688   \n",
       "\n",
       "          v112    v113    v125  \n",
       "count   114321  114321  114321  \n",
       "unique      22      36      90  \n",
       "top          F       G      BM  \n",
       "freq     22053   71556    5836  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Sundar0989/WOE-and-IV/blob/master/WOE_IV.ipynb\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import re\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "max_bin = 20\n",
    "force_bin = 3\n",
    "\n",
    "# define a binning function\n",
    "def mono_bin(Y, X, n = max_bin):\n",
    "    \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
    "    r = 0\n",
    "    while np.abs(r) < 1:\n",
    "        try:\n",
    "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
    "            d2 = d1.groupby('Bucket', as_index=True)\n",
    "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "            n = n - 1 \n",
    "        except Exception as e:\n",
    "            n = n - 1\n",
    "\n",
    "    if len(d2) == 1:\n",
    "        n = force_bin         \n",
    "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
    "        if len(np.unique(bins)) == 2:\n",
    "            bins = np.insert(bins, 0, 1)\n",
    "            bins[1] = bins[1]-(bins[1]/2)\n",
    "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
    "        d2 = d1.groupby('Bucket', as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"MIN_VALUE\"] = d2.min().X\n",
    "    d3[\"MAX_VALUE\"] = d2.max().X\n",
    "    d3[\"COUNT\"] = d2.count().Y\n",
    "    d3[\"EVENT\"] = d2.sum().Y\n",
    "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
    "    d3=d3.reset_index(drop=True)\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def char_bin(Y, X):\n",
    "        \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
    "    df2 = notmiss.groupby('X',as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"COUNT\"] = df2.count().Y\n",
    "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
    "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "    d3[\"EVENT\"] = df2.sum().Y\n",
    "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    d3 = d3.reset_index(drop=True)\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def data_vars(df1, target):\n",
    "    \n",
    "    stack = traceback.extract_stack()\n",
    "    filename, lineno, function_name, code = stack[-2]\n",
    "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
    "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
    "    \n",
    "    x = df1.dtypes.index\n",
    "    count = -1\n",
    "    \n",
    "    for i in x:\n",
    "        if i.upper() not in (final.upper()):\n",
    "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
    "                conv = mono_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i\n",
    "                count = count + 1\n",
    "            else:\n",
    "                conv = char_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i            \n",
    "                count = count + 1\n",
    "                \n",
    "            if count == 0:\n",
    "                iv_df = conv\n",
    "            else:\n",
    "                iv_df = iv_df.append(conv,ignore_index=True)\n",
    "    \n",
    "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
    "    iv = iv.reset_index()\n",
    "    return(iv_df,iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:679: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VAR_NAME</th>\n",
       "      <th>MIN_VALUE</th>\n",
       "      <th>MAX_VALUE</th>\n",
       "      <th>COUNT</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>EVENT_RATE</th>\n",
       "      <th>NONEVENT</th>\n",
       "      <th>NON_EVENT_RATE</th>\n",
       "      <th>DIST_EVENT</th>\n",
       "      <th>DIST_NON_EVENT</th>\n",
       "      <th>WOE</th>\n",
       "      <th>IV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9055</th>\n",
       "      <td>v22</td>\n",
       "      <td>JEA</td>\n",
       "      <td>JEA</td>\n",
       "      <td>36</td>\n",
       "      <td>35</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>2.396085</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14740</th>\n",
       "      <td>v22</td>\n",
       "      <td>TPX</td>\n",
       "      <td>TPX</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>2.018791</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>v22</td>\n",
       "      <td>AEUR</td>\n",
       "      <td>AEUR</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>1.931780</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12366</th>\n",
       "      <td>v22</td>\n",
       "      <td>PEF</td>\n",
       "      <td>PEF</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>1.931780</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16518</th>\n",
       "      <td>v22</td>\n",
       "      <td>WWN</td>\n",
       "      <td>WWN</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>1.885260</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>v22</td>\n",
       "      <td>ABIM</td>\n",
       "      <td>ABIM</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>-2.768701</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4876</th>\n",
       "      <td>v22</td>\n",
       "      <td>BMV</td>\n",
       "      <td>BMV</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>-2.768701</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>v22</td>\n",
       "      <td>ACBD</td>\n",
       "      <td>ACBD</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>-2.768701</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12304</th>\n",
       "      <td>v22</td>\n",
       "      <td>PAZ</td>\n",
       "      <td>PAZ</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>-2.951022</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9623</th>\n",
       "      <td>v22</td>\n",
       "      <td>KEK</td>\n",
       "      <td>KEK</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>-3.105173</td>\n",
       "      <td>0.297343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18210 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      VAR_NAME MIN_VALUE MAX_VALUE  COUNT  EVENT  EVENT_RATE  NONEVENT  \\\n",
       "9055       v22       JEA       JEA     36     35    0.972222         1   \n",
       "14740      v22       TPX       TPX     25     24    0.960000         1   \n",
       "2624       v22      AEUR      AEUR     23     22    0.956522         1   \n",
       "12366      v22       PEF       PEF     23     22    0.956522         1   \n",
       "16518      v22       WWN       WWN     22     21    0.954545         1   \n",
       "...        ...       ...       ...    ...    ...         ...       ...   \n",
       "731        v22      ABIM      ABIM      6      1    0.166667         5   \n",
       "4876       v22       BMV       BMV      6      1    0.166667         5   \n",
       "1112       v22      ACBD      ACBD      6      1    0.166667         5   \n",
       "12304      v22       PAZ       PAZ      7      1    0.142857         6   \n",
       "9623       v22       KEK       KEK      8      1    0.125000         7   \n",
       "\n",
       "       NON_EVENT_RATE  DIST_EVENT  DIST_NON_EVENT       WOE        IV  \n",
       "9055         0.027778    0.000402        0.000037  2.396085  0.297343  \n",
       "14740        0.040000    0.000276        0.000037  2.018791  0.297343  \n",
       "2624         0.043478    0.000253        0.000037  1.931780  0.297343  \n",
       "12366        0.043478    0.000253        0.000037  1.931780  0.297343  \n",
       "16518        0.045455    0.000241        0.000037  1.885260  0.297343  \n",
       "...               ...         ...             ...       ...       ...  \n",
       "731          0.833333    0.000011        0.000183 -2.768701  0.297343  \n",
       "4876         0.833333    0.000011        0.000183 -2.768701  0.297343  \n",
       "1112         0.833333    0.000011        0.000183 -2.768701  0.297343  \n",
       "12304        0.857143    0.000011        0.000220 -2.951022  0.297343  \n",
       "9623         0.875000    0.000011        0.000256 -3.105173  0.297343  \n",
       "\n",
       "[18210 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "forWOE = df[[\"v22\",\"target\"]].copy()\n",
    "\n",
    "final_iv, IV = data_vars(forWOE , forWOE.target)\n",
    "final_iv.sort_values(\"WOE\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def transform_data(data):\n",
    "    #OH encode\n",
    "    label_encode = data.select_dtypes(include='object').columns\n",
    "    normalize = data.drop(columns=[\"ID\",\"target\"]).select_dtypes(include='number').columns\n",
    "\n",
    "    data_OHE = pd.get_dummies(data, columns=label_encode)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data_OHE[normalize] = scaler.fit_transform(data_OHE[normalize])\n",
    " \n",
    "    return data_OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_iv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f33ea4f63fef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'v22'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'v22'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_iv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"MIN_VALUE\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"WOE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"v22\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"MIN_VALUE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpreModel_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"v22\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"MIN_VALUE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_iv' is not defined"
     ]
    }
   ],
   "source": [
    "df['v22'] = df['v22'].astype('category')\n",
    "df2 = transform_data(df)\n",
    "df3 = df2.merge(final_iv[[\"MIN_VALUE\",\"WOE\"]], how='left', left_on=\"v22\",right_on=\"MIN_VALUE\")\n",
    "preModel_data = df3.drop(columns=[\"v22\",\"MIN_VALUE\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v125_R</th>\n",
       "      <th>v125_S</th>\n",
       "      <th>v125_T</th>\n",
       "      <th>v125_U</th>\n",
       "      <th>v125_V</th>\n",
       "      <th>v125_W</th>\n",
       "      <th>v125_X</th>\n",
       "      <th>v125_Y</th>\n",
       "      <th>v125_Z</th>\n",
       "      <th>WOE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.626710e-01</td>\n",
       "      <td>5.676624e-01</td>\n",
       "      <td>-0.259746</td>\n",
       "      <td>-0.535879</td>\n",
       "      <td>3.614557e-01</td>\n",
       "      <td>1.565290</td>\n",
       "      <td>-7.032151e-01</td>\n",
       "      <td>0.667897</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.730298e-16</td>\n",
       "      <td>7.983532e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290849</td>\n",
       "      <td>9.855250e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.815862e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.445118e-01</td>\n",
       "      <td>-9.682275e-01</td>\n",
       "      <td>0.308200</td>\n",
       "      <td>-2.213376</td>\n",
       "      <td>3.424656e+00</td>\n",
       "      <td>3.263175</td>\n",
       "      <td>-7.000377e-01</td>\n",
       "      <td>2.507568</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.024604e+00</td>\n",
       "      <td>3.776793e-01</td>\n",
       "      <td>0.093701</td>\n",
       "      <td>1.869260</td>\n",
       "      <td>-7.516472e-01</td>\n",
       "      <td>-1.121205</td>\n",
       "      <td>-6.278492e-01</td>\n",
       "      <td>-0.045768</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2.730298e-16</td>\n",
       "      <td>7.983532e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.855250e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.052455e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.238633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 479 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target            v1            v2        v4        v5            v6  \\\n",
       "0   3       1 -3.626710e-01  5.676624e-01 -0.259746 -0.535879  3.614557e-01   \n",
       "1   4       1  2.730298e-16  7.983532e-16  0.000000  0.290849  9.855250e-16   \n",
       "2   5       1 -8.445118e-01 -9.682275e-01  0.308200 -2.213376  3.424656e+00   \n",
       "3   6       1 -1.024604e+00  3.776793e-01  0.093701  1.869260 -7.516472e-01   \n",
       "4   8       1  2.730298e-16  7.983532e-16  0.000000  0.000000  9.855250e-16   \n",
       "\n",
       "         v7            v8        v9  ...  v125_R  v125_S  v125_T  v125_U  \\\n",
       "0  1.565290 -7.032151e-01  0.667897  ...       0       0       0       0   \n",
       "1  0.000000  3.815862e-01  0.000000  ...       0       0       0       0   \n",
       "2  3.263175 -7.000377e-01  2.507568  ...       0       0       0       0   \n",
       "3 -1.121205 -6.278492e-01 -0.045768  ...       0       0       0       0   \n",
       "4  0.000000 -1.052455e-16  0.000000  ...       0       0       0       0   \n",
       "\n",
       "   v125_V  v125_W  v125_X  v125_Y  v125_Z       WOE  \n",
       "0       0       0       0       0       0  0.000000  \n",
       "1       0       0       0       0       0  0.023433  \n",
       "2       0       0       0       0       0  0.000000  \n",
       "3       0       0       0       0       0  0.307074  \n",
       "4       0       0       0       0       1  1.238633  \n",
       "\n",
       "[5 rows x 479 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preModel_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score #https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimator_grid_search:[80, 90, 100]\n",
      "max_features_grid_search:[5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
      "min_samples_split_grid_search:[500, 600, 700]\n",
      "min_samples_leaf_grid_search:[10, 20]\n"
     ]
    }
   ],
   "source": [
    "n_estimators= list(range(80, 110, 10))\n",
    "max_features = list(range(5, 50, 5))\n",
    "min_samples_split = list(range(500, 701, 100))\n",
    "min_samples_leaf = [10, 20]\n",
    "print(f'n_estimator_grid_search:{n_estimators}')\n",
    "print(f'max_features_grid_search:{max_features}')\n",
    "print(f'min_samples_split_grid_search:{min_samples_split}')\n",
    "print(f'min_samples_leaf_grid_search:{min_samples_leaf}')\n",
    "\n",
    "\n",
    "param_dist = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "scoring = {  'Accuracy':'accuracy'\n",
    "            , 'Log Loss':'neg_log_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is:  (114321, 112)\n",
      "The shape of y is:  (114321,)\n"
     ]
    }
   ],
   "source": [
    "X = df.copy().drop(columns=[\"ID\",\"target\"]).select_dtypes(include=['number'])\n",
    "print(\"The shape of X is: \", X.shape)\n",
    "\n",
    "y = df.loc[:,\"target\"].copy()\n",
    "print(\"The shape of y is: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3e010b9786ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0, n_jobs=-1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%%script false --no-raise-error\n",
    "\n",
    "n_iter_search = 2\n",
    "rf_random_search = RandomizedSearchCV(clf, param_distributions=param_dist, scoring=scoring, cv = cv, random_state=42,\n",
    "                                   n_iter=n_iter_search, refit='Accuracy')\n",
    "rf_random_search.fit(X, y)\n",
    "\n",
    "filename = 'rf_random_search.p'\n",
    "pickle.dump(rf_random_search, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_Accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_Log Loss</th>\n",
       "      <th>std_test_Log Loss</th>\n",
       "      <th>rank_test_Log Loss</th>\n",
       "      <th>split0_train_Log Loss</th>\n",
       "      <th>split1_train_Log Loss</th>\n",
       "      <th>split2_train_Log Loss</th>\n",
       "      <th>split3_train_Log Loss</th>\n",
       "      <th>split4_train_Log Loss</th>\n",
       "      <th>mean_train_Log Loss</th>\n",
       "      <th>std_train_Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.877302</td>\n",
       "      <td>0.840360</td>\n",
       "      <td>0.376586</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>100</td>\n",
       "      <td>600</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 600...</td>\n",
       "      <td>0.761207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511646</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.510321</td>\n",
       "      <td>-0.510189</td>\n",
       "      <td>-0.510482</td>\n",
       "      <td>-0.510396</td>\n",
       "      <td>-0.510773</td>\n",
       "      <td>-0.510432</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.229324</td>\n",
       "      <td>0.051449</td>\n",
       "      <td>0.377685</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>90</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>{'n_estimators': 90, 'min_samples_split': 500,...</td>\n",
       "      <td>0.761207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.514810</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.513723</td>\n",
       "      <td>-0.513504</td>\n",
       "      <td>-0.513954</td>\n",
       "      <td>-0.513783</td>\n",
       "      <td>-0.513657</td>\n",
       "      <td>-0.513724</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       4.877302      0.840360         0.376586        0.000893   \n",
       "1       3.229324      0.051449         0.377685        0.004225   \n",
       "\n",
       "  param_n_estimators param_min_samples_split param_min_samples_leaf  \\\n",
       "0                100                     600                     20   \n",
       "1                 90                     500                     10   \n",
       "\n",
       "  param_max_features                                             params  \\\n",
       "0                 45  {'n_estimators': 100, 'min_samples_split': 600...   \n",
       "1                 35  {'n_estimators': 90, 'min_samples_split': 500,...   \n",
       "\n",
       "   split0_test_Accuracy  ...  mean_test_Log Loss  std_test_Log Loss  \\\n",
       "0              0.761207  ...           -0.511646           0.000566   \n",
       "1              0.761207  ...           -0.514810           0.000694   \n",
       "\n",
       "   rank_test_Log Loss  split0_train_Log Loss  split1_train_Log Loss  \\\n",
       "0                   1              -0.510321              -0.510189   \n",
       "1                   2              -0.513723              -0.513504   \n",
       "\n",
       "   split2_train_Log Loss  split3_train_Log Loss  split4_train_Log Loss  \\\n",
       "0              -0.510482              -0.510396              -0.510773   \n",
       "1              -0.513954              -0.513783              -0.513657   \n",
       "\n",
       "   mean_train_Log Loss  std_train_Log Loss  \n",
       "0            -0.510432            0.000196  \n",
       "1            -0.513724            0.000148  \n",
       "\n",
       "[2 rows x 54 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random_search = pd.read_pickle('rf_random_search.p')\n",
    "pd.DataFrame(rf_random_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "#https://xgboost.readthedocs.io/en/latest/build.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'learning_rate': [0.005, 0.01, 0.02, 0.05, 0.1],\n",
    "        'n_estimators': [100,200,400,600,800,1000],\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "xgb = XGBClassifier(objective='binary:logistic', early_stopping_rounds=10, tree_method='hist',\n",
    "                    silent=True, nthread=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%%script false --no-raise-error\n",
    "n_iter_search = 2\n",
    "xgb_random_search = RandomizedSearchCV(xgb, param_distributions=params, scoring=scoring, cv = cv, random_state=42,\n",
    "                                   n_iter=n_iter_search, refit='Accuracy')\n",
    "xgb_random_search.fit(X, y)\n",
    "\n",
    "#filename = 'xgb_random_search.p'\n",
    "#pickle.dump(xgb_random_search, open(filename, 'wb'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_Accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_F-1 Score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_Log Loss'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_Log Loss</th>\n",
       "      <th>std_test_Log Loss</th>\n",
       "      <th>rank_test_Log Loss</th>\n",
       "      <th>split0_train_Log Loss</th>\n",
       "      <th>split1_train_Log Loss</th>\n",
       "      <th>split2_train_Log Loss</th>\n",
       "      <th>split3_train_Log Loss</th>\n",
       "      <th>split4_train_Log Loss</th>\n",
       "      <th>mean_train_Log Loss</th>\n",
       "      <th>std_train_Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.695948</td>\n",
       "      <td>0.513996</td>\n",
       "      <td>0.563534</td>\n",
       "      <td>0.012532</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.499054</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.371650</td>\n",
       "      <td>-0.371244</td>\n",
       "      <td>-0.370991</td>\n",
       "      <td>-0.371692</td>\n",
       "      <td>-0.370583</td>\n",
       "      <td>-0.371232</td>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.084821</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>0.416493</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496882</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.493681</td>\n",
       "      <td>-0.493714</td>\n",
       "      <td>-0.493701</td>\n",
       "      <td>-0.493410</td>\n",
       "      <td>-0.494110</td>\n",
       "      <td>-0.493723</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      78.695948      0.513996         0.563534        0.012532   \n",
       "1      37.084821      0.255249         0.416493        0.004925   \n",
       "\n",
       "  param_subsample param_n_estimators param_min_child_weight param_max_depth  \\\n",
       "0             0.8               1000                      5               5   \n",
       "1               1                800                     10               3   \n",
       "\n",
       "  param_learning_rate param_gamma  ... mean_test_Log Loss std_test_Log Loss  \\\n",
       "0                 0.1           2  ...          -0.499054          0.001442   \n",
       "1               0.005           1  ...          -0.496882          0.000619   \n",
       "\n",
       "   rank_test_Log Loss  split0_train_Log Loss  split1_train_Log Loss  \\\n",
       "0                   2              -0.371650              -0.371244   \n",
       "1                   1              -0.493681              -0.493714   \n",
       "\n",
       "   split2_train_Log Loss  split3_train_Log Loss  split4_train_Log Loss  \\\n",
       "0              -0.370991              -0.371692              -0.370583   \n",
       "1              -0.493701              -0.493410              -0.494110   \n",
       "\n",
       "   mean_train_Log Loss  std_train_Log Loss  \n",
       "0            -0.371232            0.000416  \n",
       "1            -0.493723            0.000224  \n",
       "\n",
       "[2 rows x 57 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_random_search = pd.read_pickle('xgb_random_search.p')\n",
    "pd.DataFrame(xgb_random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC\n",
    "This never finishes. Need to reduce features for it to do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is:  (500, 477)\n",
      "The shape of y is:  (500,)\n"
     ]
    }
   ],
   "source": [
    "sampled = preModel_data.sample(n=500, replace=False, random_state=1)\n",
    "\n",
    "\n",
    "X = sampled.copy().drop(columns=[\"ID\",\"target\"]).select_dtypes(include=['number'])\n",
    "print(\"The shape of X is: \", X.shape)\n",
    "\n",
    "y = sampled.loc[:,\"target\"].copy()\n",
    "print(\"The shape of y is: \", y.shape)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 893 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%%script false --no-raise-error\n",
    "n_iter_search = 2\n",
    "svc_random_search = RandomizedSearchCV(SVC(probability=True), param_distributions=param_grid, scoring=scoring, cv = 2, random_state=42,\n",
    "                                   n_iter=n_iter_search, refit='Accuracy')\n",
    "svc_random_search.fit(X, y)\n",
    "\n",
    "filename = 'svc_random_search.p'\n",
    "pickle.dump(svc_random_search, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_Accuracy</th>\n",
       "      <th>split1_test_Accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>std_train_Accuracy</th>\n",
       "      <th>split0_test_Log Loss</th>\n",
       "      <th>split1_test_Log Loss</th>\n",
       "      <th>mean_test_Log Loss</th>\n",
       "      <th>std_test_Log Loss</th>\n",
       "      <th>rank_test_Log Loss</th>\n",
       "      <th>split0_train_Log Loss</th>\n",
       "      <th>split1_train_Log Loss</th>\n",
       "      <th>mean_train_Log Loss</th>\n",
       "      <th>std_train_Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.105361</td>\n",
       "      <td>1.226664e-04</td>\n",
       "      <td>0.029006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'kernel': 'rbf', 'gamma': 0.001, 'C': 1000}</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.531730</td>\n",
       "      <td>-0.532940</td>\n",
       "      <td>-0.532335</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.348140</td>\n",
       "      <td>-0.437948</td>\n",
       "      <td>-0.393044</td>\n",
       "      <td>0.044904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078018</td>\n",
       "      <td>4.768372e-07</td>\n",
       "      <td>0.029008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>{'kernel': 'rbf', 'gamma': 0.0001, 'C': 10}</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.533451</td>\n",
       "      <td>-0.529176</td>\n",
       "      <td>-0.531314</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.499605</td>\n",
       "      <td>-0.504951</td>\n",
       "      <td>-0.502278</td>\n",
       "      <td>0.002673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_kernel  \\\n",
       "0       0.105361  1.226664e-04         0.029006           0.001          rbf   \n",
       "1       0.078018  4.768372e-07         0.029008           0.000          rbf   \n",
       "\n",
       "  param_gamma param_C                                        params  \\\n",
       "0       0.001    1000  {'kernel': 'rbf', 'gamma': 0.001, 'C': 1000}   \n",
       "1      0.0001      10   {'kernel': 'rbf', 'gamma': 0.0001, 'C': 10}   \n",
       "\n",
       "   split0_test_Accuracy  split1_test_Accuracy  ...  std_train_Accuracy  \\\n",
       "0                 0.708                 0.636  ...               0.000   \n",
       "1                 0.768                 0.768  ...               0.002   \n",
       "\n",
       "   split0_test_Log Loss  split1_test_Log Loss  mean_test_Log Loss  \\\n",
       "0             -0.531730             -0.532940           -0.532335   \n",
       "1             -0.533451             -0.529176           -0.531314   \n",
       "\n",
       "   std_test_Log Loss  rank_test_Log Loss  split0_train_Log Loss  \\\n",
       "0           0.000605                   2              -0.348140   \n",
       "1           0.002137                   1              -0.499605   \n",
       "\n",
       "   split1_train_Log Loss  mean_train_Log Loss  std_train_Log Loss  \n",
       "0              -0.437948            -0.393044            0.044904  \n",
       "1              -0.504951            -0.502278            0.002673  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(svc_random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss, accuracy_score \n",
    "import pickle\n",
    "#https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v125_R</th>\n",
       "      <th>v125_S</th>\n",
       "      <th>v125_T</th>\n",
       "      <th>v125_U</th>\n",
       "      <th>v125_V</th>\n",
       "      <th>v125_W</th>\n",
       "      <th>v125_X</th>\n",
       "      <th>v125_Y</th>\n",
       "      <th>v125_Z</th>\n",
       "      <th>WOE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.626710e-01</td>\n",
       "      <td>5.676624e-01</td>\n",
       "      <td>-0.259746</td>\n",
       "      <td>-0.535879</td>\n",
       "      <td>3.614557e-01</td>\n",
       "      <td>1.565290</td>\n",
       "      <td>-7.032151e-01</td>\n",
       "      <td>0.667897</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.730298e-16</td>\n",
       "      <td>7.983532e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290849</td>\n",
       "      <td>9.855250e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.815862e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.445118e-01</td>\n",
       "      <td>-9.682275e-01</td>\n",
       "      <td>0.308200</td>\n",
       "      <td>-2.213376</td>\n",
       "      <td>3.424656e+00</td>\n",
       "      <td>3.263175</td>\n",
       "      <td>-7.000377e-01</td>\n",
       "      <td>2.507568</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.024604e+00</td>\n",
       "      <td>3.776793e-01</td>\n",
       "      <td>0.093701</td>\n",
       "      <td>1.869260</td>\n",
       "      <td>-7.516472e-01</td>\n",
       "      <td>-1.121205</td>\n",
       "      <td>-6.278492e-01</td>\n",
       "      <td>-0.045768</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2.730298e-16</td>\n",
       "      <td>7.983532e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.855250e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.052455e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.238633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 479 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target            v1            v2        v4        v5            v6  \\\n",
       "0   3       1 -3.626710e-01  5.676624e-01 -0.259746 -0.535879  3.614557e-01   \n",
       "1   4       1  2.730298e-16  7.983532e-16  0.000000  0.290849  9.855250e-16   \n",
       "2   5       1 -8.445118e-01 -9.682275e-01  0.308200 -2.213376  3.424656e+00   \n",
       "3   6       1 -1.024604e+00  3.776793e-01  0.093701  1.869260 -7.516472e-01   \n",
       "4   8       1  2.730298e-16  7.983532e-16  0.000000  0.000000  9.855250e-16   \n",
       "\n",
       "         v7            v8        v9  ...  v125_R  v125_S  v125_T  v125_U  \\\n",
       "0  1.565290 -7.032151e-01  0.667897  ...       0       0       0       0   \n",
       "1  0.000000  3.815862e-01  0.000000  ...       0       0       0       0   \n",
       "2  3.263175 -7.000377e-01  2.507568  ...       0       0       0       0   \n",
       "3 -1.121205 -6.278492e-01 -0.045768  ...       0       0       0       0   \n",
       "4  0.000000 -1.052455e-16  0.000000  ...       0       0       0       0   \n",
       "\n",
       "   v125_V  v125_W  v125_X  v125_Y  v125_Z       WOE  \n",
       "0       0       0       0       0       0  0.000000  \n",
       "1       0       0       0       0       0  0.023433  \n",
       "2       0       0       0       0       0  0.000000  \n",
       "3       0       0       0       0       0  0.307074  \n",
       "4       0       0       0       0       1  1.238633  \n",
       "\n",
       "[5 rows x 479 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preModel_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is:  (11432, 477)\n",
      "The shape of y is:  (11432,)\n"
     ]
    }
   ],
   "source": [
    "sampled = preModel_data.sample(frac=0.1, replace=False, random_state=1)\n",
    "\n",
    "\n",
    "X = sampled.copy().drop(columns=[\"ID\",\"target\"]).select_dtypes(include=['number'])\n",
    "print(\"The shape of X is: \", X.shape)\n",
    "\n",
    "y = sampled.loc[:,\"target\"].copy()\n",
    "print(\"The shape of y is: \", y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scoring = {  'Log Loss':'neg_log_loss'\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-516b1e4789ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m lsvc_random_search = RandomizedSearchCV(SVC(kernel='linear',probability=True), param_distributions=param_grid, random_state=42,\n\u001b[0;32m      3\u001b[0m                                    n_iter=n_iter_search)\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlsvc_random_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lsvc_random_search.p'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    269\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter_search = 2\n",
    "lsvc_random_search = RandomizedSearchCV(SVC(kernel='linear',probability=True), param_distributions=param_grid, random_state=42,\n",
    "                                   n_iter=n_iter_search)\n",
    "lsvc_random_search.fit(X_train, y_train)\n",
    "\n",
    "filename = 'lsvc_random_search.p'\n",
    "pickle.dump(lsvc_random_search, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc_random_search = pd.read_pickle('lsvc_random_search.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8bc0d6d0dd3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlsvc_random_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'results'"
     ]
    }
   ],
   "source": [
    "lsvc_random_search.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':[1,10],'penalty':['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 'accuracy', 'Log Loss': 'neg_log_loss'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LinearSVC' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-afdce47656fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m lsvc_random_search_2 = RandomizedSearchCV(LinearSVC(), param_distributions=param_grid, random_state=42, scoring= 'neg_log_loss',\n\u001b[0;32m      3\u001b[0m                                    n_iter=n_iter_search,cv=2)\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlsvc_random_search_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lsvc_random_search_2.p'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1515\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \"\"\"\n\u001b[0;32m    604\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[1;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, clf, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m    126\u001b[0m         \u001b[0my_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearSVC' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "n_iter_search = 2\n",
    "lsvc_random_search_2 = RandomizedSearchCV(LinearSVC(), param_distributions=param_grid, random_state=42, scoring= 'neg_log_loss',\n",
    "                                   n_iter=n_iter_search,cv=2)\n",
    "lsvc_random_search_2.fit(X_train, y_train)\n",
    "\n",
    "filename = 'lsvc_random_search_2.p'\n",
    "pickle.dump(lsvc_random_search_2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.465384</td>\n",
       "      <td>0.037277</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>1.000524e-03</td>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'penalty': 'l2', 'C': 1}</td>\n",
       "      <td>0.759269</td>\n",
       "      <td>0.759206</td>\n",
       "      <td>0.759237</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>1</td>\n",
       "      <td>0.805693</td>\n",
       "      <td>0.805744</td>\n",
       "      <td>0.805719</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.591785</td>\n",
       "      <td>0.059439</td>\n",
       "      <td>0.011003</td>\n",
       "      <td>3.576279e-07</td>\n",
       "      <td>l2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'penalty': 'l2', 'C': 10}</td>\n",
       "      <td>0.694517</td>\n",
       "      <td>0.724471</td>\n",
       "      <td>0.709492</td>\n",
       "      <td>0.014977</td>\n",
       "      <td>2</td>\n",
       "      <td>0.735962</td>\n",
       "      <td>0.762402</td>\n",
       "      <td>0.749182</td>\n",
       "      <td>0.013220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_penalty  \\\n",
       "0       1.465384      0.037277         0.012004    1.000524e-03            l2   \n",
       "1       1.591785      0.059439         0.011003    3.576279e-07            l2   \n",
       "\n",
       "  param_C                      params  split0_test_score  split1_test_score  \\\n",
       "0       1   {'penalty': 'l2', 'C': 1}           0.759269           0.759206   \n",
       "1      10  {'penalty': 'l2', 'C': 10}           0.694517           0.724471   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.759237        0.000031                1            0.805693   \n",
       "1         0.709492        0.014977                2            0.735962   \n",
       "\n",
       "   split1_train_score  mean_train_score  std_train_score  \n",
       "0            0.805744          0.805719         0.000025  \n",
       "1            0.762402          0.749182         0.013220  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lsvc_random_search_2.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
