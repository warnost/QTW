{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# import shutil\n",
    "# # C:\\Users\\allro\\JupyterNotebook\\QTW\\Data\n",
    "# with gzip.open('C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv.gz', 'rb') as f_in:\n",
    "#    with open('C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv', 'wb') as f_out:\n",
    "#        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/ml/datasets/HIGGS#\n",
    "#df = pd.read_csv(\"./Data/HIGGS.csv\", header=None)\n",
    "df = pd.read_csv(\"../../HIGGS.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns =['target', 'lepton_ph', 'lepton_eta', 'lepton_phi','missing_energy_magnitude','missing_energy_phi',\n",
    "             'jet_1_pt','jet_1_eta','jet_1_phi','jet_1_btag','jet_2_pt','jet_2_eta','jet_2_phi','jet_2_btag',\n",
    "             'jet_3_pt','jet_3_eta','jet_3_phi','jet_3_btag','jet_4_pt','jet_4_eta','jet_4_phi','jet_4_btag',\n",
    "             'm_jj','m_jjj','m_lv','m_jlv','m_bb','m_wbb','m_wwbb'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000000 entries, 0 to 10999999\n",
      "Data columns (total 29 columns):\n",
      "target                      float64\n",
      "lepton_ph                   float64\n",
      "lepton_eta                  float64\n",
      "lepton_phi                  float64\n",
      "missing_energy_magnitude    float64\n",
      "missing_energy_phi          float64\n",
      "jet_1_pt                    float64\n",
      "jet_1_eta                   float64\n",
      "jet_1_phi                   float64\n",
      "jet_1_btag                  float64\n",
      "jet_2_pt                    float64\n",
      "jet_2_eta                   float64\n",
      "jet_2_phi                   float64\n",
      "jet_2_btag                  float64\n",
      "jet_3_pt                    float64\n",
      "jet_3_eta                   float64\n",
      "jet_3_phi                   float64\n",
      "jet_3_btag                  float64\n",
      "jet_4_pt                    float64\n",
      "jet_4_eta                   float64\n",
      "jet_4_phi                   float64\n",
      "jet_4_btag                  float64\n",
      "m_jj                        float64\n",
      "m_jjj                       float64\n",
      "m_lv                        float64\n",
      "m_jlv                       float64\n",
      "m_bb                        float64\n",
      "m_wbb                       float64\n",
      "m_wwbb                      float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "# Print out the data types\n",
    "df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.374992</td>\n",
       "      <td>-0.653674</td>\n",
       "      <td>0.930349</td>\n",
       "      <td>1.107436</td>\n",
       "      <td>1.138904</td>\n",
       "      <td>-1.578198</td>\n",
       "      <td>-1.046985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.657930</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.812581</td>\n",
       "      <td>-0.213642</td>\n",
       "      <td>1.271015</td>\n",
       "      <td>2.214872</td>\n",
       "      <td>0.499994</td>\n",
       "      <td>-1.261432</td>\n",
       "      <td>0.732156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398701</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851737</td>\n",
       "      <td>1.540659</td>\n",
       "      <td>-0.819690</td>\n",
       "      <td>2.214872</td>\n",
       "      <td>0.993490</td>\n",
       "      <td>0.356080</td>\n",
       "      <td>-0.208778</td>\n",
       "      <td>2.548224</td>\n",
       "      <td>1.256955</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.423265</td>\n",
       "      <td>-0.676016</td>\n",
       "      <td>0.736159</td>\n",
       "      <td>2.214872</td>\n",
       "      <td>1.298720</td>\n",
       "      <td>-1.430738</td>\n",
       "      <td>-0.364658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.745313</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800872</td>\n",
       "      <td>1.020974</td>\n",
       "      <td>0.971407</td>\n",
       "      <td>2.214872</td>\n",
       "      <td>0.596761</td>\n",
       "      <td>-0.350273</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479999</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  lepton_ph  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0     1.0   0.869293   -0.635082    0.225690                  0.327470   \n",
       "1     1.0   0.907542    0.329147    0.359412                  1.497970   \n",
       "2     1.0   0.798835    1.470639   -1.635975                  0.453773   \n",
       "3     0.0   1.344385   -0.876626    0.935913                  1.992050   \n",
       "4     1.0   1.105009    0.321356    1.522401                  0.882808   \n",
       "\n",
       "   missing_energy_phi  jet_1_pt  jet_1_eta  jet_1_phi  jet_1_btag  jet_2_pt  \\\n",
       "0           -0.689993  0.754202  -0.248573  -1.092064    0.000000  1.374992   \n",
       "1           -0.313010  1.095531  -0.557525  -1.588230    2.173076  0.812581   \n",
       "2            0.425629  1.104875   1.282322   1.381664    0.000000  0.851737   \n",
       "3            0.882454  1.786066  -1.646778  -0.942383    0.000000  2.423265   \n",
       "4           -1.205349  0.681466  -1.070464  -0.921871    0.000000  0.800872   \n",
       "\n",
       "   jet_2_eta  jet_2_phi  jet_2_btag  jet_3_pt  jet_3_eta  jet_3_phi  \\\n",
       "0  -0.653674   0.930349    1.107436  1.138904  -1.578198  -1.046985   \n",
       "1  -0.213642   1.271015    2.214872  0.499994  -1.261432   0.732156   \n",
       "2   1.540659  -0.819690    2.214872  0.993490   0.356080  -0.208778   \n",
       "3  -0.676016   0.736159    2.214872  1.298720  -1.430738  -0.364658   \n",
       "4   1.020974   0.971407    2.214872  0.596761  -0.350273   0.631194   \n",
       "\n",
       "   jet_3_btag  jet_4_pt  jet_4_eta  jet_4_phi  jet_4_btag      m_jj     m_jjj  \\\n",
       "0    0.000000  0.657930  -0.010455  -0.045767    3.101961  1.353760  0.979563   \n",
       "1    0.000000  0.398701  -1.138930  -0.000819    0.000000  0.302220  0.833048   \n",
       "2    2.548224  1.256955   1.128848   0.900461    0.000000  0.909753  1.108330   \n",
       "3    0.000000  0.745313  -0.678379  -1.360356    0.000000  0.946652  1.028704   \n",
       "4    0.000000  0.479999  -0.373566   0.113041    0.000000  0.755856  1.361057   \n",
       "\n",
       "       m_lv     m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "0  0.978076  0.920005  0.721657  0.988751  0.876678  \n",
       "1  0.985700  0.978098  0.779732  0.992356  0.798343  \n",
       "2  0.985692  0.951331  0.803252  0.865924  0.780118  \n",
       "3  0.998656  0.728281  0.869200  1.026736  0.957904  \n",
       "4  0.986610  0.838085  1.133295  0.872245  0.808487  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample the data\n",
    "train = df.sample(n=2600000, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.299203e-01</td>\n",
       "      <td>9.914658e-01</td>\n",
       "      <td>-8.297618e-06</td>\n",
       "      <td>-1.327225e-05</td>\n",
       "      <td>9.985364e-01</td>\n",
       "      <td>2.613459e-05</td>\n",
       "      <td>9.909152e-01</td>\n",
       "      <td>-2.027520e-05</td>\n",
       "      <td>7.716199e-06</td>\n",
       "      <td>9.999687e-01</td>\n",
       "      <td>9.927294e-01</td>\n",
       "      <td>-1.026444e-05</td>\n",
       "      <td>-2.076887e-05</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>9.922591e-01</td>\n",
       "      <td>1.459561e-05</td>\n",
       "      <td>3.678632e-06</td>\n",
       "      <td>1.000011e+00</td>\n",
       "      <td>9.861087e-01</td>\n",
       "      <td>-5.756954e-06</td>\n",
       "      <td>1.744903e-05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.034290e+00</td>\n",
       "      <td>1.024805e+00</td>\n",
       "      <td>1.050554e+00</td>\n",
       "      <td>1.009742e+00</td>\n",
       "      <td>9.729596e-01</td>\n",
       "      <td>1.033036e+00</td>\n",
       "      <td>9.598120e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.991040e-01</td>\n",
       "      <td>5.653777e-01</td>\n",
       "      <td>1.008827e+00</td>\n",
       "      <td>1.006346e+00</td>\n",
       "      <td>6.000185e-01</td>\n",
       "      <td>1.006326e+00</td>\n",
       "      <td>4.749747e-01</td>\n",
       "      <td>1.009303e+00</td>\n",
       "      <td>1.005901e+00</td>\n",
       "      <td>1.027808e+00</td>\n",
       "      <td>4.999939e-01</td>\n",
       "      <td>1.009331e+00</td>\n",
       "      <td>1.006154e+00</td>\n",
       "      <td>1.049398e+00</td>\n",
       "      <td>4.876623e-01</td>\n",
       "      <td>1.008747e+00</td>\n",
       "      <td>1.006305e+00</td>\n",
       "      <td>1.193676e+00</td>\n",
       "      <td>5.057777e-01</td>\n",
       "      <td>1.007694e+00</td>\n",
       "      <td>1.006366e+00</td>\n",
       "      <td>1.400209e+00</td>\n",
       "      <td>6.746354e-01</td>\n",
       "      <td>3.808074e-01</td>\n",
       "      <td>1.645763e-01</td>\n",
       "      <td>3.974453e-01</td>\n",
       "      <td>5.254063e-01</td>\n",
       "      <td>3.652556e-01</td>\n",
       "      <td>3.133378e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.746966e-01</td>\n",
       "      <td>-2.434976e+00</td>\n",
       "      <td>-1.742508e+00</td>\n",
       "      <td>2.370088e-04</td>\n",
       "      <td>-1.743944e+00</td>\n",
       "      <td>1.375024e-01</td>\n",
       "      <td>-2.969725e+00</td>\n",
       "      <td>-1.741237e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.889811e-01</td>\n",
       "      <td>-2.913090e+00</td>\n",
       "      <td>-1.742372e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.636076e-01</td>\n",
       "      <td>-2.729663e+00</td>\n",
       "      <td>-1.742069e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.653542e-01</td>\n",
       "      <td>-2.497265e+00</td>\n",
       "      <td>-1.742691e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.507046e-02</td>\n",
       "      <td>1.986757e-01</td>\n",
       "      <td>8.304866e-02</td>\n",
       "      <td>1.320062e-01</td>\n",
       "      <td>4.786215e-02</td>\n",
       "      <td>2.951122e-01</td>\n",
       "      <td>3.307214e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.907533e-01</td>\n",
       "      <td>-7.383225e-01</td>\n",
       "      <td>-8.719308e-01</td>\n",
       "      <td>5.768156e-01</td>\n",
       "      <td>-8.712081e-01</td>\n",
       "      <td>6.789927e-01</td>\n",
       "      <td>-6.872450e-01</td>\n",
       "      <td>-8.680962e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.564608e-01</td>\n",
       "      <td>-6.944718e-01</td>\n",
       "      <td>-8.701791e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.508527e-01</td>\n",
       "      <td>-6.998083e-01</td>\n",
       "      <td>-8.711343e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.177673e-01</td>\n",
       "      <td>-7.141902e-01</td>\n",
       "      <td>-8.714789e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.906095e-01</td>\n",
       "      <td>8.462266e-01</td>\n",
       "      <td>9.857525e-01</td>\n",
       "      <td>7.675732e-01</td>\n",
       "      <td>6.738168e-01</td>\n",
       "      <td>8.193964e-01</td>\n",
       "      <td>7.703901e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.533714e-01</td>\n",
       "      <td>-5.415563e-05</td>\n",
       "      <td>-2.410638e-04</td>\n",
       "      <td>8.916277e-01</td>\n",
       "      <td>2.125454e-04</td>\n",
       "      <td>8.948193e-01</td>\n",
       "      <td>-2.543566e-05</td>\n",
       "      <td>5.813991e-05</td>\n",
       "      <td>1.086538e+00</td>\n",
       "      <td>8.901377e-01</td>\n",
       "      <td>6.027267e-05</td>\n",
       "      <td>3.514990e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.972494e-01</td>\n",
       "      <td>1.728937e-04</td>\n",
       "      <td>-7.519117e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.682333e-01</td>\n",
       "      <td>3.721330e-04</td>\n",
       "      <td>-2.642369e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.949304e-01</td>\n",
       "      <td>9.506853e-01</td>\n",
       "      <td>9.897798e-01</td>\n",
       "      <td>9.165110e-01</td>\n",
       "      <td>8.733798e-01</td>\n",
       "      <td>9.473447e-01</td>\n",
       "      <td>8.719701e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.236226e+00</td>\n",
       "      <td>7.382142e-01</td>\n",
       "      <td>8.709940e-01</td>\n",
       "      <td>1.293056e+00</td>\n",
       "      <td>8.714708e-01</td>\n",
       "      <td>1.170740e+00</td>\n",
       "      <td>6.871941e-01</td>\n",
       "      <td>8.683126e-01</td>\n",
       "      <td>2.173076e+00</td>\n",
       "      <td>1.201875e+00</td>\n",
       "      <td>6.945924e-01</td>\n",
       "      <td>8.698727e-01</td>\n",
       "      <td>2.214872e+00</td>\n",
       "      <td>1.221798e+00</td>\n",
       "      <td>7.001541e-01</td>\n",
       "      <td>8.713947e-01</td>\n",
       "      <td>2.548224e+00</td>\n",
       "      <td>1.220930e+00</td>\n",
       "      <td>7.141017e-01</td>\n",
       "      <td>8.716055e-01</td>\n",
       "      <td>3.101961e+00</td>\n",
       "      <td>1.024730e+00</td>\n",
       "      <td>1.083493e+00</td>\n",
       "      <td>1.020528e+00</td>\n",
       "      <td>1.142226e+00</td>\n",
       "      <td>1.138439e+00</td>\n",
       "      <td>1.140458e+00</td>\n",
       "      <td>1.059248e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.209891e+01</td>\n",
       "      <td>2.434868e+00</td>\n",
       "      <td>1.743236e+00</td>\n",
       "      <td>1.539682e+01</td>\n",
       "      <td>1.743257e+00</td>\n",
       "      <td>9.940391e+00</td>\n",
       "      <td>2.969674e+00</td>\n",
       "      <td>1.741454e+00</td>\n",
       "      <td>2.173076e+00</td>\n",
       "      <td>1.164708e+01</td>\n",
       "      <td>2.913210e+00</td>\n",
       "      <td>1.743175e+00</td>\n",
       "      <td>2.214872e+00</td>\n",
       "      <td>1.470899e+01</td>\n",
       "      <td>2.730009e+00</td>\n",
       "      <td>1.742884e+00</td>\n",
       "      <td>2.548224e+00</td>\n",
       "      <td>1.288257e+01</td>\n",
       "      <td>2.498009e+00</td>\n",
       "      <td>1.743372e+00</td>\n",
       "      <td>3.101961e+00</td>\n",
       "      <td>4.019237e+01</td>\n",
       "      <td>2.037278e+01</td>\n",
       "      <td>7.992739e+00</td>\n",
       "      <td>1.426244e+01</td>\n",
       "      <td>1.776285e+01</td>\n",
       "      <td>1.149652e+01</td>\n",
       "      <td>8.374498e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target     lepton_ph    lepton_eta    lepton_phi  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   5.299203e-01  9.914658e-01 -8.297618e-06 -1.327225e-05   \n",
       "std    4.991040e-01  5.653777e-01  1.008827e+00  1.006346e+00   \n",
       "min    0.000000e+00  2.746966e-01 -2.434976e+00 -1.742508e+00   \n",
       "25%    0.000000e+00  5.907533e-01 -7.383225e-01 -8.719308e-01   \n",
       "50%    1.000000e+00  8.533714e-01 -5.415563e-05 -2.410638e-04   \n",
       "75%    1.000000e+00  1.236226e+00  7.382142e-01  8.709940e-01   \n",
       "max    1.000000e+00  1.209891e+01  2.434868e+00  1.743236e+00   \n",
       "\n",
       "       missing_energy_magnitude  missing_energy_phi      jet_1_pt  \\\n",
       "count              1.100000e+07        1.100000e+07  1.100000e+07   \n",
       "mean               9.985364e-01        2.613459e-05  9.909152e-01   \n",
       "std                6.000185e-01        1.006326e+00  4.749747e-01   \n",
       "min                2.370088e-04       -1.743944e+00  1.375024e-01   \n",
       "25%                5.768156e-01       -8.712081e-01  6.789927e-01   \n",
       "50%                8.916277e-01        2.125454e-04  8.948193e-01   \n",
       "75%                1.293056e+00        8.714708e-01  1.170740e+00   \n",
       "max                1.539682e+01        1.743257e+00  9.940391e+00   \n",
       "\n",
       "          jet_1_eta     jet_1_phi    jet_1_btag      jet_2_pt     jet_2_eta  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean  -2.027520e-05  7.716199e-06  9.999687e-01  9.927294e-01 -1.026444e-05   \n",
       "std    1.009303e+00  1.005901e+00  1.027808e+00  4.999939e-01  1.009331e+00   \n",
       "min   -2.969725e+00 -1.741237e+00  0.000000e+00  1.889811e-01 -2.913090e+00   \n",
       "25%   -6.872450e-01 -8.680962e-01  0.000000e+00  6.564608e-01 -6.944718e-01   \n",
       "50%   -2.543566e-05  5.813991e-05  1.086538e+00  8.901377e-01  6.027267e-05   \n",
       "75%    6.871941e-01  8.683126e-01  2.173076e+00  1.201875e+00  6.945924e-01   \n",
       "max    2.969674e+00  1.741454e+00  2.173076e+00  1.164708e+01  2.913210e+00   \n",
       "\n",
       "          jet_2_phi    jet_2_btag      jet_3_pt     jet_3_eta     jet_3_phi  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean  -2.076887e-05  1.000008e+00  9.922591e-01  1.459561e-05  3.678632e-06   \n",
       "std    1.006154e+00  1.049398e+00  4.876623e-01  1.008747e+00  1.006305e+00   \n",
       "min   -1.742372e+00  0.000000e+00  2.636076e-01 -2.729663e+00 -1.742069e+00   \n",
       "25%   -8.701791e-01  0.000000e+00  6.508527e-01 -6.998083e-01 -8.711343e-01   \n",
       "50%    3.514990e-04  0.000000e+00  8.972494e-01  1.728937e-04 -7.519117e-04   \n",
       "75%    8.698727e-01  2.214872e+00  1.221798e+00  7.001541e-01  8.713947e-01   \n",
       "max    1.743175e+00  2.214872e+00  1.470899e+01  2.730009e+00  1.742884e+00   \n",
       "\n",
       "         jet_3_btag      jet_4_pt     jet_4_eta     jet_4_phi    jet_4_btag  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   1.000011e+00  9.861087e-01 -5.756954e-06  1.744903e-05  1.000000e+00   \n",
       "std    1.193676e+00  5.057777e-01  1.007694e+00  1.006366e+00  1.400209e+00   \n",
       "min    0.000000e+00  3.653542e-01 -2.497265e+00 -1.742691e+00  0.000000e+00   \n",
       "25%    0.000000e+00  6.177673e-01 -7.141902e-01 -8.714789e-01  0.000000e+00   \n",
       "50%    0.000000e+00  8.682333e-01  3.721330e-04 -2.642369e-04  0.000000e+00   \n",
       "75%    2.548224e+00  1.220930e+00  7.141017e-01  8.716055e-01  3.101961e+00   \n",
       "max    2.548224e+00  1.288257e+01  2.498009e+00  1.743372e+00  3.101961e+00   \n",
       "\n",
       "               m_jj         m_jjj          m_lv         m_jlv          m_bb  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   1.034290e+00  1.024805e+00  1.050554e+00  1.009742e+00  9.729596e-01   \n",
       "std    6.746354e-01  3.808074e-01  1.645763e-01  3.974453e-01  5.254063e-01   \n",
       "min    7.507046e-02  1.986757e-01  8.304866e-02  1.320062e-01  4.786215e-02   \n",
       "25%    7.906095e-01  8.462266e-01  9.857525e-01  7.675732e-01  6.738168e-01   \n",
       "50%    8.949304e-01  9.506853e-01  9.897798e-01  9.165110e-01  8.733798e-01   \n",
       "75%    1.024730e+00  1.083493e+00  1.020528e+00  1.142226e+00  1.138439e+00   \n",
       "max    4.019237e+01  2.037278e+01  7.992739e+00  1.426244e+01  1.776285e+01   \n",
       "\n",
       "              m_wbb        m_wwbb  \n",
       "count  1.100000e+07  1.100000e+07  \n",
       "mean   1.033036e+00  9.598120e-01  \n",
       "std    3.652556e-01  3.133378e-01  \n",
       "min    2.951122e-01  3.307214e-01  \n",
       "25%    8.193964e-01  7.703901e-01  \n",
       "50%    9.473447e-01  8.719701e-01  \n",
       "75%    1.140458e+00  1.059248e+00  \n",
       "max    1.149652e+01  8.374498e+00  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#figure out which columns have values strictly greater than 0, for scaler purposes\n",
    "pd.set_option(\"display.max_rows\", 500, \"display.max_columns\", None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600000, 29)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "pre_X = train.loc[:, df.columns != 'target']\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_train = scaler.fit_transform(pre_X)\n",
    "# X = pd.DataFrame(data=scaled_train, columns=pre_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.695209e-01</td>\n",
       "      <td>-6.625666e-01</td>\n",
       "      <td>-7.968404e-01</td>\n",
       "      <td>-6.073939e-01</td>\n",
       "      <td>-2.264619e-01</td>\n",
       "      <td>-4.952153e-01</td>\n",
       "      <td>-4.221755e-01</td>\n",
       "      <td>-1.084725e+00</td>\n",
       "      <td>-4.670169e+00</td>\n",
       "      <td>-9.966000e-01</td>\n",
       "      <td>-7.556114e-01</td>\n",
       "      <td>-9.412959e-01</td>\n",
       "      <td>-9.336352e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.906835e-01</td>\n",
       "      <td>2.969626e-01</td>\n",
       "      <td>3.428450e-01</td>\n",
       "      <td>3.272016e-01</td>\n",
       "      <td>2.719195e-01</td>\n",
       "      <td>2.992355e-01</td>\n",
       "      <td>6.380529e-01</td>\n",
       "      <td>5.307856e-01</td>\n",
       "      <td>6.057191e-01</td>\n",
       "      <td>3.903798e-01</td>\n",
       "      <td>4.299310e-01</td>\n",
       "      <td>4.150297e-01</td>\n",
       "      <td>3.953860e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.556306e-01</td>\n",
       "      <td>8.220505e-01</td>\n",
       "      <td>7.978708e-01</td>\n",
       "      <td>7.947510e-01</td>\n",
       "      <td>7.664560e-01</td>\n",
       "      <td>8.050897e-01</td>\n",
       "      <td>7.926677e-01</td>\n",
       "      <td>8.047199e-01</td>\n",
       "      <td>6.301876e-01</td>\n",
       "      <td>7.658091e-01</td>\n",
       "      <td>8.108293e-01</td>\n",
       "      <td>7.650499e-01</td>\n",
       "      <td>7.195430e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.432801e+00</td>\n",
       "      <td>1.489800e+00</td>\n",
       "      <td>1.378221e+00</td>\n",
       "      <td>1.418066e+00</td>\n",
       "      <td>1.464286e+00</td>\n",
       "      <td>1.471996e+00</td>\n",
       "      <td>9.855056e-01</td>\n",
       "      <td>1.154184e+00</td>\n",
       "      <td>8.183915e-01</td>\n",
       "      <td>1.333369e+00</td>\n",
       "      <td>1.315954e+00</td>\n",
       "      <td>1.293396e+00</td>\n",
       "      <td>1.316636e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.991209e+01</td>\n",
       "      <td>2.073755e+01</td>\n",
       "      <td>1.696594e+01</td>\n",
       "      <td>1.894311e+01</td>\n",
       "      <td>2.448835e+01</td>\n",
       "      <td>2.914014e+01</td>\n",
       "      <td>4.321080e+01</td>\n",
       "      <td>3.756345e+01</td>\n",
       "      <td>3.609834e+01</td>\n",
       "      <td>2.720432e+01</td>\n",
       "      <td>2.530246e+01</td>\n",
       "      <td>2.221914e+01</td>\n",
       "      <td>1.902509e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lepton_ph  missing_energy_magnitude      jet_1_pt      jet_2_pt  \\\n",
       "count  2.600000e+06              2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.000000e+00              1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00              1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.695209e-01             -6.625666e-01 -7.968404e-01 -6.073939e-01   \n",
       "25%    2.906835e-01              2.969626e-01  3.428450e-01  3.272016e-01   \n",
       "50%    7.556306e-01              8.220505e-01  7.978708e-01  7.947510e-01   \n",
       "75%    1.432801e+00              1.489800e+00  1.378221e+00  1.418066e+00   \n",
       "max    1.991209e+01              2.073755e+01  1.696594e+01  1.894311e+01   \n",
       "\n",
       "           jet_4_pt      jet_3_pt          m_jj         m_jjj          m_lv  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.264619e-01 -4.952153e-01 -4.221755e-01 -1.084725e+00 -4.670169e+00   \n",
       "25%    2.719195e-01  2.992355e-01  6.380529e-01  5.307856e-01  6.057191e-01   \n",
       "50%    7.664560e-01  8.050897e-01  7.926677e-01  8.047199e-01  6.301876e-01   \n",
       "75%    1.464286e+00  1.471996e+00  9.855056e-01  1.154184e+00  8.183915e-01   \n",
       "max    2.448835e+01  2.914014e+01  4.321080e+01  3.756345e+01  3.609834e+01   \n",
       "\n",
       "              m_jlv          m_bb         m_wbb        m_wwbb  \n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "min   -9.966000e-01 -7.556114e-01 -9.412959e-01 -9.336352e-01  \n",
       "25%    3.903798e-01  4.299310e-01  4.150297e-01  3.953860e-01  \n",
       "50%    7.658091e-01  8.108293e-01  7.650499e-01  7.195430e-01  \n",
       "75%    1.333369e+00  1.315954e+00  1.293396e+00  1.316636e+00  \n",
       "max    2.720432e+01  2.530246e+01  2.221914e+01  1.902509e+01  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not complete, need columns where mean=1 and stdev=1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#NOT strictly greater than 0 columns:\n",
    "not_greater_than_0 = ['lepton_eta','lepton_phi','missing_energy_phi','jet_1_eta','jet_1_phi','jet_1_btag',\n",
    "                      'jet_2_eta','jet_2_phi','jet_2_btag','jet_3_eta','jet_3_phi','jet_3_btag','jet_4_eta',\n",
    "                      'jet_4_phi','jet_4_btag']\n",
    "\n",
    "#strictly greater than 0 columns:\n",
    "greater_than_0 = ['lepton_ph','missing_energy_magnitude','jet_1_pt','jet_2_pt','jet_4_pt','jet_3_pt','m_jj','m_jjj','m_lv',\n",
    "                  'm_jlv','m_bb','m_wbb','m_wwbb']\n",
    "\n",
    "\n",
    "#these columns scale where mean=0 and stdev=1\n",
    "to_scale1 = pre_X[not_greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train1 = scaler.fit_transform(to_scale1)\n",
    "scaled_train_df1 = pd.DataFrame(scaled_train1, columns=not_greater_than_0)\n",
    "\n",
    "#these columns scale where mean=1 and stdev=1\n",
    "to_scale2 = pre_X[greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train2 = scaler.fit_transform(to_scale2)\n",
    "scaled_train_df2 = pd.DataFrame(scaled_train2 + 1, columns=greater_than_0)\n",
    "scaled_train_df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600000, 13)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled_train2 + 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_X[greater_than_0].to_numpy().shape\n",
    "\n",
    "# np_greater_than_0 = pre_X[greater_than_0].to_numpy()\n",
    "\n",
    "def manual_scaling(x, desired_mean=0, desired_std=1):\n",
    "    new_x = np.empty(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        mean = np.mean(x[:, i])\n",
    "        std = np.std(x[:, i])\n",
    "        new_x[:, i] = desired_mean + (x[:, i] - mean) * (desired_std / std)\n",
    "    return new_x\n",
    "\n",
    "np_greater_than_0 = manual_scaling(pre_X[greater_than_0].to_numpy(), desired_mean=1, desired_std=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.175,  0.16 , -0.132, ...,  1.105,  1.272,  0.858],\n",
       "       [ 0.51 ,  4.281,  1.223, ...,  0.709,  1.38 ,  1.777],\n",
       "       [-0.131,  0.97 ,  0.063, ...,  0.414,  0.217,  0.345],\n",
       "       ...,\n",
       "       [ 0.141,  0.259,  1.578, ...,  0.017,  1.233,  0.923],\n",
       "       [ 1.813,  0.645,  1.675, ...,  1.379,  1.363,  1.836],\n",
       "       [ 0.164,  0.846,  1.208, ...,  1.776,  0.987,  0.697]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round((np_greater_than_0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.175,  0.16 , -0.132, ...,  1.105,  1.272,  0.858],\n",
       "       [ 0.51 ,  4.281,  1.223, ...,  0.709,  1.38 ,  1.777],\n",
       "       [-0.131,  0.97 ,  0.063, ...,  0.414,  0.217,  0.345],\n",
       "       ...,\n",
       "       [ 0.141,  0.259,  1.578, ...,  0.017,  1.233,  0.923],\n",
       "       [ 1.813,  0.645,  1.675, ...,  1.379,  1.363,  1.836],\n",
       "       [ 0.164,  0.846,  1.208, ...,  1.776,  0.987,  0.697]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round((scaled_train2 + 1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_they_the_same = np.round((np_greater_than_0), 3) == np.round((scaled_train2 + 1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2600000, 2600000, 2600000, 2600000, 2600000, 2600000, 2600000,\n",
       "       2600000, 2600000, 2600000, 2600000, 2600000, 2600000])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(are_they_the_same == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.322691</td>\n",
       "      <td>0.863880</td>\n",
       "      <td>-0.572453</td>\n",
       "      <td>-2.141693</td>\n",
       "      <td>-0.523203</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>1.723081</td>\n",
       "      <td>-1.709792</td>\n",
       "      <td>-0.952660</td>\n",
       "      <td>0.025451</td>\n",
       "      <td>1.289919</td>\n",
       "      <td>1.296983</td>\n",
       "      <td>-0.002654</td>\n",
       "      <td>-0.786827</td>\n",
       "      <td>-0.714619</td>\n",
       "      <td>1.174569</td>\n",
       "      <td>0.160122</td>\n",
       "      <td>-0.132040</td>\n",
       "      <td>0.571042</td>\n",
       "      <td>0.458873</td>\n",
       "      <td>0.872105</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>1.043547</td>\n",
       "      <td>0.613459</td>\n",
       "      <td>0.855754</td>\n",
       "      <td>1.105013</td>\n",
       "      <td>1.271511</td>\n",
       "      <td>0.857620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416089</td>\n",
       "      <td>0.695152</td>\n",
       "      <td>0.802544</td>\n",
       "      <td>-0.470049</td>\n",
       "      <td>-0.668134</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>-0.789944</td>\n",
       "      <td>-1.505140</td>\n",
       "      <td>1.158021</td>\n",
       "      <td>0.802190</td>\n",
       "      <td>-0.192560</td>\n",
       "      <td>-0.837770</td>\n",
       "      <td>0.480915</td>\n",
       "      <td>1.476132</td>\n",
       "      <td>-0.714619</td>\n",
       "      <td>0.509711</td>\n",
       "      <td>4.281044</td>\n",
       "      <td>1.223397</td>\n",
       "      <td>2.620156</td>\n",
       "      <td>1.344617</td>\n",
       "      <td>2.941280</td>\n",
       "      <td>0.587525</td>\n",
       "      <td>0.553713</td>\n",
       "      <td>0.652878</td>\n",
       "      <td>2.778725</td>\n",
       "      <td>0.708706</td>\n",
       "      <td>1.379553</td>\n",
       "      <td>1.777043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.014625</td>\n",
       "      <td>0.186212</td>\n",
       "      <td>-1.239583</td>\n",
       "      <td>1.129981</td>\n",
       "      <td>-1.392778</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>1.064422</td>\n",
       "      <td>-0.952660</td>\n",
       "      <td>-0.055741</td>\n",
       "      <td>-0.050936</td>\n",
       "      <td>1.296983</td>\n",
       "      <td>2.108518</td>\n",
       "      <td>-0.857386</td>\n",
       "      <td>-0.714619</td>\n",
       "      <td>-0.131495</td>\n",
       "      <td>0.969540</td>\n",
       "      <td>0.063081</td>\n",
       "      <td>1.850889</td>\n",
       "      <td>1.313378</td>\n",
       "      <td>2.026490</td>\n",
       "      <td>0.902127</td>\n",
       "      <td>1.344002</td>\n",
       "      <td>0.611636</td>\n",
       "      <td>0.129997</td>\n",
       "      <td>0.414243</td>\n",
       "      <td>0.217026</td>\n",
       "      <td>0.345026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.836948</td>\n",
       "      <td>-1.475806</td>\n",
       "      <td>0.854693</td>\n",
       "      <td>2.403334</td>\n",
       "      <td>-1.289730</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>0.352078</td>\n",
       "      <td>0.411301</td>\n",
       "      <td>1.158021</td>\n",
       "      <td>-0.669193</td>\n",
       "      <td>0.629738</td>\n",
       "      <td>-0.837770</td>\n",
       "      <td>0.763617</td>\n",
       "      <td>-0.477577</td>\n",
       "      <td>1.500372</td>\n",
       "      <td>0.974982</td>\n",
       "      <td>-0.110502</td>\n",
       "      <td>0.356920</td>\n",
       "      <td>0.772858</td>\n",
       "      <td>2.591291</td>\n",
       "      <td>1.104855</td>\n",
       "      <td>0.865492</td>\n",
       "      <td>1.335313</td>\n",
       "      <td>0.656397</td>\n",
       "      <td>1.385766</td>\n",
       "      <td>0.819796</td>\n",
       "      <td>1.213709</td>\n",
       "      <td>0.937928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.177572</td>\n",
       "      <td>0.531387</td>\n",
       "      <td>-1.002016</td>\n",
       "      <td>0.825868</td>\n",
       "      <td>0.737725</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>1.490251</td>\n",
       "      <td>-0.738936</td>\n",
       "      <td>-0.952660</td>\n",
       "      <td>-0.136933</td>\n",
       "      <td>1.334005</td>\n",
       "      <td>1.296983</td>\n",
       "      <td>1.046319</td>\n",
       "      <td>-0.080681</td>\n",
       "      <td>0.392877</td>\n",
       "      <td>-0.051790</td>\n",
       "      <td>2.577822</td>\n",
       "      <td>0.874030</td>\n",
       "      <td>0.138219</td>\n",
       "      <td>1.611349</td>\n",
       "      <td>0.954612</td>\n",
       "      <td>0.795040</td>\n",
       "      <td>1.126510</td>\n",
       "      <td>1.901067</td>\n",
       "      <td>2.499051</td>\n",
       "      <td>1.154495</td>\n",
       "      <td>1.196558</td>\n",
       "      <td>1.717747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton_eta  lepton_phi  missing_energy_phi  jet_1_eta  jet_1_phi  \\\n",
       "0   -0.322691    0.863880           -0.572453  -2.141693  -0.523203   \n",
       "1    0.416089    0.695152            0.802544  -0.470049  -0.668134   \n",
       "2   -0.014625    0.186212           -1.239583   1.129981  -1.392778   \n",
       "3   -1.836948   -1.475806            0.854693   2.403334  -1.289730   \n",
       "4    2.177572    0.531387           -1.002016   0.825868   0.737725   \n",
       "\n",
       "   jet_1_btag  jet_2_eta  jet_2_phi  jet_2_btag  jet_3_eta  jet_3_phi  \\\n",
       "0   -0.972454   1.723081  -1.709792   -0.952660   0.025451   1.289919   \n",
       "1   -0.972454  -0.789944  -1.505140    1.158021   0.802190  -0.192560   \n",
       "2   -0.972454   0.016302   1.064422   -0.952660  -0.055741  -0.050936   \n",
       "3   -0.972454   0.352078   0.411301    1.158021  -0.669193   0.629738   \n",
       "4   -0.972454   1.490251  -0.738936   -0.952660  -0.136933   1.334005   \n",
       "\n",
       "   jet_3_btag  jet_4_eta  jet_4_phi  jet_4_btag  lepton_ph  \\\n",
       "0    1.296983  -0.002654  -0.786827   -0.714619   1.174569   \n",
       "1   -0.837770   0.480915   1.476132   -0.714619   0.509711   \n",
       "2    1.296983   2.108518  -0.857386   -0.714619  -0.131495   \n",
       "3   -0.837770   0.763617  -0.477577    1.500372   0.974982   \n",
       "4    1.296983   1.046319  -0.080681    0.392877  -0.051790   \n",
       "\n",
       "   missing_energy_magnitude  jet_1_pt  jet_2_pt  jet_4_pt  jet_3_pt      m_jj  \\\n",
       "0                  0.160122 -0.132040  0.571042  0.458873  0.872105  0.537537   \n",
       "1                  4.281044  1.223397  2.620156  1.344617  2.941280  0.587525   \n",
       "2                  0.969540  0.063081  1.850889  1.313378  2.026490  0.902127   \n",
       "3                 -0.110502  0.356920  0.772858  2.591291  1.104855  0.865492   \n",
       "4                  2.577822  0.874030  0.138219  1.611349  0.954612  0.795040   \n",
       "\n",
       "      m_jjj      m_lv     m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "0  1.043547  0.613459  0.855754  1.105013  1.271511  0.857620  \n",
       "1  0.553713  0.652878  2.778725  0.708706  1.379553  1.777043  \n",
       "2  1.344002  0.611636  0.129997  0.414243  0.217026  0.345026  \n",
       "3  1.335313  0.656397  1.385766  0.819796  1.213709  0.937928  \n",
       "4  1.126510  1.901067  2.499051  1.154495  1.196558  1.717747  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([scaled_train_df1, scaled_train_df2], axis=1, sort=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=500000, random_state=1776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = y_train.to_numpy()\n",
    "# type(np.unique(y)[0])\n",
    "\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100000, 28)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.131178e-17</td>\n",
       "      <td>8.924613e-17</td>\n",
       "      <td>-3.083901e-17</td>\n",
       "      <td>4.807138e-18</td>\n",
       "      <td>-4.718029e-17</td>\n",
       "      <td>9.236995e-15</td>\n",
       "      <td>-8.113305e-17</td>\n",
       "      <td>-2.608358e-17</td>\n",
       "      <td>2.392281e-15</td>\n",
       "      <td>2.121924e-16</td>\n",
       "      <td>-2.010950e-16</td>\n",
       "      <td>1.225050e-14</td>\n",
       "      <td>-8.899181e-17</td>\n",
       "      <td>2.780400e-17</td>\n",
       "      <td>-2.669054e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.414452e+00</td>\n",
       "      <td>-1.731104e+00</td>\n",
       "      <td>-1.731925e+00</td>\n",
       "      <td>-2.942199e+00</td>\n",
       "      <td>-1.730027e+00</td>\n",
       "      <td>-9.724543e-01</td>\n",
       "      <td>-2.885415e+00</td>\n",
       "      <td>-1.732408e+00</td>\n",
       "      <td>-9.526596e-01</td>\n",
       "      <td>-2.705312e+00</td>\n",
       "      <td>-1.730599e+00</td>\n",
       "      <td>-8.377697e-01</td>\n",
       "      <td>-2.479189e+00</td>\n",
       "      <td>-1.731107e+00</td>\n",
       "      <td>-7.146187e-01</td>\n",
       "      <td>-2.695209e-01</td>\n",
       "      <td>-6.625666e-01</td>\n",
       "      <td>-7.968404e-01</td>\n",
       "      <td>-6.073939e-01</td>\n",
       "      <td>-2.264619e-01</td>\n",
       "      <td>-4.952153e-01</td>\n",
       "      <td>-4.221755e-01</td>\n",
       "      <td>-1.084725e+00</td>\n",
       "      <td>-4.670169e+00</td>\n",
       "      <td>-9.966000e-01</td>\n",
       "      <td>-7.556114e-01</td>\n",
       "      <td>-9.412959e-01</td>\n",
       "      <td>-9.336352e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.321584e-01</td>\n",
       "      <td>-8.659604e-01</td>\n",
       "      <td>-8.658190e-01</td>\n",
       "      <td>-6.819477e-01</td>\n",
       "      <td>-8.632094e-01</td>\n",
       "      <td>-9.724543e-01</td>\n",
       "      <td>-6.889230e-01</td>\n",
       "      <td>-8.647062e-01</td>\n",
       "      <td>-9.526596e-01</td>\n",
       "      <td>-6.935506e-01</td>\n",
       "      <td>-8.659706e-01</td>\n",
       "      <td>-8.377697e-01</td>\n",
       "      <td>-7.085818e-01</td>\n",
       "      <td>-8.667571e-01</td>\n",
       "      <td>-7.146187e-01</td>\n",
       "      <td>2.906835e-01</td>\n",
       "      <td>2.969626e-01</td>\n",
       "      <td>3.428450e-01</td>\n",
       "      <td>3.272016e-01</td>\n",
       "      <td>2.719195e-01</td>\n",
       "      <td>2.992355e-01</td>\n",
       "      <td>6.380529e-01</td>\n",
       "      <td>5.307856e-01</td>\n",
       "      <td>6.057191e-01</td>\n",
       "      <td>3.903798e-01</td>\n",
       "      <td>4.299310e-01</td>\n",
       "      <td>4.150297e-01</td>\n",
       "      <td>3.953860e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.386904e-04</td>\n",
       "      <td>-1.365973e-03</td>\n",
       "      <td>2.090639e-04</td>\n",
       "      <td>-1.125500e-03</td>\n",
       "      <td>-7.977719e-04</td>\n",
       "      <td>8.484855e-02</td>\n",
       "      <td>9.082319e-04</td>\n",
       "      <td>8.939700e-04</td>\n",
       "      <td>-9.526596e-01</td>\n",
       "      <td>1.093501e-03</td>\n",
       "      <td>3.138265e-04</td>\n",
       "      <td>-8.377697e-01</td>\n",
       "      <td>6.528141e-04</td>\n",
       "      <td>-1.998949e-04</td>\n",
       "      <td>-7.146187e-01</td>\n",
       "      <td>7.556306e-01</td>\n",
       "      <td>8.220505e-01</td>\n",
       "      <td>7.978708e-01</td>\n",
       "      <td>7.947510e-01</td>\n",
       "      <td>7.664560e-01</td>\n",
       "      <td>8.050897e-01</td>\n",
       "      <td>7.926677e-01</td>\n",
       "      <td>8.047199e-01</td>\n",
       "      <td>6.301876e-01</td>\n",
       "      <td>7.658091e-01</td>\n",
       "      <td>8.108293e-01</td>\n",
       "      <td>7.650499e-01</td>\n",
       "      <td>7.195430e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.318811e-01</td>\n",
       "      <td>8.655338e-01</td>\n",
       "      <td>8.662048e-01</td>\n",
       "      <td>6.806777e-01</td>\n",
       "      <td>8.628153e-01</td>\n",
       "      <td>1.142151e+00</td>\n",
       "      <td>6.888153e-01</td>\n",
       "      <td>8.641829e-01</td>\n",
       "      <td>1.158021e+00</td>\n",
       "      <td>6.939334e-01</td>\n",
       "      <td>8.661466e-01</td>\n",
       "      <td>1.296983e+00</td>\n",
       "      <td>7.082342e-01</td>\n",
       "      <td>8.659056e-01</td>\n",
       "      <td>1.500372e+00</td>\n",
       "      <td>1.432801e+00</td>\n",
       "      <td>1.489800e+00</td>\n",
       "      <td>1.378221e+00</td>\n",
       "      <td>1.418066e+00</td>\n",
       "      <td>1.464286e+00</td>\n",
       "      <td>1.471996e+00</td>\n",
       "      <td>9.855056e-01</td>\n",
       "      <td>1.154184e+00</td>\n",
       "      <td>8.183915e-01</td>\n",
       "      <td>1.333369e+00</td>\n",
       "      <td>1.315954e+00</td>\n",
       "      <td>1.293396e+00</td>\n",
       "      <td>1.316636e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.414175e+00</td>\n",
       "      <td>1.732883e+00</td>\n",
       "      <td>1.732090e+00</td>\n",
       "      <td>2.941910e+00</td>\n",
       "      <td>1.731837e+00</td>\n",
       "      <td>1.142151e+00</td>\n",
       "      <td>2.885307e+00</td>\n",
       "      <td>1.732988e+00</td>\n",
       "      <td>1.158021e+00</td>\n",
       "      <td>2.705695e+00</td>\n",
       "      <td>1.731326e+00</td>\n",
       "      <td>1.296983e+00</td>\n",
       "      <td>2.478841e+00</td>\n",
       "      <td>1.731909e+00</td>\n",
       "      <td>1.500372e+00</td>\n",
       "      <td>1.991209e+01</td>\n",
       "      <td>2.073755e+01</td>\n",
       "      <td>1.696594e+01</td>\n",
       "      <td>1.894311e+01</td>\n",
       "      <td>2.448835e+01</td>\n",
       "      <td>2.914014e+01</td>\n",
       "      <td>4.321080e+01</td>\n",
       "      <td>3.756345e+01</td>\n",
       "      <td>3.609834e+01</td>\n",
       "      <td>2.720432e+01</td>\n",
       "      <td>2.530246e+01</td>\n",
       "      <td>2.221914e+01</td>\n",
       "      <td>1.902509e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lepton_eta    lepton_phi  missing_energy_phi     jet_1_eta  \\\n",
       "count  2.600000e+06  2.600000e+06        2.600000e+06  2.600000e+06   \n",
       "mean   4.131178e-17  8.924613e-17       -3.083901e-17  4.807138e-18   \n",
       "std    1.000000e+00  1.000000e+00        1.000000e+00  1.000000e+00   \n",
       "min   -2.414452e+00 -1.731104e+00       -1.731925e+00 -2.942199e+00   \n",
       "25%   -7.321584e-01 -8.659604e-01       -8.658190e-01 -6.819477e-01   \n",
       "50%   -1.386904e-04 -1.365973e-03        2.090639e-04 -1.125500e-03   \n",
       "75%    7.318811e-01  8.655338e-01        8.662048e-01  6.806777e-01   \n",
       "max    2.414175e+00  1.732883e+00        1.732090e+00  2.941910e+00   \n",
       "\n",
       "          jet_1_phi    jet_1_btag     jet_2_eta     jet_2_phi    jet_2_btag  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean  -4.718029e-17  9.236995e-15 -8.113305e-17 -2.608358e-17  2.392281e-15   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -1.730027e+00 -9.724543e-01 -2.885415e+00 -1.732408e+00 -9.526596e-01   \n",
       "25%   -8.632094e-01 -9.724543e-01 -6.889230e-01 -8.647062e-01 -9.526596e-01   \n",
       "50%   -7.977719e-04  8.484855e-02  9.082319e-04  8.939700e-04 -9.526596e-01   \n",
       "75%    8.628153e-01  1.142151e+00  6.888153e-01  8.641829e-01  1.158021e+00   \n",
       "max    1.731837e+00  1.142151e+00  2.885307e+00  1.732988e+00  1.158021e+00   \n",
       "\n",
       "          jet_3_eta     jet_3_phi    jet_3_btag     jet_4_eta     jet_4_phi  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   2.121924e-16 -2.010950e-16  1.225050e-14 -8.899181e-17  2.780400e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.705312e+00 -1.730599e+00 -8.377697e-01 -2.479189e+00 -1.731107e+00   \n",
       "25%   -6.935506e-01 -8.659706e-01 -8.377697e-01 -7.085818e-01 -8.667571e-01   \n",
       "50%    1.093501e-03  3.138265e-04 -8.377697e-01  6.528141e-04 -1.998949e-04   \n",
       "75%    6.939334e-01  8.661466e-01  1.296983e+00  7.082342e-01  8.659056e-01   \n",
       "max    2.705695e+00  1.731326e+00  1.296983e+00  2.478841e+00  1.731909e+00   \n",
       "\n",
       "         jet_4_btag     lepton_ph  missing_energy_magnitude      jet_1_pt  \\\n",
       "count  2.600000e+06  2.600000e+06              2.600000e+06  2.600000e+06   \n",
       "mean  -2.669054e-16  1.000000e+00              1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00  1.000000e+00              1.000000e+00  1.000000e+00   \n",
       "min   -7.146187e-01 -2.695209e-01             -6.625666e-01 -7.968404e-01   \n",
       "25%   -7.146187e-01  2.906835e-01              2.969626e-01  3.428450e-01   \n",
       "50%   -7.146187e-01  7.556306e-01              8.220505e-01  7.978708e-01   \n",
       "75%    1.500372e+00  1.432801e+00              1.489800e+00  1.378221e+00   \n",
       "max    1.500372e+00  1.991209e+01              2.073755e+01  1.696594e+01   \n",
       "\n",
       "           jet_2_pt      jet_4_pt      jet_3_pt          m_jj         m_jjj  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -6.073939e-01 -2.264619e-01 -4.952153e-01 -4.221755e-01 -1.084725e+00   \n",
       "25%    3.272016e-01  2.719195e-01  2.992355e-01  6.380529e-01  5.307856e-01   \n",
       "50%    7.947510e-01  7.664560e-01  8.050897e-01  7.926677e-01  8.047199e-01   \n",
       "75%    1.418066e+00  1.464286e+00  1.471996e+00  9.855056e-01  1.154184e+00   \n",
       "max    1.894311e+01  2.448835e+01  2.914014e+01  4.321080e+01  3.756345e+01   \n",
       "\n",
       "               m_lv         m_jlv          m_bb         m_wbb        m_wwbb  \n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "min   -4.670169e+00 -9.966000e-01 -7.556114e-01 -9.412959e-01 -9.336352e-01  \n",
       "25%    6.057191e-01  3.903798e-01  4.299310e-01  4.150297e-01  3.953860e-01  \n",
       "50%    6.301876e-01  7.658091e-01  8.108293e-01  7.650499e-01  7.195430e-01  \n",
       "75%    8.183915e-01  1.333369e+00  1.315954e+00  1.293396e+00  1.316636e+00  \n",
       "max    3.609834e+01  2.720432e+01  2.530246e+01  2.221914e+01  1.902509e+01  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# input\n",
    "model.add(tf.keras.Input(shape=(28,)))\n",
    "# hidden\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.1)))  \n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(1,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.001)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=1e-5)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "21000/21000 [==============================] - 62s 3ms/step - loss: 0.5690 - auc: 0.7712 - val_loss: 0.5400 - val_auc: 0.8027\n",
      "Epoch 2/250\n",
      "21000/21000 [==============================] - 32624s 2s/step - loss: 0.5274 - auc: 0.8106 - val_loss: 0.5220 - val_auc: 0.8157\n",
      "Epoch 3/250\n",
      "21000/21000 [==============================] - 43s 2ms/step - loss: 0.5142 - auc: 0.8215 - val_loss: 0.5125 - val_auc: 0.8251\n",
      "Epoch 4/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.5059 - auc: 0.8280 - val_loss: 0.5031 - val_auc: 0.8307\n",
      "Epoch 5/250\n",
      "21000/21000 [==============================] - 57s 3ms/step - loss: 0.5002 - auc: 0.8324 - val_loss: 0.4973 - val_auc: 0.8349\n",
      "Epoch 6/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4958 - auc: 0.8358 - val_loss: 0.4955 - val_auc: 0.8360\n",
      "Epoch 7/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4924 - auc: 0.8383 - val_loss: 0.4926 - val_auc: 0.8387\n",
      "Epoch 8/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4896 - auc: 0.8405 - val_loss: 0.4941 - val_auc: 0.8383\n",
      "Epoch 9/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4873 - auc: 0.8423 - val_loss: 0.4903 - val_auc: 0.8402\n",
      "Epoch 10/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4851 - auc: 0.8438 - val_loss: 0.4880 - val_auc: 0.8415\n",
      "Epoch 11/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4832 - auc: 0.8452 - val_loss: 0.4867 - val_auc: 0.8426\n",
      "Epoch 12/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4815 - auc: 0.8464 - val_loss: 0.4878 - val_auc: 0.8417\n",
      "Epoch 13/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4799 - auc: 0.8476 - val_loss: 0.4880 - val_auc: 0.8435\n",
      "Epoch 14/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4783 - auc: 0.8487 - val_loss: 0.4856 - val_auc: 0.8437\n",
      "Epoch 15/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4769 - auc: 0.8497 - val_loss: 0.4863 - val_auc: 0.8434\n",
      "Epoch 16/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4756 - auc: 0.8507 - val_loss: 0.4861 - val_auc: 0.8438\n",
      "Epoch 17/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4742 - auc: 0.8517 - val_loss: 0.4835 - val_auc: 0.8452\n",
      "Epoch 18/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4729 - auc: 0.8526 - val_loss: 0.4861 - val_auc: 0.8454\n",
      "Epoch 19/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4716 - auc: 0.8535 - val_loss: 0.4828 - val_auc: 0.8456\n",
      "Epoch 20/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4705 - auc: 0.8543 - val_loss: 0.4826 - val_auc: 0.8459\n",
      "Epoch 21/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4694 - auc: 0.8551 - val_loss: 0.4843 - val_auc: 0.8453\n",
      "Epoch 22/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4683 - auc: 0.8558 - val_loss: 0.4844 - val_auc: 0.8450\n",
      "Epoch 23/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4671 - auc: 0.8567 - val_loss: 0.4811 - val_auc: 0.8470\n",
      "Epoch 24/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4661 - auc: 0.8573 - val_loss: 0.4846 - val_auc: 0.8454\n",
      "Epoch 25/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4650 - auc: 0.8581 - val_loss: 0.4832 - val_auc: 0.8460\n",
      "Epoch 26/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4640 - auc: 0.8588 - val_loss: 0.4813 - val_auc: 0.8474\n",
      "Epoch 27/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4630 - auc: 0.8595 - val_loss: 0.4831 - val_auc: 0.8460\n",
      "Epoch 28/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4619 - auc: 0.8602 - val_loss: 0.4824 - val_auc: 0.8466\n",
      "Epoch 29/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4608 - auc: 0.8610 - val_loss: 0.4830 - val_auc: 0.8459\n",
      "Epoch 30/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4599 - auc: 0.8616 - val_loss: 0.4828 - val_auc: 0.8467\n",
      "Epoch 31/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4588 - auc: 0.8623 - val_loss: 0.4825 - val_auc: 0.8464\n",
      "Epoch 32/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4579 - auc: 0.8629 - val_loss: 0.4855 - val_auc: 0.8455\n",
      "Epoch 33/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4570 - auc: 0.8635 - val_loss: 0.4844 - val_auc: 0.8457\n",
      "Epoch 34/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4560 - auc: 0.8642 - val_loss: 0.4831 - val_auc: 0.8464\n",
      "Epoch 35/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4550 - auc: 0.8649 - val_loss: 0.4841 - val_auc: 0.8458\n",
      "Epoch 36/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4539 - auc: 0.8656 - val_loss: 0.4874 - val_auc: 0.8440\n",
      "Epoch 37/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4531 - auc: 0.8661 - val_loss: 0.4846 - val_auc: 0.8459\n",
      "Epoch 38/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4520 - auc: 0.8669 - val_loss: 0.4869 - val_auc: 0.8448\n",
      "Epoch 39/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4510 - auc: 0.8675 - val_loss: 0.4850 - val_auc: 0.8454\n",
      "Epoch 40/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4500 - auc: 0.8681 - val_loss: 0.4880 - val_auc: 0.8436\n",
      "Epoch 41/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4490 - auc: 0.8689 - val_loss: 0.4867 - val_auc: 0.8445\n",
      "Epoch 42/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4480 - auc: 0.8695 - val_loss: 0.4897 - val_auc: 0.8431\n",
      "Epoch 43/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4470 - auc: 0.8701 - val_loss: 0.4894 - val_auc: 0.8441\n",
      "Epoch 44/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4459 - auc: 0.8708 - val_loss: 0.4894 - val_auc: 0.8433\n",
      "Epoch 45/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4449 - auc: 0.8715 - val_loss: 0.4892 - val_auc: 0.8428\n",
      "Epoch 46/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4439 - auc: 0.8721 - val_loss: 0.4900 - val_auc: 0.8425\n",
      "Epoch 47/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4429 - auc: 0.8727 - val_loss: 0.4909 - val_auc: 0.8424\n",
      "Epoch 48/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4419 - auc: 0.8734 - val_loss: 0.4915 - val_auc: 0.8427\n",
      "Epoch 49/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4409 - auc: 0.8740 - val_loss: 0.4929 - val_auc: 0.8413\n",
      "Epoch 50/250\n",
      "21000/21000 [==============================] - 73s 3ms/step - loss: 0.4397 - auc: 0.8748 - val_loss: 0.4938 - val_auc: 0.8406\n",
      "Epoch 51/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4388 - auc: 0.8754 - val_loss: 0.4948 - val_auc: 0.8401\n",
      "Epoch 52/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4376 - auc: 0.8761 - val_loss: 0.4967 - val_auc: 0.8407\n",
      "Epoch 53/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4368 - auc: 0.8766 - val_loss: 0.4964 - val_auc: 0.8403\n",
      "Epoch 54/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4356 - auc: 0.8773 - val_loss: 0.5004 - val_auc: 0.8379\n",
      "Epoch 55/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4346 - auc: 0.8780 - val_loss: 0.5001 - val_auc: 0.8397\n",
      "Epoch 56/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4336 - auc: 0.8786 - val_loss: 0.4989 - val_auc: 0.8390\n",
      "Epoch 57/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4326 - auc: 0.8792 - val_loss: 0.4983 - val_auc: 0.8384\n",
      "Epoch 58/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4315 - auc: 0.8799 - val_loss: 0.5008 - val_auc: 0.8384\n",
      "Epoch 59/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4306 - auc: 0.8805 - val_loss: 0.5008 - val_auc: 0.8384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4295 - auc: 0.8812 - val_loss: 0.5029 - val_auc: 0.8377\n",
      "Epoch 61/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4285 - auc: 0.8817 - val_loss: 0.5040 - val_auc: 0.8367\n",
      "Epoch 62/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4274 - auc: 0.8824 - val_loss: 0.5059 - val_auc: 0.8362\n",
      "Epoch 63/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4264 - auc: 0.8830 - val_loss: 0.5078 - val_auc: 0.8348\n",
      "Epoch 64/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4254 - auc: 0.8836 - val_loss: 0.5062 - val_auc: 0.8362\n",
      "Epoch 65/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4243 - auc: 0.8843 - val_loss: 0.5096 - val_auc: 0.8353\n",
      "Epoch 66/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4234 - auc: 0.8848 - val_loss: 0.5091 - val_auc: 0.8352\n",
      "Epoch 67/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4223 - auc: 0.8855 - val_loss: 0.5117 - val_auc: 0.8341\n",
      "Epoch 68/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4213 - auc: 0.8860 - val_loss: 0.5137 - val_auc: 0.8339\n",
      "Epoch 69/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4203 - auc: 0.8867 - val_loss: 0.5132 - val_auc: 0.8333\n",
      "Epoch 70/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4193 - auc: 0.8872 - val_loss: 0.5127 - val_auc: 0.8329\n",
      "Epoch 71/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4184 - auc: 0.8878 - val_loss: 0.5136 - val_auc: 0.8324\n",
      "Epoch 72/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4173 - auc: 0.8884 - val_loss: 0.5176 - val_auc: 0.8312\n",
      "Epoch 73/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4163 - auc: 0.8890 - val_loss: 0.5165 - val_auc: 0.8310\n",
      "Epoch 74/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4155 - auc: 0.8894 - val_loss: 0.5216 - val_auc: 0.8301\n",
      "Epoch 75/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4145 - auc: 0.8900 - val_loss: 0.5210 - val_auc: 0.8300\n",
      "Epoch 76/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4136 - auc: 0.8906 - val_loss: 0.5215 - val_auc: 0.8291\n",
      "Epoch 77/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4127 - auc: 0.8911 - val_loss: 0.5234 - val_auc: 0.8303\n",
      "Epoch 78/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4117 - auc: 0.8917 - val_loss: 0.5251 - val_auc: 0.8283\n",
      "Epoch 79/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4109 - auc: 0.8921 - val_loss: 0.5270 - val_auc: 0.8302\n",
      "Epoch 80/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4100 - auc: 0.8927 - val_loss: 0.5260 - val_auc: 0.8273\n",
      "Epoch 81/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4090 - auc: 0.8932 - val_loss: 0.5259 - val_auc: 0.8277\n",
      "Epoch 82/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4082 - auc: 0.8937 - val_loss: 0.5269 - val_auc: 0.8258\n",
      "Epoch 83/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4073 - auc: 0.8942 - val_loss: 0.5314 - val_auc: 0.8285\n",
      "Epoch 84/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4064 - auc: 0.8947 - val_loss: 0.5302 - val_auc: 0.8270\n",
      "Epoch 85/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4056 - auc: 0.8951 - val_loss: 0.5326 - val_auc: 0.8270\n",
      "Epoch 86/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4047 - auc: 0.8956 - val_loss: 0.5311 - val_auc: 0.8263\n",
      "Epoch 87/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4038 - auc: 0.8961 - val_loss: 0.5347 - val_auc: 0.8263\n",
      "Epoch 88/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4031 - auc: 0.8965 - val_loss: 0.5365 - val_auc: 0.8263\n",
      "Epoch 89/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4023 - auc: 0.8969 - val_loss: 0.5363 - val_auc: 0.8259\n",
      "Epoch 90/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4016 - auc: 0.8974 - val_loss: 0.5375 - val_auc: 0.8249\n",
      "Epoch 91/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4008 - auc: 0.8978 - val_loss: 0.5385 - val_auc: 0.8247\n",
      "Epoch 92/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4000 - auc: 0.8983 - val_loss: 0.5419 - val_auc: 0.8225\n",
      "Epoch 93/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3993 - auc: 0.8986 - val_loss: 0.5408 - val_auc: 0.8237\n",
      "Epoch 94/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3986 - auc: 0.8990 - val_loss: 0.5406 - val_auc: 0.8233\n",
      "Epoch 95/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3977 - auc: 0.8995 - val_loss: 0.5428 - val_auc: 0.8212\n",
      "Epoch 96/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3972 - auc: 0.8998 - val_loss: 0.5426 - val_auc: 0.8220\n",
      "Epoch 97/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3964 - auc: 0.9002 - val_loss: 0.5459 - val_auc: 0.8218\n",
      "Epoch 98/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3958 - auc: 0.9005 - val_loss: 0.5481 - val_auc: 0.8225\n",
      "Epoch 99/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3951 - auc: 0.9009 - val_loss: 0.5462 - val_auc: 0.8222\n",
      "Epoch 100/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3944 - auc: 0.9013 - val_loss: 0.5493 - val_auc: 0.8214\n",
      "Epoch 101/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3937 - auc: 0.9017 - val_loss: 0.5476 - val_auc: 0.8210\n",
      "Epoch 102/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3929 - auc: 0.9021 - val_loss: 0.5490 - val_auc: 0.8197\n",
      "Epoch 103/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3924 - auc: 0.9024 - val_loss: 0.5506 - val_auc: 0.8193\n",
      "Epoch 104/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3918 - auc: 0.9026 - val_loss: 0.5526 - val_auc: 0.8190\n",
      "Epoch 105/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3912 - auc: 0.9030 - val_loss: 0.5524 - val_auc: 0.8193\n",
      "Epoch 106/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3905 - auc: 0.9034 - val_loss: 0.5548 - val_auc: 0.8210\n",
      "Epoch 107/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3900 - auc: 0.9036 - val_loss: 0.5559 - val_auc: 0.8182\n",
      "Epoch 108/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3894 - auc: 0.9039 - val_loss: 0.5575 - val_auc: 0.8187\n",
      "Epoch 109/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3888 - auc: 0.9043 - val_loss: 0.5589 - val_auc: 0.8165\n",
      "Epoch 110/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3883 - auc: 0.9046 - val_loss: 0.5590 - val_auc: 0.8190\n",
      "Epoch 111/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3878 - auc: 0.9048 - val_loss: 0.5584 - val_auc: 0.8193\n",
      "Epoch 112/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3869 - auc: 0.9052 - val_loss: 0.5607 - val_auc: 0.8149\n",
      "Epoch 113/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3865 - auc: 0.9055 - val_loss: 0.5618 - val_auc: 0.8171\n",
      "Epoch 114/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3861 - auc: 0.9057 - val_loss: 0.5629 - val_auc: 0.8176\n",
      "Epoch 115/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3855 - auc: 0.9060 - val_loss: 0.5620 - val_auc: 0.8164\n",
      "Epoch 116/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3850 - auc: 0.9062 - val_loss: 0.5629 - val_auc: 0.8166\n",
      "Epoch 117/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3845 - auc: 0.9065 - val_loss: 0.5666 - val_auc: 0.8160\n",
      "Epoch 118/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3839 - auc: 0.9068 - val_loss: 0.5646 - val_auc: 0.8170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3836 - auc: 0.9070 - val_loss: 0.5666 - val_auc: 0.8152\n",
      "Epoch 120/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3832 - auc: 0.9072 - val_loss: 0.5690 - val_auc: 0.8141\n",
      "Epoch 121/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3825 - auc: 0.9076 - val_loss: 0.5699 - val_auc: 0.8160\n",
      "Epoch 122/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3822 - auc: 0.9077 - val_loss: 0.5694 - val_auc: 0.8157\n",
      "Epoch 123/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3817 - auc: 0.9079 - val_loss: 0.5707 - val_auc: 0.8143\n",
      "Epoch 124/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3812 - auc: 0.9082 - val_loss: 0.5706 - val_auc: 0.8146\n",
      "Epoch 125/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3809 - auc: 0.9084 - val_loss: 0.5711 - val_auc: 0.8150\n",
      "Epoch 126/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3805 - auc: 0.9086 - val_loss: 0.5732 - val_auc: 0.8155\n",
      "Epoch 127/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3801 - auc: 0.9088 - val_loss: 0.5728 - val_auc: 0.8136\n",
      "Epoch 128/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3795 - auc: 0.9091 - val_loss: 0.5737 - val_auc: 0.8130\n",
      "Epoch 129/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3794 - auc: 0.9092 - val_loss: 0.5735 - val_auc: 0.8145\n",
      "Epoch 130/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3787 - auc: 0.9095 - val_loss: 0.5745 - val_auc: 0.8142\n",
      "Epoch 131/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3784 - auc: 0.9097 - val_loss: 0.5762 - val_auc: 0.8120\n",
      "Epoch 132/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.3781 - auc: 0.9098 - val_loss: 0.5745 - val_auc: 0.8131\n",
      "Epoch 133/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3776 - auc: 0.9101 - val_loss: 0.5758 - val_auc: 0.8131\n",
      "Epoch 134/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3774 - auc: 0.9101 - val_loss: 0.5803 - val_auc: 0.8129\n",
      "Epoch 135/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3769 - auc: 0.9104 - val_loss: 0.5780 - val_auc: 0.8143\n",
      "Epoch 136/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3768 - auc: 0.9105 - val_loss: 0.5803 - val_auc: 0.8129\n",
      "Epoch 137/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3762 - auc: 0.9108 - val_loss: 0.5792 - val_auc: 0.8111\n",
      "Epoch 138/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3759 - auc: 0.9109 - val_loss: 0.5803 - val_auc: 0.8109\n",
      "Epoch 139/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3757 - auc: 0.9110 - val_loss: 0.5795 - val_auc: 0.8129\n",
      "Epoch 140/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3753 - auc: 0.9112 - val_loss: 0.5830 - val_auc: 0.8106\n",
      "Epoch 141/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3750 - auc: 0.9114 - val_loss: 0.5841 - val_auc: 0.8125\n",
      "Epoch 142/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3746 - auc: 0.9116 - val_loss: 0.5818 - val_auc: 0.8123\n",
      "Epoch 143/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3742 - auc: 0.9118 - val_loss: 0.5852 - val_auc: 0.8122\n",
      "Epoch 144/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3739 - auc: 0.9119 - val_loss: 0.5845 - val_auc: 0.8109\n",
      "Epoch 145/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3738 - auc: 0.9119 - val_loss: 0.5832 - val_auc: 0.8104\n",
      "Epoch 146/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3736 - auc: 0.9121 - val_loss: 0.5820 - val_auc: 0.8093\n",
      "Epoch 147/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3731 - auc: 0.9123 - val_loss: 0.5898 - val_auc: 0.8104\n",
      "Epoch 148/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3728 - auc: 0.9124 - val_loss: 0.5890 - val_auc: 0.8105\n",
      "Epoch 149/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3724 - auc: 0.9126 - val_loss: 0.5880 - val_auc: 0.8094\n",
      "Epoch 150/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3723 - auc: 0.9127 - val_loss: 0.5860 - val_auc: 0.8106\n",
      "Epoch 151/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3720 - auc: 0.9129 - val_loss: 0.5877 - val_auc: 0.8102\n",
      "Epoch 152/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3717 - auc: 0.9130 - val_loss: 0.5903 - val_auc: 0.8103\n",
      "Epoch 153/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3716 - auc: 0.9131 - val_loss: 0.5885 - val_auc: 0.8099\n",
      "Epoch 154/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3712 - auc: 0.9132 - val_loss: 0.5894 - val_auc: 0.8102\n",
      "Epoch 155/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3709 - auc: 0.9134 - val_loss: 0.5921 - val_auc: 0.8099\n",
      "Epoch 156/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3708 - auc: 0.9135 - val_loss: 0.5901 - val_auc: 0.8082\n",
      "Epoch 157/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3705 - auc: 0.9136 - val_loss: 0.5904 - val_auc: 0.8096\n",
      "Epoch 158/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3703 - auc: 0.9137 - val_loss: 0.5909 - val_auc: 0.8083\n",
      "Epoch 159/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3699 - auc: 0.9139 - val_loss: 0.5926 - val_auc: 0.8082\n",
      "Epoch 160/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3697 - auc: 0.9140 - val_loss: 0.5910 - val_auc: 0.8084\n",
      "Epoch 161/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3696 - auc: 0.9140 - val_loss: 0.5923 - val_auc: 0.8095\n",
      "Epoch 162/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3691 - auc: 0.9143 - val_loss: 0.5946 - val_auc: 0.8094\n",
      "Epoch 163/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3691 - auc: 0.9143 - val_loss: 0.5911 - val_auc: 0.8072\n",
      "Epoch 164/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3689 - auc: 0.9144 - val_loss: 0.5921 - val_auc: 0.8090\n",
      "Epoch 165/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3686 - auc: 0.9145 - val_loss: 0.5916 - val_auc: 0.8094\n",
      "Epoch 166/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3685 - auc: 0.9146 - val_loss: 0.5949 - val_auc: 0.8092\n",
      "Epoch 167/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3681 - auc: 0.9147 - val_loss: 0.5944 - val_auc: 0.8079\n",
      "Epoch 168/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3680 - auc: 0.9148 - val_loss: 0.5938 - val_auc: 0.8066\n",
      "Epoch 169/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3679 - auc: 0.9148 - val_loss: 0.5980 - val_auc: 0.8075\n",
      "Epoch 170/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3675 - auc: 0.9151 - val_loss: 0.5969 - val_auc: 0.8076\n",
      "Epoch 171/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3673 - auc: 0.9152 - val_loss: 0.5970 - val_auc: 0.8065\n",
      "Epoch 172/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3674 - auc: 0.9151 - val_loss: 0.5965 - val_auc: 0.8075\n",
      "Epoch 173/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3670 - auc: 0.9153 - val_loss: 0.5958 - val_auc: 0.8070\n",
      "Epoch 174/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3669 - auc: 0.9153 - val_loss: 0.5991 - val_auc: 0.8068\n",
      "Epoch 175/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3665 - auc: 0.9156 - val_loss: 0.5990 - val_auc: 0.8054\n",
      "Epoch 176/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3664 - auc: 0.9156 - val_loss: 0.5972 - val_auc: 0.8068\n",
      "Epoch 177/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3663 - auc: 0.9156 - val_loss: 0.6020 - val_auc: 0.8068\n",
      "Epoch 178/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3662 - auc: 0.9157 - val_loss: 0.6006 - val_auc: 0.8062\n",
      "Epoch 179/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3660 - auc: 0.9158 - val_loss: 0.5993 - val_auc: 0.8053\n",
      "Epoch 180/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3658 - auc: 0.9159 - val_loss: 0.6038 - val_auc: 0.8080\n",
      "Epoch 181/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3657 - auc: 0.9160 - val_loss: 0.6008 - val_auc: 0.8070\n",
      "Epoch 182/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3653 - auc: 0.9161 - val_loss: 0.6002 - val_auc: 0.8063\n",
      "Epoch 183/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3651 - auc: 0.9162 - val_loss: 0.6040 - val_auc: 0.8072\n",
      "Epoch 184/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3649 - auc: 0.9163 - val_loss: 0.6025 - val_auc: 0.8061\n",
      "Epoch 185/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3650 - auc: 0.9163 - val_loss: 0.6025 - val_auc: 0.8059\n",
      "Epoch 186/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3648 - auc: 0.9164 - val_loss: 0.6026 - val_auc: 0.8064\n",
      "Epoch 187/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3647 - auc: 0.9164 - val_loss: 0.6023 - val_auc: 0.8051\n",
      "Epoch 188/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3644 - auc: 0.9165 - val_loss: 0.6058 - val_auc: 0.8076\n",
      "Epoch 189/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3643 - auc: 0.9166 - val_loss: 0.6058 - val_auc: 0.8049\n",
      "Epoch 190/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3641 - auc: 0.9167 - val_loss: 0.6056 - val_auc: 0.8048\n",
      "Epoch 191/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3640 - auc: 0.9168 - val_loss: 0.6039 - val_auc: 0.8053\n",
      "Epoch 192/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3639 - auc: 0.9168 - val_loss: 0.6064 - val_auc: 0.8046\n",
      "Epoch 193/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3636 - auc: 0.9169 - val_loss: 0.6039 - val_auc: 0.8048\n",
      "Epoch 194/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3634 - auc: 0.9170 - val_loss: 0.6063 - val_auc: 0.8056\n",
      "Epoch 195/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3633 - auc: 0.9171 - val_loss: 0.6057 - val_auc: 0.8055\n",
      "Epoch 196/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3632 - auc: 0.9171 - val_loss: 0.6046 - val_auc: 0.8045\n",
      "Epoch 197/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3631 - auc: 0.9172 - val_loss: 0.6060 - val_auc: 0.8051\n",
      "Epoch 198/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3630 - auc: 0.9172 - val_loss: 0.6060 - val_auc: 0.8055\n",
      "Epoch 199/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3628 - auc: 0.9173 - val_loss: 0.6085 - val_auc: 0.8047\n",
      "Epoch 200/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3626 - auc: 0.9174 - val_loss: 0.6062 - val_auc: 0.8046\n",
      "Epoch 201/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3625 - auc: 0.9175 - val_loss: 0.6074 - val_auc: 0.8063\n",
      "Epoch 202/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3624 - auc: 0.9175 - val_loss: 0.6085 - val_auc: 0.8042\n",
      "Epoch 203/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3622 - auc: 0.9176 - val_loss: 0.6091 - val_auc: 0.8050\n",
      "Epoch 204/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3622 - auc: 0.9176 - val_loss: 0.6103 - val_auc: 0.8036\n",
      "Epoch 205/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3620 - auc: 0.9177 - val_loss: 0.6110 - val_auc: 0.8046\n",
      "Epoch 206/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3619 - auc: 0.9177 - val_loss: 0.6081 - val_auc: 0.8053\n",
      "Epoch 207/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3618 - auc: 0.9178 - val_loss: 0.6110 - val_auc: 0.8042\n",
      "Epoch 208/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3617 - auc: 0.9179 - val_loss: 0.6110 - val_auc: 0.8043\n",
      "Epoch 209/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3614 - auc: 0.9180 - val_loss: 0.6093 - val_auc: 0.8041\n",
      "Epoch 210/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3614 - auc: 0.9180 - val_loss: 0.6112 - val_auc: 0.8042\n",
      "Epoch 211/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3611 - auc: 0.9181 - val_loss: 0.6117 - val_auc: 0.8036\n",
      "Epoch 212/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3612 - auc: 0.9181 - val_loss: 0.6119 - val_auc: 0.8038\n",
      "Epoch 213/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3609 - auc: 0.9182 - val_loss: 0.6123 - val_auc: 0.8037\n",
      "Epoch 214/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3609 - auc: 0.9183 - val_loss: 0.6092 - val_auc: 0.8045\n",
      "Epoch 215/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3605 - auc: 0.9184 - val_loss: 0.6124 - val_auc: 0.8032\n",
      "Epoch 216/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3610 - auc: 0.9182 - val_loss: 0.6130 - val_auc: 0.8050\n",
      "Epoch 217/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3606 - auc: 0.9184 - val_loss: 0.6102 - val_auc: 0.8029\n",
      "Epoch 218/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3604 - auc: 0.9185 - val_loss: 0.6129 - val_auc: 0.8043\n",
      "Epoch 219/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3603 - auc: 0.9185 - val_loss: 0.6136 - val_auc: 0.8022\n",
      "Epoch 220/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3601 - auc: 0.9186 - val_loss: 0.6133 - val_auc: 0.8037\n",
      "Epoch 221/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3601 - auc: 0.9186 - val_loss: 0.6111 - val_auc: 0.8045\n",
      "Epoch 222/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3601 - auc: 0.9186 - val_loss: 0.6099 - val_auc: 0.8035\n",
      "Epoch 223/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3598 - auc: 0.9187 - val_loss: 0.6108 - val_auc: 0.8035\n",
      "Epoch 224/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3595 - auc: 0.9189 - val_loss: 0.6154 - val_auc: 0.8036\n",
      "Epoch 225/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3597 - auc: 0.9188 - val_loss: 0.6161 - val_auc: 0.8022\n",
      "Epoch 226/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3595 - auc: 0.9189 - val_loss: 0.6147 - val_auc: 0.8035\n",
      "Epoch 227/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3596 - auc: 0.9189 - val_loss: 0.6142 - val_auc: 0.8026\n",
      "Epoch 228/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3595 - auc: 0.9189 - val_loss: 0.6142 - val_auc: 0.8031\n",
      "Epoch 229/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3593 - auc: 0.9190 - val_loss: 0.6135 - val_auc: 0.8016\n",
      "Epoch 230/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3592 - auc: 0.9191 - val_loss: 0.6151 - val_auc: 0.8025\n",
      "Epoch 231/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3590 - auc: 0.9192 - val_loss: 0.6170 - val_auc: 0.8037\n",
      "Epoch 232/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3590 - auc: 0.9191 - val_loss: 0.6145 - val_auc: 0.8028\n",
      "Epoch 233/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3589 - auc: 0.9192 - val_loss: 0.6157 - val_auc: 0.8028\n",
      "Epoch 234/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3589 - auc: 0.9192 - val_loss: 0.6173 - val_auc: 0.8031\n",
      "Epoch 235/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3588 - auc: 0.9193 - val_loss: 0.6181 - val_auc: 0.8037\n",
      "Epoch 236/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3587 - auc: 0.9193 - val_loss: 0.6164 - val_auc: 0.8035\n",
      "Epoch 237/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3586 - auc: 0.9193 - val_loss: 0.6173 - val_auc: 0.8025\n",
      "Epoch 238/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3586 - auc: 0.9194 - val_loss: 0.6162 - val_auc: 0.8033\n",
      "Epoch 239/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3582 - auc: 0.9195 - val_loss: 0.6126 - val_auc: 0.8025\n",
      "Epoch 240/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3581 - auc: 0.9195 - val_loss: 0.6186 - val_auc: 0.8014\n",
      "Epoch 241/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3583 - auc: 0.9195 - val_loss: 0.6148 - val_auc: 0.8019\n",
      "Epoch 242/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3579 - auc: 0.9197 - val_loss: 0.6162 - val_auc: 0.8015\n",
      "Epoch 243/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3577 - auc: 0.9198 - val_loss: 0.6191 - val_auc: 0.8019\n",
      "Epoch 244/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3581 - auc: 0.9196 - val_loss: 0.6169 - val_auc: 0.8022\n",
      "Epoch 245/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3575 - auc: 0.9199 - val_loss: 0.6191 - val_auc: 0.8023\n",
      "Epoch 246/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3577 - auc: 0.9198 - val_loss: 0.6192 - val_auc: 0.8018\n",
      "Epoch 247/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3577 - auc: 0.9198 - val_loss: 0.6186 - val_auc: 0.8021\n",
      "Epoch 248/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3575 - auc: 0.9199 - val_loss: 0.6219 - val_auc: 0.8034\n",
      "Epoch 249/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3575 - auc: 0.9199 - val_loss: 0.6190 - val_auc: 0.8030\n",
      "Epoch 250/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3574 - auc: 0.9199 - val_loss: 0.6199 - val_auc: 0.8023\n",
      "Wall time: 12h 17min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bcc59bb348>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(x_train, y_train, epochs=250, validation_data=(x_test,y_test), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100000 samples, validate on 500000 samples\n",
      "Epoch 1/250\n",
      "2100000/2100000 [==============================] - 163s 77us/sample - loss: 0.5790 - AUC: 0.7607 - val_loss: 0.5737 - val_AUC: 0.7663\n",
      "Epoch 2/250\n",
      "2100000/2100000 [==============================] - 171s 82us/sample - loss: 0.5637 - AUC: 0.7762 - val_loss: 0.5561 - val_AUC: 0.7838\n",
      "Epoch 3/250\n",
      "2100000/2100000 [==============================] - 211s 100us/sample - loss: 0.5503 - AUC: 0.7893 - val_loss: 0.5496 - val_AUC: 0.7898\n",
      "Epoch 4/250\n",
      "2100000/2100000 [==============================] - 218s 104us/sample - loss: 0.5428 - AUC: 0.7963 - val_loss: 0.5443 - val_AUC: 0.7955\n",
      "Epoch 5/250\n",
      "2100000/2100000 [==============================] - 248s 118us/sample - loss: 0.5374 - AUC: 0.8011 - val_loss: 0.5422 - val_AUC: 0.7986\n",
      "Epoch 6/250\n",
      "2100000/2100000 [==============================] - 233s 111us/sample - loss: 0.5331 - AUC: 0.8049 - val_loss: 0.5412 - val_AUC: 0.8005\n",
      "Epoch 7/250\n",
      "2100000/2100000 [==============================] - 265s 126us/sample - loss: 0.5294 - AUC: 0.8082 - val_loss: 0.5364 - val_AUC: 0.8019\n",
      "Epoch 8/250\n",
      "2100000/2100000 [==============================] - 296s 141us/sample - loss: 0.5262 - AUC: 0.8110 - val_loss: 0.5348 - val_AUC: 0.8038\n",
      "Epoch 9/250\n",
      "2100000/2100000 [==============================] - 237s 113us/sample - loss: 0.5232 - AUC: 0.8135 - val_loss: 0.5344 - val_AUC: 0.8053\n",
      "Epoch 10/250\n",
      "2100000/2100000 [==============================] - 236s 112us/sample - loss: 0.5204 - AUC: 0.8159 - val_loss: 0.5324 - val_AUC: 0.8059\n",
      "Epoch 11/250\n",
      "2100000/2100000 [==============================] - 224s 107us/sample - loss: 0.5179 - AUC: 0.8180 - val_loss: 0.5311 - val_AUC: 0.8075\n",
      "Epoch 12/250\n",
      "2100000/2100000 [==============================] - 291s 138us/sample - loss: 0.5154 - AUC: 0.8201 - val_loss: 0.5292 - val_AUC: 0.8087\n",
      "Epoch 13/250\n",
      "2100000/2100000 [==============================] - 226s 108us/sample - loss: 0.5130 - AUC: 0.8221 - val_loss: 0.5307 - val_AUC: 0.8091\n",
      "Epoch 14/250\n",
      "2100000/2100000 [==============================] - 243s 116us/sample - loss: 0.5109 - AUC: 0.8238 - val_loss: 0.5286 - val_AUC: 0.8105\n",
      "Epoch 15/250\n",
      "2100000/2100000 [==============================] - 214s 102us/sample - loss: 0.5087 - AUC: 0.8255 - val_loss: 0.5288 - val_AUC: 0.8109\n",
      "Epoch 16/250\n",
      "2100000/2100000 [==============================] - 245s 117us/sample - loss: 0.5065 - AUC: 0.8273 - val_loss: 0.5288 - val_AUC: 0.8108\n",
      "Epoch 17/250\n",
      "2100000/2100000 [==============================] - 321s 153us/sample - loss: 0.5046 - AUC: 0.8288 - val_loss: 0.5283 - val_AUC: 0.8109\n",
      "Epoch 18/250\n",
      "2100000/2100000 [==============================] - 257s 122us/sample - loss: 0.5026 - AUC: 0.8304 - val_loss: 0.5276 - val_AUC: 0.8121\n",
      "Epoch 19/250\n",
      "2100000/2100000 [==============================] - 411s 196us/sample - loss: 0.5008 - AUC: 0.8318 - val_loss: 0.5276 - val_AUC: 0.8124\n",
      "Epoch 20/250\n",
      "2100000/2100000 [==============================] - 312s 149us/sample - loss: 0.4989 - AUC: 0.8333 - val_loss: 0.5267 - val_AUC: 0.8132\n",
      "Epoch 21/250\n",
      "2100000/2100000 [==============================] - 385s 183us/sample - loss: 0.4970 - AUC: 0.8348 - val_loss: 0.5271 - val_AUC: 0.8128\n",
      "Epoch 22/250\n",
      "2100000/2100000 [==============================] - 268s 127us/sample - loss: 0.4953 - AUC: 0.8361 - val_loss: 0.5292 - val_AUC: 0.8117\n",
      "Epoch 23/250\n",
      "2100000/2100000 [==============================] - 261s 124us/sample - loss: 0.4935 - AUC: 0.8375 - val_loss: 0.5283 - val_AUC: 0.8131\n",
      "Epoch 24/250\n",
      "2100000/2100000 [==============================] - 257s 122us/sample - loss: 0.4918 - AUC: 0.8388 - val_loss: 0.5310 - val_AUC: 0.8135\n",
      "Epoch 25/250\n",
      "2100000/2100000 [==============================] - 207s 98us/sample - loss: 0.4902 - AUC: 0.8400 - val_loss: 0.5281 - val_AUC: 0.8122\n",
      "Epoch 26/250\n",
      "2100000/2100000 [==============================] - 174s 83us/sample - loss: 0.4885 - AUC: 0.8413 - val_loss: 0.5272 - val_AUC: 0.8124\n",
      "Epoch 27/250\n",
      "2100000/2100000 [==============================] - 151s 72us/sample - loss: 0.4868 - AUC: 0.8425 - val_loss: 0.5289 - val_AUC: 0.8126\n",
      "Epoch 28/250\n",
      "2100000/2100000 [==============================] - 154s 73us/sample - loss: 0.4853 - AUC: 0.8437 - val_loss: 0.5296 - val_AUC: 0.8113\n",
      "Epoch 29/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4837 - AUC: 0.8449 - val_loss: 0.5288 - val_AUC: 0.8124\n",
      "Epoch 30/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4823 - AUC: 0.8459 - val_loss: 0.5287 - val_AUC: 0.8129\n",
      "Epoch 31/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4807 - AUC: 0.8471 - val_loss: 0.5321 - val_AUC: 0.8120\n",
      "Epoch 32/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.4793 - AUC: 0.8481 - val_loss: 0.5292 - val_AUC: 0.8121\n",
      "Epoch 33/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.4779 - AUC: 0.8491 - val_loss: 0.5346 - val_AUC: 0.8113\n",
      "Epoch 34/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4764 - AUC: 0.8502 - val_loss: 0.5336 - val_AUC: 0.8111\n",
      "Epoch 35/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4750 - AUC: 0.8512 - val_loss: 0.5328 - val_AUC: 0.8108\n",
      "Epoch 36/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4736 - AUC: 0.8522 - val_loss: 0.5347 - val_AUC: 0.8101\n",
      "Epoch 37/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4723 - AUC: 0.8532 - val_loss: 0.5354 - val_AUC: 0.8102\n",
      "Epoch 38/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4709 - AUC: 0.8541 - val_loss: 0.5378 - val_AUC: 0.8106\n",
      "Epoch 39/250\n",
      "2100000/2100000 [==============================] - 150s 71us/sample - loss: 0.4696 - AUC: 0.8550 - val_loss: 0.5377 - val_AUC: 0.8096\n",
      "Epoch 40/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4683 - AUC: 0.8560 - val_loss: 0.5406 - val_AUC: 0.8078\n",
      "Epoch 41/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4670 - AUC: 0.8568 - val_loss: 0.5387 - val_AUC: 0.8086\n",
      "Epoch 42/250\n",
      "2100000/2100000 [==============================] - 155s 74us/sample - loss: 0.4657 - AUC: 0.8578 - val_loss: 0.5413 - val_AUC: 0.8079\n",
      "Epoch 43/250\n",
      "2100000/2100000 [==============================] - 150s 72us/sample - loss: 0.4644 - AUC: 0.8586 - val_loss: 0.5424 - val_AUC: 0.8072\n",
      "Epoch 44/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4632 - AUC: 0.8595 - val_loss: 0.5445 - val_AUC: 0.8066\n",
      "Epoch 45/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4620 - AUC: 0.8603 - val_loss: 0.5422 - val_AUC: 0.8067\n",
      "Epoch 46/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4608 - AUC: 0.8611 - val_loss: 0.5491 - val_AUC: 0.8054\n",
      "Epoch 47/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4596 - AUC: 0.8620 - val_loss: 0.5487 - val_AUC: 0.8063\n",
      "Epoch 48/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4584 - AUC: 0.8627 - val_loss: 0.5454 - val_AUC: 0.8065\n",
      "Epoch 49/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4573 - AUC: 0.8635 - val_loss: 0.5496 - val_AUC: 0.8051\n",
      "Epoch 50/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.4561 - AUC: 0.8642 - val_loss: 0.5494 - val_AUC: 0.8028\n",
      "Epoch 51/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4550 - AUC: 0.8650 - val_loss: 0.5488 - val_AUC: 0.8043\n",
      "Epoch 52/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4539 - AUC: 0.8657 - val_loss: 0.5524 - val_AUC: 0.8030\n",
      "Epoch 53/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4529 - AUC: 0.8665 - val_loss: 0.5498 - val_AUC: 0.8024\n",
      "Epoch 54/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4518 - AUC: 0.8672 - val_loss: 0.5534 - val_AUC: 0.8040\n",
      "Epoch 55/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4507 - AUC: 0.8679 - val_loss: 0.5549 - val_AUC: 0.8029\n",
      "Epoch 56/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4496 - AUC: 0.8686 - val_loss: 0.5574 - val_AUC: 0.8017\n",
      "Epoch 57/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4487 - AUC: 0.8692 - val_loss: 0.5559 - val_AUC: 0.8010\n",
      "Epoch 58/250\n",
      "2100000/2100000 [==============================] - 153s 73us/sample - loss: 0.4475 - AUC: 0.8699 - val_loss: 0.5571 - val_AUC: 0.8013\n",
      "Epoch 59/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4466 - AUC: 0.8705 - val_loss: 0.5593 - val_AUC: 0.7991\n",
      "Epoch 60/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4456 - AUC: 0.8712 - val_loss: 0.5589 - val_AUC: 0.8005\n",
      "Epoch 61/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4447 - AUC: 0.8718 - val_loss: 0.5630 - val_AUC: 0.8008\n",
      "Epoch 62/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4437 - AUC: 0.8724 - val_loss: 0.5604 - val_AUC: 0.7982\n",
      "Epoch 63/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4426 - AUC: 0.8731 - val_loss: 0.5634 - val_AUC: 0.7985\n",
      "Epoch 64/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4418 - AUC: 0.8736 - val_loss: 0.5682 - val_AUC: 0.7982\n",
      "Epoch 65/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.4408 - AUC: 0.8742 - val_loss: 0.5657 - val_AUC: 0.7986\n",
      "Epoch 66/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4400 - AUC: 0.8748 - val_loss: 0.5673 - val_AUC: 0.7995\n",
      "Epoch 67/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.4392 - AUC: 0.8753 - val_loss: 0.5683 - val_AUC: 0.7981\n",
      "Epoch 68/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4382 - AUC: 0.8759 - val_loss: 0.5673 - val_AUC: 0.7969\n",
      "Epoch 69/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4374 - AUC: 0.8764 - val_loss: 0.5687 - val_AUC: 0.7961\n",
      "Epoch 70/250\n",
      "2100000/2100000 [==============================] - 153s 73us/sample - loss: 0.4367 - AUC: 0.8769 - val_loss: 0.5768 - val_AUC: 0.7971\n",
      "Epoch 71/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4358 - AUC: 0.8774 - val_loss: 0.5726 - val_AUC: 0.7969\n",
      "Epoch 72/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4350 - AUC: 0.8779 - val_loss: 0.5723 - val_AUC: 0.7952\n",
      "Epoch 73/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4342 - AUC: 0.8784 - val_loss: 0.5728 - val_AUC: 0.7960\n",
      "Epoch 74/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4334 - AUC: 0.8789 - val_loss: 0.5784 - val_AUC: 0.7956\n",
      "Epoch 75/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4326 - AUC: 0.8794 - val_loss: 0.5755 - val_AUC: 0.7952\n",
      "Epoch 76/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4318 - AUC: 0.8799 - val_loss: 0.5785 - val_AUC: 0.7949\n",
      "Epoch 77/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4312 - AUC: 0.8802 - val_loss: 0.5782 - val_AUC: 0.7922\n",
      "Epoch 78/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4304 - AUC: 0.8807 - val_loss: 0.5790 - val_AUC: 0.7964\n",
      "Epoch 79/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4296 - AUC: 0.8812 - val_loss: 0.5794 - val_AUC: 0.7929\n",
      "Epoch 80/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4291 - AUC: 0.8815 - val_loss: 0.5805 - val_AUC: 0.7933\n",
      "Epoch 81/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.4282 - AUC: 0.8821 - val_loss: 0.5825 - val_AUC: 0.7942\n",
      "Epoch 82/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4276 - AUC: 0.8824 - val_loss: 0.5830 - val_AUC: 0.7918\n",
      "Epoch 83/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4270 - AUC: 0.8828 - val_loss: 0.5829 - val_AUC: 0.7918\n",
      "Epoch 84/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4263 - AUC: 0.8833 - val_loss: 0.5856 - val_AUC: 0.7916\n",
      "Epoch 85/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4257 - AUC: 0.8835 - val_loss: 0.5858 - val_AUC: 0.7914\n",
      "Epoch 86/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4249 - AUC: 0.8840 - val_loss: 0.5850 - val_AUC: 0.7929\n",
      "Epoch 87/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4243 - AUC: 0.8844 - val_loss: 0.5910 - val_AUC: 0.7902\n",
      "Epoch 88/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4237 - AUC: 0.8848 - val_loss: 0.5881 - val_AUC: 0.7905\n",
      "Epoch 89/250\n",
      "2100000/2100000 [==============================] - 151s 72us/sample - loss: 0.4232 - AUC: 0.8850 - val_loss: 0.5891 - val_AUC: 0.7910\n",
      "Epoch 90/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4226 - AUC: 0.8854 - val_loss: 0.5885 - val_AUC: 0.7908\n",
      "Epoch 91/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4221 - AUC: 0.8857 - val_loss: 0.5915 - val_AUC: 0.7927\n",
      "Epoch 92/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.4214 - AUC: 0.8862 - val_loss: 0.5928 - val_AUC: 0.7912\n",
      "Epoch 93/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4208 - AUC: 0.8865 - val_loss: 0.5915 - val_AUC: 0.7887\n",
      "Epoch 94/250\n",
      "2100000/2100000 [==============================] - 151s 72us/sample - loss: 0.4203 - AUC: 0.8868 - val_loss: 0.5922 - val_AUC: 0.7893\n",
      "Epoch 95/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.4197 - AUC: 0.8871 - val_loss: 0.5933 - val_AUC: 0.7891\n",
      "Epoch 96/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4192 - AUC: 0.8874 - val_loss: 0.5931 - val_AUC: 0.7879\n",
      "Epoch 97/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4186 - AUC: 0.8878 - val_loss: 0.5969 - val_AUC: 0.7891\n",
      "Epoch 98/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.4182 - AUC: 0.8880 - val_loss: 0.5971 - val_AUC: 0.7882\n",
      "Epoch 99/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4177 - AUC: 0.8883 - val_loss: 0.5976 - val_AUC: 0.7881\n",
      "Epoch 100/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4170 - AUC: 0.8887 - val_loss: 0.5983 - val_AUC: 0.7890\n",
      "Epoch 101/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.4168 - AUC: 0.8888 - val_loss: 0.5974 - val_AUC: 0.7882\n",
      "Epoch 102/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4161 - AUC: 0.8892 - val_loss: 0.5978 - val_AUC: 0.7869\n",
      "Epoch 103/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4158 - AUC: 0.8894 - val_loss: 0.6005 - val_AUC: 0.7876\n",
      "Epoch 104/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4153 - AUC: 0.8897 - val_loss: 0.6027 - val_AUC: 0.7889\n",
      "Epoch 105/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.4148 - AUC: 0.8900 - val_loss: 0.6032 - val_AUC: 0.7879\n",
      "Epoch 106/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4143 - AUC: 0.8902 - val_loss: 0.6037 - val_AUC: 0.7864\n",
      "Epoch 107/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4138 - AUC: 0.8905 - val_loss: 0.6035 - val_AUC: 0.7873\n",
      "Epoch 108/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4135 - AUC: 0.8907 - val_loss: 0.6078 - val_AUC: 0.7881\n",
      "Epoch 109/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4132 - AUC: 0.8909 - val_loss: 0.6050 - val_AUC: 0.7875\n",
      "Epoch 110/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4126 - AUC: 0.8913 - val_loss: 0.6075 - val_AUC: 0.7874\n",
      "Epoch 111/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4121 - AUC: 0.8916 - val_loss: 0.6093 - val_AUC: 0.7871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4117 - AUC: 0.8918 - val_loss: 0.6082 - val_AUC: 0.7864\n",
      "Epoch 113/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.4113 - AUC: 0.8920 - val_loss: 0.6092 - val_AUC: 0.7855\n",
      "Epoch 114/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4111 - AUC: 0.8921 - val_loss: 0.6078 - val_AUC: 0.7852\n",
      "Epoch 115/250\n",
      "2100000/2100000 [==============================] - 150s 71us/sample - loss: 0.4105 - AUC: 0.8924 - val_loss: 0.6113 - val_AUC: 0.7850\n",
      "Epoch 116/250\n",
      "2100000/2100000 [==============================] - 152s 72us/sample - loss: 0.4102 - AUC: 0.8926 - val_loss: 0.6080 - val_AUC: 0.7868\n",
      "Epoch 117/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4099 - AUC: 0.8928 - val_loss: 0.6111 - val_AUC: 0.7865\n",
      "Epoch 118/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.4094 - AUC: 0.8931 - val_loss: 0.6073 - val_AUC: 0.7840\n",
      "Epoch 119/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4091 - AUC: 0.8932 - val_loss: 0.6111 - val_AUC: 0.7852\n",
      "Epoch 120/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4088 - AUC: 0.8934 - val_loss: 0.6132 - val_AUC: 0.7846\n",
      "Epoch 121/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4083 - AUC: 0.8937 - val_loss: 0.6144 - val_AUC: 0.7854\n",
      "Epoch 122/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4078 - AUC: 0.8940 - val_loss: 0.6117 - val_AUC: 0.7835\n",
      "Epoch 123/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4077 - AUC: 0.8940 - val_loss: 0.6164 - val_AUC: 0.7845\n",
      "Epoch 124/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4074 - AUC: 0.8942 - val_loss: 0.6154 - val_AUC: 0.7836\n",
      "Epoch 125/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4069 - AUC: 0.8945 - val_loss: 0.6158 - val_AUC: 0.7825\n",
      "Epoch 126/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4066 - AUC: 0.8946 - val_loss: 0.6164 - val_AUC: 0.7839\n",
      "Epoch 127/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4065 - AUC: 0.8947 - val_loss: 0.6159 - val_AUC: 0.7841\n",
      "Epoch 128/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4060 - AUC: 0.8949 - val_loss: 0.6178 - val_AUC: 0.7843\n",
      "Epoch 129/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4058 - AUC: 0.8950 - val_loss: 0.6155 - val_AUC: 0.7840\n",
      "Epoch 130/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.4054 - AUC: 0.8953 - val_loss: 0.6172 - val_AUC: 0.7850\n",
      "Epoch 131/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4051 - AUC: 0.8954 - val_loss: 0.6193 - val_AUC: 0.7851\n",
      "Epoch 132/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4047 - AUC: 0.8957 - val_loss: 0.6200 - val_AUC: 0.7817\n",
      "Epoch 133/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4045 - AUC: 0.8958 - val_loss: 0.6206 - val_AUC: 0.7830\n",
      "Epoch 134/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4042 - AUC: 0.8960 - val_loss: 0.6200 - val_AUC: 0.7828\n",
      "Epoch 135/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4041 - AUC: 0.8961 - val_loss: 0.6213 - val_AUC: 0.7822\n",
      "Epoch 136/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4037 - AUC: 0.8962 - val_loss: 0.6200 - val_AUC: 0.7821\n",
      "Epoch 137/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4035 - AUC: 0.8964 - val_loss: 0.6217 - val_AUC: 0.7833\n",
      "Epoch 138/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4034 - AUC: 0.8964 - val_loss: 0.6219 - val_AUC: 0.7816\n",
      "Epoch 139/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4028 - AUC: 0.8967 - val_loss: 0.6253 - val_AUC: 0.7832\n",
      "Epoch 140/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4025 - AUC: 0.8969 - val_loss: 0.6243 - val_AUC: 0.7823\n",
      "Epoch 141/250\n",
      "2100000/2100000 [==============================] - 157s 75us/sample - loss: 0.4023 - AUC: 0.8970 - val_loss: 0.6245 - val_AUC: 0.7817\n",
      "Epoch 142/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.4022 - AUC: 0.8971 - val_loss: 0.6226 - val_AUC: 0.7827\n",
      "Epoch 143/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.4019 - AUC: 0.8972 - val_loss: 0.6254 - val_AUC: 0.7819\n",
      "Epoch 144/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4018 - AUC: 0.8973 - val_loss: 0.6261 - val_AUC: 0.7820\n",
      "Epoch 145/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4014 - AUC: 0.8975 - val_loss: 0.6250 - val_AUC: 0.7807\n",
      "Epoch 146/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4012 - AUC: 0.8976 - val_loss: 0.6233 - val_AUC: 0.7789\n",
      "Epoch 147/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4011 - AUC: 0.8977 - val_loss: 0.6251 - val_AUC: 0.7788\n",
      "Epoch 148/250\n",
      "2100000/2100000 [==============================] - 151s 72us/sample - loss: 0.4007 - AUC: 0.8979 - val_loss: 0.6278 - val_AUC: 0.7802\n",
      "Epoch 149/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4006 - AUC: 0.8979 - val_loss: 0.6270 - val_AUC: 0.7806\n",
      "Epoch 150/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4003 - AUC: 0.8981 - val_loss: 0.6269 - val_AUC: 0.7792\n",
      "Epoch 151/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3999 - AUC: 0.8983 - val_loss: 0.6274 - val_AUC: 0.7794\n",
      "Epoch 152/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3999 - AUC: 0.8983 - val_loss: 0.6283 - val_AUC: 0.7812\n",
      "Epoch 153/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3998 - AUC: 0.8984 - val_loss: 0.6283 - val_AUC: 0.7805\n",
      "Epoch 154/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3994 - AUC: 0.8986 - val_loss: 0.6319 - val_AUC: 0.7790\n",
      "Epoch 155/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3993 - AUC: 0.8987 - val_loss: 0.6311 - val_AUC: 0.7804\n",
      "Epoch 156/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3990 - AUC: 0.8988 - val_loss: 0.6302 - val_AUC: 0.7797\n",
      "Epoch 157/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3988 - AUC: 0.8989 - val_loss: 0.6315 - val_AUC: 0.7785\n",
      "Epoch 158/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3988 - AUC: 0.8989 - val_loss: 0.6280 - val_AUC: 0.7798\n",
      "Epoch 159/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3984 - AUC: 0.8992 - val_loss: 0.6355 - val_AUC: 0.7804\n",
      "Epoch 160/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3982 - AUC: 0.8993 - val_loss: 0.6322 - val_AUC: 0.7818\n",
      "Epoch 161/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3982 - AUC: 0.8993 - val_loss: 0.6321 - val_AUC: 0.7785\n",
      "Epoch 162/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3978 - AUC: 0.8995 - val_loss: 0.6318 - val_AUC: 0.7783\n",
      "Epoch 163/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.3976 - AUC: 0.8996 - val_loss: 0.6333 - val_AUC: 0.7783\n",
      "Epoch 164/250\n",
      "2100000/2100000 [==============================] - 150s 72us/sample - loss: 0.3975 - AUC: 0.8997 - val_loss: 0.6337 - val_AUC: 0.7788\n",
      "Epoch 165/250\n",
      "2100000/2100000 [==============================] - 162s 77us/sample - loss: 0.3974 - AUC: 0.8997 - val_loss: 0.6334 - val_AUC: 0.7802\n",
      "Epoch 166/250\n",
      "2100000/2100000 [==============================] - 165s 79us/sample - loss: 0.3972 - AUC: 0.8998 - val_loss: 0.6331 - val_AUC: 0.7784\n",
      "Epoch 167/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100000/2100000 [==============================] - 158s 75us/sample - loss: 0.3968 - AUC: 0.9000 - val_loss: 0.6363 - val_AUC: 0.7793\n",
      "Epoch 168/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3968 - AUC: 0.9000 - val_loss: 0.6358 - val_AUC: 0.7790\n",
      "Epoch 169/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.3964 - AUC: 0.9002 - val_loss: 0.6326 - val_AUC: 0.7796\n",
      "Epoch 170/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3965 - AUC: 0.9002 - val_loss: 0.6360 - val_AUC: 0.7794\n",
      "Epoch 171/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3963 - AUC: 0.9003 - val_loss: 0.6386 - val_AUC: 0.7792\n",
      "Epoch 172/250\n",
      "2100000/2100000 [==============================] - 154s 73us/sample - loss: 0.3960 - AUC: 0.9004 - val_loss: 0.6342 - val_AUC: 0.7787\n",
      "Epoch 173/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3959 - AUC: 0.9005 - val_loss: 0.6359 - val_AUC: 0.7796\n",
      "Epoch 174/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3956 - AUC: 0.9007 - val_loss: 0.6352 - val_AUC: 0.7798\n",
      "Epoch 175/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3957 - AUC: 0.9006 - val_loss: 0.6329 - val_AUC: 0.7778\n",
      "Epoch 176/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3955 - AUC: 0.9007 - val_loss: 0.6345 - val_AUC: 0.7786\n",
      "Epoch 177/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3952 - AUC: 0.9009 - val_loss: 0.6387 - val_AUC: 0.7788\n",
      "Epoch 178/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3951 - AUC: 0.9010 - val_loss: 0.6375 - val_AUC: 0.7798\n",
      "Epoch 179/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3949 - AUC: 0.9010 - val_loss: 0.6444 - val_AUC: 0.7779\n",
      "Epoch 180/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3947 - AUC: 0.9011 - val_loss: 0.6394 - val_AUC: 0.7769\n",
      "Epoch 181/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3946 - AUC: 0.9012 - val_loss: 0.6387 - val_AUC: 0.7789\n",
      "Epoch 182/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3945 - AUC: 0.9012 - val_loss: 0.6391 - val_AUC: 0.7777\n",
      "Epoch 183/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3945 - AUC: 0.9012 - val_loss: 0.6376 - val_AUC: 0.7790\n",
      "Epoch 184/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.3942 - AUC: 0.9014 - val_loss: 0.6381 - val_AUC: 0.7784\n",
      "Epoch 185/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3940 - AUC: 0.9015 - val_loss: 0.6415 - val_AUC: 0.7783\n",
      "Epoch 186/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3939 - AUC: 0.9016 - val_loss: 0.6413 - val_AUC: 0.7778\n",
      "Epoch 187/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.3937 - AUC: 0.9016 - val_loss: 0.6432 - val_AUC: 0.7787\n",
      "Epoch 188/250\n",
      "2100000/2100000 [==============================] - 150s 71us/sample - loss: 0.3937 - AUC: 0.9017 - val_loss: 0.6372 - val_AUC: 0.7771\n",
      "Epoch 189/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3933 - AUC: 0.9019 - val_loss: 0.6472 - val_AUC: 0.7765\n",
      "Epoch 190/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3932 - AUC: 0.9019 - val_loss: 0.6418 - val_AUC: 0.7768\n",
      "Epoch 191/250\n",
      "2100000/2100000 [==============================] - 161s 76us/sample - loss: 0.3931 - AUC: 0.9020 - val_loss: 0.6434 - val_AUC: 0.7775\n",
      "Epoch 192/250\n",
      "2100000/2100000 [==============================] - 154s 73us/sample - loss: 0.3930 - AUC: 0.9021 - val_loss: 0.6423 - val_AUC: 0.7772\n",
      "Epoch 193/250\n",
      "2100000/2100000 [==============================] - 150s 72us/sample - loss: 0.3930 - AUC: 0.9020 - val_loss: 0.6410 - val_AUC: 0.7773\n",
      "Epoch 194/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3928 - AUC: 0.9021 - val_loss: 0.6428 - val_AUC: 0.7773\n",
      "Epoch 195/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3926 - AUC: 0.9022 - val_loss: 0.6429 - val_AUC: 0.7773\n",
      "Epoch 196/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3926 - AUC: 0.9023 - val_loss: 0.6394 - val_AUC: 0.7775\n",
      "Epoch 197/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3925 - AUC: 0.9023 - val_loss: 0.6386 - val_AUC: 0.7766\n",
      "Epoch 198/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3922 - AUC: 0.9024 - val_loss: 0.6412 - val_AUC: 0.7772\n",
      "Epoch 199/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3922 - AUC: 0.9024 - val_loss: 0.6438 - val_AUC: 0.7766\n",
      "Epoch 200/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3918 - AUC: 0.9027 - val_loss: 0.6430 - val_AUC: 0.7773\n",
      "Epoch 201/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3920 - AUC: 0.9026 - val_loss: 0.6457 - val_AUC: 0.7764\n",
      "Epoch 202/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.3919 - AUC: 0.9026 - val_loss: 0.6479 - val_AUC: 0.7780\n",
      "Epoch 203/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3917 - AUC: 0.9027 - val_loss: 0.6433 - val_AUC: 0.7762\n",
      "Epoch 204/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.3915 - AUC: 0.9028 - val_loss: 0.6452 - val_AUC: 0.7768\n",
      "Epoch 205/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3912 - AUC: 0.9030 - val_loss: 0.6435 - val_AUC: 0.7774\n",
      "Epoch 206/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3913 - AUC: 0.9030 - val_loss: 0.6466 - val_AUC: 0.7772\n",
      "Epoch 207/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3913 - AUC: 0.9029 - val_loss: 0.6446 - val_AUC: 0.7752\n",
      "Epoch 208/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3912 - AUC: 0.9030 - val_loss: 0.6449 - val_AUC: 0.7764\n",
      "Epoch 209/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3909 - AUC: 0.9032 - val_loss: 0.6433 - val_AUC: 0.7755\n",
      "Epoch 210/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3907 - AUC: 0.9033 - val_loss: 0.6477 - val_AUC: 0.7759\n",
      "Epoch 211/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3910 - AUC: 0.9031 - val_loss: 0.6458 - val_AUC: 0.7764\n",
      "Epoch 212/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3907 - AUC: 0.9033 - val_loss: 0.6453 - val_AUC: 0.7747\n",
      "Epoch 213/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3905 - AUC: 0.9034 - val_loss: 0.6459 - val_AUC: 0.7773\n",
      "Epoch 214/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3904 - AUC: 0.9034 - val_loss: 0.6446 - val_AUC: 0.7753\n",
      "Epoch 215/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3904 - AUC: 0.9034 - val_loss: 0.6473 - val_AUC: 0.7769\n",
      "Epoch 216/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3900 - AUC: 0.9036 - val_loss: 0.6463 - val_AUC: 0.7745\n",
      "Epoch 217/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3901 - AUC: 0.9036 - val_loss: 0.6511 - val_AUC: 0.7748\n",
      "Epoch 218/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3899 - AUC: 0.9037 - val_loss: 0.6489 - val_AUC: 0.7759\n",
      "Epoch 219/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3900 - AUC: 0.9036 - val_loss: 0.6456 - val_AUC: 0.7757\n",
      "Epoch 220/250\n",
      "2100000/2100000 [==============================] - 152s 72us/sample - loss: 0.3898 - AUC: 0.9037 - val_loss: 0.6488 - val_AUC: 0.7772\n",
      "Epoch 221/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3896 - AUC: 0.9038 - val_loss: 0.6462 - val_AUC: 0.7752\n",
      "Epoch 222/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3895 - AUC: 0.9039 - val_loss: 0.6521 - val_AUC: 0.7761\n",
      "Epoch 223/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3895 - AUC: 0.9039 - val_loss: 0.6495 - val_AUC: 0.7770\n",
      "Epoch 224/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3895 - AUC: 0.9039 - val_loss: 0.6449 - val_AUC: 0.7756\n",
      "Epoch 225/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3894 - AUC: 0.9040 - val_loss: 0.6482 - val_AUC: 0.7774\n",
      "Epoch 226/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3893 - AUC: 0.9040 - val_loss: 0.6478 - val_AUC: 0.7756\n",
      "Epoch 227/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3891 - AUC: 0.9041 - val_loss: 0.6483 - val_AUC: 0.7749\n",
      "Epoch 228/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3890 - AUC: 0.9041 - val_loss: 0.6487 - val_AUC: 0.7763\n",
      "Epoch 229/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3890 - AUC: 0.9042 - val_loss: 0.6512 - val_AUC: 0.7738\n",
      "Epoch 230/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3889 - AUC: 0.9042 - val_loss: 0.6521 - val_AUC: 0.7772\n",
      "Epoch 231/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3887 - AUC: 0.9043 - val_loss: 0.6505 - val_AUC: 0.7747\n",
      "Epoch 232/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3889 - AUC: 0.9042 - val_loss: 0.6500 - val_AUC: 0.7765\n",
      "Epoch 233/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3887 - AUC: 0.9043 - val_loss: 0.6506 - val_AUC: 0.7747\n",
      "Epoch 234/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3886 - AUC: 0.9043 - val_loss: 0.6481 - val_AUC: 0.7753\n",
      "Epoch 235/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.3886 - AUC: 0.9043 - val_loss: 0.6514 - val_AUC: 0.7755\n",
      "Epoch 236/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3885 - AUC: 0.9044 - val_loss: 0.6496 - val_AUC: 0.7732\n",
      "Epoch 237/250\n",
      "2100000/2100000 [==============================] - 150s 71us/sample - loss: 0.3883 - AUC: 0.9045 - val_loss: 0.6537 - val_AUC: 0.7758\n",
      "Epoch 238/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3883 - AUC: 0.9045 - val_loss: 0.6520 - val_AUC: 0.7765\n",
      "Epoch 239/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3881 - AUC: 0.9047 - val_loss: 0.6542 - val_AUC: 0.7762\n",
      "Epoch 240/250\n",
      "2100000/2100000 [==============================] - 155s 74us/sample - loss: 0.3881 - AUC: 0.9046 - val_loss: 0.6540 - val_AUC: 0.7745\n",
      "Epoch 241/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.3882 - AUC: 0.9046 - val_loss: 0.6497 - val_AUC: 0.7762\n",
      "Epoch 242/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3878 - AUC: 0.9048 - val_loss: 0.6507 - val_AUC: 0.7747\n",
      "Epoch 243/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.3881 - AUC: 0.9046 - val_loss: 0.6493 - val_AUC: 0.7748\n",
      "Epoch 244/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3878 - AUC: 0.9048 - val_loss: 0.6505 - val_AUC: 0.7745\n",
      "Epoch 245/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3878 - AUC: 0.9048 - val_loss: 0.6518 - val_AUC: 0.7748\n",
      "Epoch 246/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.3876 - AUC: 0.9049 - val_loss: 0.6522 - val_AUC: 0.7767\n",
      "Epoch 247/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3877 - AUC: 0.9048 - val_loss: 0.6525 - val_AUC: 0.7745\n",
      "Epoch 248/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.3875 - AUC: 0.9049 - val_loss: 0.6540 - val_AUC: 0.7734\n",
      "Epoch 249/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3875 - AUC: 0.9049 - val_loss: 0.6528 - val_AUC: 0.7757\n",
      "Epoch 250/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3874 - AUC: 0.9050 - val_loss: 0.6498 - val_AUC: 0.7750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a75da2fc08>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change learning rate\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch == 1:\n",
    "        return float(0.05)\n",
    "    else:\n",
    "        if lr * (1-1.0000002) < 0.000001:\n",
    "            return float(lr)\n",
    "        else:\n",
    "            return float(lr * (1-1.0000002))\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(momentum=1e-5)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "#set early stopping criteria\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.00001 , patience=10)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=250, validation_data=(x_test,y_test), batch_size=100, callbacks=[lr_callback,es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100000 samples, validate on 500000 samples\n",
      "Epoch 1/250\n",
      "2100000/2100000 [==============================] - 205s 98us/sample - loss: 0.4367 - AUC: 0.8767 - val_loss: 0.5955 - val_AUC: 0.7841\n",
      "Epoch 2/250\n",
      "2100000/2100000 [==============================] - 209s 99us/sample - loss: 0.5580 - AUC: 0.7873 - val_loss: 0.5573 - val_AUC: 0.7953\n",
      "Epoch 3/250\n",
      "2100000/2100000 [==============================] - 205s 98us/sample - loss: 0.5545 - AUC: 0.7909 - val_loss: 0.5617 - val_AUC: 0.7937\n",
      "Epoch 4/250\n",
      "2100000/2100000 [==============================] - 214s 102us/sample - loss: 0.5520 - AUC: 0.7936 - val_loss: 0.5387 - val_AUC: 0.8038\n",
      "Epoch 5/250\n",
      "2100000/2100000 [==============================] - 203s 97us/sample - loss: 0.5496 - AUC: 0.7961 - val_loss: 0.5474 - val_AUC: 0.8025\n",
      "Epoch 6/250\n",
      "2100000/2100000 [==============================] - 184s 88us/sample - loss: 0.5485 - AUC: 0.7973 - val_loss: 0.5434 - val_AUC: 0.7987\n",
      "Epoch 7/250\n",
      "2100000/2100000 [==============================] - 194s 93us/sample - loss: 0.5481 - AUC: 0.7981 - val_loss: 0.5419 - val_AUC: 0.8008\n",
      "Epoch 8/250\n",
      "2100000/2100000 [==============================] - 175s 83us/sample - loss: 0.5474 - AUC: 0.7989 - val_loss: 0.5987 - val_AUC: 0.7692\n",
      "Epoch 9/250\n",
      "2100000/2100000 [==============================] - 186s 89us/sample - loss: 0.5475 - AUC: 0.7995 - val_loss: 0.5462 - val_AUC: 0.8044\n",
      "Epoch 10/250\n",
      "2100000/2100000 [==============================] - 190s 90us/sample - loss: 0.5472 - AUC: 0.8000 - val_loss: 0.5480 - val_AUC: 0.7976\n",
      "Epoch 11/250\n",
      "2100000/2100000 [==============================] - 196s 93us/sample - loss: 0.5475 - AUC: 0.8001 - val_loss: 0.5460 - val_AUC: 0.8091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a75d3a49c8>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change learning rate and add stopping criteria\n",
    "#momentum not working here\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch == 1:\n",
    "        return float(0.05)\n",
    "    else:\n",
    "        if lr * (1-1.0000002) < 0.000001:\n",
    "            return float(lr)\n",
    "        else:\n",
    "            return float(lr * (1-1.0000002))\n",
    "        \n",
    "def mmnt(epoch,moment):\n",
    "    if epoch == 1:\n",
    "        return float(0.9)\n",
    "    else:\n",
    "        if epoch <= 200:\n",
    "            return float(moment + 0.00045)\n",
    "        else:\n",
    "            return float(0.99)\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(momentum=.9)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "#set early stopping criteria\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.00001 , patience=10)\n",
    "\n",
    "#opt = tf.compat.v1.train.MomentumOptimizer(learning_rate=.05, momentum=mmnt, use_locking=False, name='Momentum', use_nesterov=False)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=250, validation_data=(x_test,y_test), batch_size=100, callbacks=[lr_callback,es_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
