{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# import shutil\n",
    "# # C:\\Users\\allro\\JupyterNotebook\\QTW\\Data\n",
    "# with gzip.open('C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv.gz', 'rb') as f_in:\n",
    "#    with open('C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv', 'wb') as f_out:\n",
    "#        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/ml/datasets/HIGGS#\n",
    "#df = pd.read_csv(\"./Data/HIGGS.csv\", header=None)\n",
    "df = pd.read_csv(\"../../HIGGS.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns =['target', 'lepton_ph', 'lepton_eta', 'lepton_phi','missing_energy_magnitude','missing_energy_phi',\n",
    "             'jet_1_pt','jet_1_eta','jet_1_phi','jet_1_btag','jet_2_pt','jet_2_eta','jet_2_phi','jet_2_btag',\n",
    "             'jet_3_pt','jet_3_eta','jet_3_phi','jet_3_btag','jet_4_pt','jet_4_eta','jet_4_phi','jet_4_btag',\n",
    "             'm_jj','m_jjj','m_lv','m_jlv','m_bb','m_wbb','m_wwbb'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000000 entries, 0 to 10999999\n",
      "Data columns (total 29 columns):\n",
      "target                      float64\n",
      "lepton_ph                   float64\n",
      "lepton_eta                  float64\n",
      "lepton_phi                  float64\n",
      "missing_energy_magnitude    float64\n",
      "missing_energy_phi          float64\n",
      "jet_1_pt                    float64\n",
      "jet_1_eta                   float64\n",
      "jet_1_phi                   float64\n",
      "jet_1_btag                  float64\n",
      "jet_2_pt                    float64\n",
      "jet_2_eta                   float64\n",
      "jet_2_phi                   float64\n",
      "jet_2_btag                  float64\n",
      "jet_3_pt                    float64\n",
      "jet_3_eta                   float64\n",
      "jet_3_phi                   float64\n",
      "jet_3_btag                  float64\n",
      "jet_4_pt                    float64\n",
      "jet_4_eta                   float64\n",
      "jet_4_phi                   float64\n",
      "jet_4_btag                  float64\n",
      "m_jj                        float64\n",
      "m_jjj                       float64\n",
      "m_lv                        float64\n",
      "m_jlv                       float64\n",
      "m_bb                        float64\n",
      "m_wbb                       float64\n",
      "m_wwbb                      float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "# Print out the data types\n",
    "df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  lepton_ph  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0     1.0   0.869293   -0.635082    0.225690                  0.327470   \n",
       "1     1.0   0.907542    0.329147    0.359412                  1.497970   \n",
       "2     1.0   0.798835    1.470639   -1.635975                  0.453773   \n",
       "3     0.0   1.344385   -0.876626    0.935913                  1.992050   \n",
       "4     1.0   1.105009    0.321356    1.522401                  0.882808   \n",
       "\n",
       "   missing_energy_phi  jet_1_pt  jet_1_eta  jet_1_phi  jet_1_btag  ...  \\\n",
       "0           -0.689993  0.754202  -0.248573  -1.092064    0.000000  ...   \n",
       "1           -0.313010  1.095531  -0.557525  -1.588230    2.173076  ...   \n",
       "2            0.425629  1.104875   1.282322   1.381664    0.000000  ...   \n",
       "3            0.882454  1.786066  -1.646778  -0.942383    0.000000  ...   \n",
       "4           -1.205349  0.681466  -1.070464  -0.921871    0.000000  ...   \n",
       "\n",
       "   jet_4_eta  jet_4_phi  jet_4_btag      m_jj     m_jjj      m_lv     m_jlv  \\\n",
       "0  -0.010455  -0.045767    3.101961  1.353760  0.979563  0.978076  0.920005   \n",
       "1  -1.138930  -0.000819    0.000000  0.302220  0.833048  0.985700  0.978098   \n",
       "2   1.128848   0.900461    0.000000  0.909753  1.108330  0.985692  0.951331   \n",
       "3  -0.678379  -1.360356    0.000000  0.946652  1.028704  0.998656  0.728281   \n",
       "4  -0.373566   0.113041    0.000000  0.755856  1.361057  0.986610  0.838085   \n",
       "\n",
       "       m_bb     m_wbb    m_wwbb  \n",
       "0  0.721657  0.988751  0.876678  \n",
       "1  0.779732  0.992356  0.798343  \n",
       "2  0.803252  0.865924  0.780118  \n",
       "3  0.869200  1.026736  0.957904  \n",
       "4  1.133295  0.872245  0.808487  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample the data\n",
    "train = df.sample(n=2600000, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.299203e-01</td>\n",
       "      <td>9.914658e-01</td>\n",
       "      <td>-8.297618e-06</td>\n",
       "      <td>-1.327225e-05</td>\n",
       "      <td>9.985364e-01</td>\n",
       "      <td>2.613459e-05</td>\n",
       "      <td>9.909152e-01</td>\n",
       "      <td>-2.027520e-05</td>\n",
       "      <td>7.716199e-06</td>\n",
       "      <td>9.999687e-01</td>\n",
       "      <td>9.927294e-01</td>\n",
       "      <td>-1.026444e-05</td>\n",
       "      <td>-2.076887e-05</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>9.922591e-01</td>\n",
       "      <td>1.459561e-05</td>\n",
       "      <td>3.678632e-06</td>\n",
       "      <td>1.000011e+00</td>\n",
       "      <td>9.861087e-01</td>\n",
       "      <td>-5.756954e-06</td>\n",
       "      <td>1.744903e-05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.034290e+00</td>\n",
       "      <td>1.024805e+00</td>\n",
       "      <td>1.050554e+00</td>\n",
       "      <td>1.009742e+00</td>\n",
       "      <td>9.729596e-01</td>\n",
       "      <td>1.033036e+00</td>\n",
       "      <td>9.598120e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.991040e-01</td>\n",
       "      <td>5.653777e-01</td>\n",
       "      <td>1.008827e+00</td>\n",
       "      <td>1.006346e+00</td>\n",
       "      <td>6.000185e-01</td>\n",
       "      <td>1.006326e+00</td>\n",
       "      <td>4.749747e-01</td>\n",
       "      <td>1.009303e+00</td>\n",
       "      <td>1.005901e+00</td>\n",
       "      <td>1.027808e+00</td>\n",
       "      <td>4.999939e-01</td>\n",
       "      <td>1.009331e+00</td>\n",
       "      <td>1.006154e+00</td>\n",
       "      <td>1.049398e+00</td>\n",
       "      <td>4.876623e-01</td>\n",
       "      <td>1.008747e+00</td>\n",
       "      <td>1.006305e+00</td>\n",
       "      <td>1.193676e+00</td>\n",
       "      <td>5.057777e-01</td>\n",
       "      <td>1.007694e+00</td>\n",
       "      <td>1.006366e+00</td>\n",
       "      <td>1.400209e+00</td>\n",
       "      <td>6.746354e-01</td>\n",
       "      <td>3.808074e-01</td>\n",
       "      <td>1.645763e-01</td>\n",
       "      <td>3.974453e-01</td>\n",
       "      <td>5.254063e-01</td>\n",
       "      <td>3.652556e-01</td>\n",
       "      <td>3.133378e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.746966e-01</td>\n",
       "      <td>-2.434976e+00</td>\n",
       "      <td>-1.742508e+00</td>\n",
       "      <td>2.370088e-04</td>\n",
       "      <td>-1.743944e+00</td>\n",
       "      <td>1.375024e-01</td>\n",
       "      <td>-2.969725e+00</td>\n",
       "      <td>-1.741237e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.889811e-01</td>\n",
       "      <td>-2.913090e+00</td>\n",
       "      <td>-1.742372e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.636076e-01</td>\n",
       "      <td>-2.729663e+00</td>\n",
       "      <td>-1.742069e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.653542e-01</td>\n",
       "      <td>-2.497265e+00</td>\n",
       "      <td>-1.742691e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.507046e-02</td>\n",
       "      <td>1.986757e-01</td>\n",
       "      <td>8.304866e-02</td>\n",
       "      <td>1.320062e-01</td>\n",
       "      <td>4.786215e-02</td>\n",
       "      <td>2.951122e-01</td>\n",
       "      <td>3.307214e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.907533e-01</td>\n",
       "      <td>-7.383225e-01</td>\n",
       "      <td>-8.719308e-01</td>\n",
       "      <td>5.768156e-01</td>\n",
       "      <td>-8.712081e-01</td>\n",
       "      <td>6.789927e-01</td>\n",
       "      <td>-6.872450e-01</td>\n",
       "      <td>-8.680962e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.564608e-01</td>\n",
       "      <td>-6.944718e-01</td>\n",
       "      <td>-8.701791e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.508527e-01</td>\n",
       "      <td>-6.998083e-01</td>\n",
       "      <td>-8.711343e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.177673e-01</td>\n",
       "      <td>-7.141902e-01</td>\n",
       "      <td>-8.714789e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.906095e-01</td>\n",
       "      <td>8.462266e-01</td>\n",
       "      <td>9.857525e-01</td>\n",
       "      <td>7.675732e-01</td>\n",
       "      <td>6.738168e-01</td>\n",
       "      <td>8.193964e-01</td>\n",
       "      <td>7.703901e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.533714e-01</td>\n",
       "      <td>-5.415563e-05</td>\n",
       "      <td>-2.410638e-04</td>\n",
       "      <td>8.916277e-01</td>\n",
       "      <td>2.125454e-04</td>\n",
       "      <td>8.948193e-01</td>\n",
       "      <td>-2.543566e-05</td>\n",
       "      <td>5.813991e-05</td>\n",
       "      <td>1.086538e+00</td>\n",
       "      <td>8.901377e-01</td>\n",
       "      <td>6.027267e-05</td>\n",
       "      <td>3.514990e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.972494e-01</td>\n",
       "      <td>1.728937e-04</td>\n",
       "      <td>-7.519117e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.682333e-01</td>\n",
       "      <td>3.721330e-04</td>\n",
       "      <td>-2.642369e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.949304e-01</td>\n",
       "      <td>9.506853e-01</td>\n",
       "      <td>9.897798e-01</td>\n",
       "      <td>9.165110e-01</td>\n",
       "      <td>8.733798e-01</td>\n",
       "      <td>9.473447e-01</td>\n",
       "      <td>8.719701e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.236226e+00</td>\n",
       "      <td>7.382142e-01</td>\n",
       "      <td>8.709940e-01</td>\n",
       "      <td>1.293056e+00</td>\n",
       "      <td>8.714708e-01</td>\n",
       "      <td>1.170740e+00</td>\n",
       "      <td>6.871941e-01</td>\n",
       "      <td>8.683126e-01</td>\n",
       "      <td>2.173076e+00</td>\n",
       "      <td>1.201875e+00</td>\n",
       "      <td>6.945924e-01</td>\n",
       "      <td>8.698727e-01</td>\n",
       "      <td>2.214872e+00</td>\n",
       "      <td>1.221798e+00</td>\n",
       "      <td>7.001541e-01</td>\n",
       "      <td>8.713947e-01</td>\n",
       "      <td>2.548224e+00</td>\n",
       "      <td>1.220930e+00</td>\n",
       "      <td>7.141017e-01</td>\n",
       "      <td>8.716055e-01</td>\n",
       "      <td>3.101961e+00</td>\n",
       "      <td>1.024730e+00</td>\n",
       "      <td>1.083493e+00</td>\n",
       "      <td>1.020528e+00</td>\n",
       "      <td>1.142226e+00</td>\n",
       "      <td>1.138439e+00</td>\n",
       "      <td>1.140458e+00</td>\n",
       "      <td>1.059248e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.209891e+01</td>\n",
       "      <td>2.434868e+00</td>\n",
       "      <td>1.743236e+00</td>\n",
       "      <td>1.539682e+01</td>\n",
       "      <td>1.743257e+00</td>\n",
       "      <td>9.940391e+00</td>\n",
       "      <td>2.969674e+00</td>\n",
       "      <td>1.741454e+00</td>\n",
       "      <td>2.173076e+00</td>\n",
       "      <td>1.164708e+01</td>\n",
       "      <td>2.913210e+00</td>\n",
       "      <td>1.743175e+00</td>\n",
       "      <td>2.214872e+00</td>\n",
       "      <td>1.470899e+01</td>\n",
       "      <td>2.730009e+00</td>\n",
       "      <td>1.742884e+00</td>\n",
       "      <td>2.548224e+00</td>\n",
       "      <td>1.288257e+01</td>\n",
       "      <td>2.498009e+00</td>\n",
       "      <td>1.743372e+00</td>\n",
       "      <td>3.101961e+00</td>\n",
       "      <td>4.019237e+01</td>\n",
       "      <td>2.037278e+01</td>\n",
       "      <td>7.992739e+00</td>\n",
       "      <td>1.426244e+01</td>\n",
       "      <td>1.776285e+01</td>\n",
       "      <td>1.149652e+01</td>\n",
       "      <td>8.374498e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target     lepton_ph    lepton_eta    lepton_phi  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   5.299203e-01  9.914658e-01 -8.297618e-06 -1.327225e-05   \n",
       "std    4.991040e-01  5.653777e-01  1.008827e+00  1.006346e+00   \n",
       "min    0.000000e+00  2.746966e-01 -2.434976e+00 -1.742508e+00   \n",
       "25%    0.000000e+00  5.907533e-01 -7.383225e-01 -8.719308e-01   \n",
       "50%    1.000000e+00  8.533714e-01 -5.415563e-05 -2.410638e-04   \n",
       "75%    1.000000e+00  1.236226e+00  7.382142e-01  8.709940e-01   \n",
       "max    1.000000e+00  1.209891e+01  2.434868e+00  1.743236e+00   \n",
       "\n",
       "       missing_energy_magnitude  missing_energy_phi      jet_1_pt  \\\n",
       "count              1.100000e+07        1.100000e+07  1.100000e+07   \n",
       "mean               9.985364e-01        2.613459e-05  9.909152e-01   \n",
       "std                6.000185e-01        1.006326e+00  4.749747e-01   \n",
       "min                2.370088e-04       -1.743944e+00  1.375024e-01   \n",
       "25%                5.768156e-01       -8.712081e-01  6.789927e-01   \n",
       "50%                8.916277e-01        2.125454e-04  8.948193e-01   \n",
       "75%                1.293056e+00        8.714708e-01  1.170740e+00   \n",
       "max                1.539682e+01        1.743257e+00  9.940391e+00   \n",
       "\n",
       "          jet_1_eta     jet_1_phi    jet_1_btag      jet_2_pt     jet_2_eta  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean  -2.027520e-05  7.716199e-06  9.999687e-01  9.927294e-01 -1.026444e-05   \n",
       "std    1.009303e+00  1.005901e+00  1.027808e+00  4.999939e-01  1.009331e+00   \n",
       "min   -2.969725e+00 -1.741237e+00  0.000000e+00  1.889811e-01 -2.913090e+00   \n",
       "25%   -6.872450e-01 -8.680962e-01  0.000000e+00  6.564608e-01 -6.944718e-01   \n",
       "50%   -2.543566e-05  5.813991e-05  1.086538e+00  8.901377e-01  6.027267e-05   \n",
       "75%    6.871941e-01  8.683126e-01  2.173076e+00  1.201875e+00  6.945924e-01   \n",
       "max    2.969674e+00  1.741454e+00  2.173076e+00  1.164708e+01  2.913210e+00   \n",
       "\n",
       "          jet_2_phi    jet_2_btag      jet_3_pt     jet_3_eta     jet_3_phi  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean  -2.076887e-05  1.000008e+00  9.922591e-01  1.459561e-05  3.678632e-06   \n",
       "std    1.006154e+00  1.049398e+00  4.876623e-01  1.008747e+00  1.006305e+00   \n",
       "min   -1.742372e+00  0.000000e+00  2.636076e-01 -2.729663e+00 -1.742069e+00   \n",
       "25%   -8.701791e-01  0.000000e+00  6.508527e-01 -6.998083e-01 -8.711343e-01   \n",
       "50%    3.514990e-04  0.000000e+00  8.972494e-01  1.728937e-04 -7.519117e-04   \n",
       "75%    8.698727e-01  2.214872e+00  1.221798e+00  7.001541e-01  8.713947e-01   \n",
       "max    1.743175e+00  2.214872e+00  1.470899e+01  2.730009e+00  1.742884e+00   \n",
       "\n",
       "         jet_3_btag      jet_4_pt     jet_4_eta     jet_4_phi    jet_4_btag  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   1.000011e+00  9.861087e-01 -5.756954e-06  1.744903e-05  1.000000e+00   \n",
       "std    1.193676e+00  5.057777e-01  1.007694e+00  1.006366e+00  1.400209e+00   \n",
       "min    0.000000e+00  3.653542e-01 -2.497265e+00 -1.742691e+00  0.000000e+00   \n",
       "25%    0.000000e+00  6.177673e-01 -7.141902e-01 -8.714789e-01  0.000000e+00   \n",
       "50%    0.000000e+00  8.682333e-01  3.721330e-04 -2.642369e-04  0.000000e+00   \n",
       "75%    2.548224e+00  1.220930e+00  7.141017e-01  8.716055e-01  3.101961e+00   \n",
       "max    2.548224e+00  1.288257e+01  2.498009e+00  1.743372e+00  3.101961e+00   \n",
       "\n",
       "               m_jj         m_jjj          m_lv         m_jlv          m_bb  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   1.034290e+00  1.024805e+00  1.050554e+00  1.009742e+00  9.729596e-01   \n",
       "std    6.746354e-01  3.808074e-01  1.645763e-01  3.974453e-01  5.254063e-01   \n",
       "min    7.507046e-02  1.986757e-01  8.304866e-02  1.320062e-01  4.786215e-02   \n",
       "25%    7.906095e-01  8.462266e-01  9.857525e-01  7.675732e-01  6.738168e-01   \n",
       "50%    8.949304e-01  9.506853e-01  9.897798e-01  9.165110e-01  8.733798e-01   \n",
       "75%    1.024730e+00  1.083493e+00  1.020528e+00  1.142226e+00  1.138439e+00   \n",
       "max    4.019237e+01  2.037278e+01  7.992739e+00  1.426244e+01  1.776285e+01   \n",
       "\n",
       "              m_wbb        m_wwbb  \n",
       "count  1.100000e+07  1.100000e+07  \n",
       "mean   1.033036e+00  9.598120e-01  \n",
       "std    3.652556e-01  3.133378e-01  \n",
       "min    2.951122e-01  3.307214e-01  \n",
       "25%    8.193964e-01  7.703901e-01  \n",
       "50%    9.473447e-01  8.719701e-01  \n",
       "75%    1.140458e+00  1.059248e+00  \n",
       "max    1.149652e+01  8.374498e+00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#figure out which columns have values strictly greater than 0, for scaler purposes\n",
    "pd.set_option(\"display.max_rows\", 500, \"display.max_columns\", None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600000, 29)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "pre_X = train.loc[:, df.columns != 'target']\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_train = scaler.fit_transform(pre_X)\n",
    "# X = pd.DataFrame(data=scaled_train, columns=pre_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.695209e-01</td>\n",
       "      <td>-6.625666e-01</td>\n",
       "      <td>-7.968404e-01</td>\n",
       "      <td>-6.073939e-01</td>\n",
       "      <td>-2.264619e-01</td>\n",
       "      <td>-4.952153e-01</td>\n",
       "      <td>-4.221755e-01</td>\n",
       "      <td>-1.084725e+00</td>\n",
       "      <td>-4.670169e+00</td>\n",
       "      <td>-9.966000e-01</td>\n",
       "      <td>-7.556114e-01</td>\n",
       "      <td>-9.412959e-01</td>\n",
       "      <td>-9.336352e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.906835e-01</td>\n",
       "      <td>2.969626e-01</td>\n",
       "      <td>3.428450e-01</td>\n",
       "      <td>3.272016e-01</td>\n",
       "      <td>2.719195e-01</td>\n",
       "      <td>2.992355e-01</td>\n",
       "      <td>6.380529e-01</td>\n",
       "      <td>5.307856e-01</td>\n",
       "      <td>6.057191e-01</td>\n",
       "      <td>3.903798e-01</td>\n",
       "      <td>4.299310e-01</td>\n",
       "      <td>4.150297e-01</td>\n",
       "      <td>3.953860e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.556306e-01</td>\n",
       "      <td>8.220505e-01</td>\n",
       "      <td>7.978708e-01</td>\n",
       "      <td>7.947510e-01</td>\n",
       "      <td>7.664560e-01</td>\n",
       "      <td>8.050897e-01</td>\n",
       "      <td>7.926677e-01</td>\n",
       "      <td>8.047199e-01</td>\n",
       "      <td>6.301876e-01</td>\n",
       "      <td>7.658091e-01</td>\n",
       "      <td>8.108293e-01</td>\n",
       "      <td>7.650499e-01</td>\n",
       "      <td>7.195430e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.432801e+00</td>\n",
       "      <td>1.489800e+00</td>\n",
       "      <td>1.378221e+00</td>\n",
       "      <td>1.418066e+00</td>\n",
       "      <td>1.464286e+00</td>\n",
       "      <td>1.471996e+00</td>\n",
       "      <td>9.855056e-01</td>\n",
       "      <td>1.154184e+00</td>\n",
       "      <td>8.183915e-01</td>\n",
       "      <td>1.333369e+00</td>\n",
       "      <td>1.315954e+00</td>\n",
       "      <td>1.293396e+00</td>\n",
       "      <td>1.316636e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.991209e+01</td>\n",
       "      <td>2.073755e+01</td>\n",
       "      <td>1.696594e+01</td>\n",
       "      <td>1.894311e+01</td>\n",
       "      <td>2.448835e+01</td>\n",
       "      <td>2.914014e+01</td>\n",
       "      <td>4.321080e+01</td>\n",
       "      <td>3.756345e+01</td>\n",
       "      <td>3.609834e+01</td>\n",
       "      <td>2.720432e+01</td>\n",
       "      <td>2.530246e+01</td>\n",
       "      <td>2.221914e+01</td>\n",
       "      <td>1.902509e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lepton_ph  missing_energy_magnitude      jet_1_pt      jet_2_pt  \\\n",
       "count  2.600000e+06              2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.000000e+00              1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00              1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.695209e-01             -6.625666e-01 -7.968404e-01 -6.073939e-01   \n",
       "25%    2.906835e-01              2.969626e-01  3.428450e-01  3.272016e-01   \n",
       "50%    7.556306e-01              8.220505e-01  7.978708e-01  7.947510e-01   \n",
       "75%    1.432801e+00              1.489800e+00  1.378221e+00  1.418066e+00   \n",
       "max    1.991209e+01              2.073755e+01  1.696594e+01  1.894311e+01   \n",
       "\n",
       "           jet_4_pt      jet_3_pt          m_jj         m_jjj          m_lv  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.264619e-01 -4.952153e-01 -4.221755e-01 -1.084725e+00 -4.670169e+00   \n",
       "25%    2.719195e-01  2.992355e-01  6.380529e-01  5.307856e-01  6.057191e-01   \n",
       "50%    7.664560e-01  8.050897e-01  7.926677e-01  8.047199e-01  6.301876e-01   \n",
       "75%    1.464286e+00  1.471996e+00  9.855056e-01  1.154184e+00  8.183915e-01   \n",
       "max    2.448835e+01  2.914014e+01  4.321080e+01  3.756345e+01  3.609834e+01   \n",
       "\n",
       "              m_jlv          m_bb         m_wbb        m_wwbb  \n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "min   -9.966000e-01 -7.556114e-01 -9.412959e-01 -9.336352e-01  \n",
       "25%    3.903798e-01  4.299310e-01  4.150297e-01  3.953860e-01  \n",
       "50%    7.658091e-01  8.108293e-01  7.650499e-01  7.195430e-01  \n",
       "75%    1.333369e+00  1.315954e+00  1.293396e+00  1.316636e+00  \n",
       "max    2.720432e+01  2.530246e+01  2.221914e+01  1.902509e+01  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not complete, need columns where mean=1 and stdev=1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#NOT strictly greater than 0 columns:\n",
    "not_greater_than_0 = ['lepton_eta','lepton_phi','missing_energy_phi','jet_1_eta','jet_1_phi','jet_1_btag',\n",
    "                      'jet_2_eta','jet_2_phi','jet_2_btag','jet_3_eta','jet_3_phi','jet_3_btag','jet_4_eta',\n",
    "                      'jet_4_phi','jet_4_btag']\n",
    "\n",
    "#strictly greater than 0 columns:\n",
    "greater_than_0 = ['lepton_ph','missing_energy_magnitude','jet_1_pt','jet_2_pt','jet_4_pt','jet_3_pt','m_jj','m_jjj','m_lv',\n",
    "                  'm_jlv','m_bb','m_wbb','m_wwbb']\n",
    "\n",
    "\n",
    "#these columns scale where mean=0 and stdev=1\n",
    "to_scale1 = pre_X[not_greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train1 = scaler.fit_transform(to_scale1)\n",
    "scaled_train_df1 = pd.DataFrame(scaled_train1, columns=not_greater_than_0)\n",
    "\n",
    "#these columns scale where mean=1 and stdev=1\n",
    "to_scale2 = pre_X[greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train2 = scaler.fit_transform(to_scale2)\n",
    "scaled_train_df2 = pd.DataFrame(scaled_train2 + 1, columns=greater_than_0)\n",
    "scaled_train_df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600000, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled_train2 + 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_X[greater_than_0].to_numpy().shape\n",
    "\n",
    "# np_greater_than_0 = pre_X[greater_than_0].to_numpy()\n",
    "\n",
    "def manual_scaling(x, desired_mean=0, desired_std=1):\n",
    "    new_x = np.empty(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        mean = np.mean(x[:, i])\n",
    "        std = np.std(x[:, i])\n",
    "        new_x[:, i] = desired_mean + (x[:, i] - mean) * (desired_std / std)\n",
    "    return new_x\n",
    "\n",
    "np_greater_than_0 = manual_scaling(pre_X[greater_than_0].to_numpy(), desired_mean=1, desired_std=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.175,  0.16 , -0.132, ...,  1.105,  1.272,  0.858],\n",
       "       [ 0.51 ,  4.281,  1.223, ...,  0.709,  1.38 ,  1.777],\n",
       "       [-0.131,  0.97 ,  0.063, ...,  0.414,  0.217,  0.345],\n",
       "       ...,\n",
       "       [ 0.141,  0.259,  1.578, ...,  0.017,  1.233,  0.923],\n",
       "       [ 1.813,  0.645,  1.675, ...,  1.379,  1.363,  1.836],\n",
       "       [ 0.164,  0.846,  1.208, ...,  1.776,  0.987,  0.697]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round((np_greater_than_0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.175,  0.16 , -0.132, ...,  1.105,  1.272,  0.858],\n",
       "       [ 0.51 ,  4.281,  1.223, ...,  0.709,  1.38 ,  1.777],\n",
       "       [-0.131,  0.97 ,  0.063, ...,  0.414,  0.217,  0.345],\n",
       "       ...,\n",
       "       [ 0.141,  0.259,  1.578, ...,  0.017,  1.233,  0.923],\n",
       "       [ 1.813,  0.645,  1.675, ...,  1.379,  1.363,  1.836],\n",
       "       [ 0.164,  0.846,  1.208, ...,  1.776,  0.987,  0.697]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round((scaled_train2 + 1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_they_the_same = np.round((np_greater_than_0), 3) == np.round((scaled_train2 + 1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2600000, 2600000, 2600000, 2600000, 2600000, 2600000, 2600000,\n",
       "       2600000, 2600000, 2600000, 2600000, 2600000, 2600000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(are_they_the_same == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.322691</td>\n",
       "      <td>0.863880</td>\n",
       "      <td>-0.572453</td>\n",
       "      <td>-2.141693</td>\n",
       "      <td>-0.523203</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>1.723081</td>\n",
       "      <td>-1.709792</td>\n",
       "      <td>-0.952660</td>\n",
       "      <td>0.025451</td>\n",
       "      <td>1.289919</td>\n",
       "      <td>1.296983</td>\n",
       "      <td>-0.002654</td>\n",
       "      <td>-0.786827</td>\n",
       "      <td>-0.714619</td>\n",
       "      <td>1.174569</td>\n",
       "      <td>0.160122</td>\n",
       "      <td>-0.132040</td>\n",
       "      <td>0.571042</td>\n",
       "      <td>0.458873</td>\n",
       "      <td>0.872105</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>1.043547</td>\n",
       "      <td>0.613459</td>\n",
       "      <td>0.855754</td>\n",
       "      <td>1.105013</td>\n",
       "      <td>1.271511</td>\n",
       "      <td>0.857620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416089</td>\n",
       "      <td>0.695152</td>\n",
       "      <td>0.802544</td>\n",
       "      <td>-0.470049</td>\n",
       "      <td>-0.668134</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>-0.789944</td>\n",
       "      <td>-1.505140</td>\n",
       "      <td>1.158021</td>\n",
       "      <td>0.802190</td>\n",
       "      <td>-0.192560</td>\n",
       "      <td>-0.837770</td>\n",
       "      <td>0.480915</td>\n",
       "      <td>1.476132</td>\n",
       "      <td>-0.714619</td>\n",
       "      <td>0.509711</td>\n",
       "      <td>4.281044</td>\n",
       "      <td>1.223397</td>\n",
       "      <td>2.620156</td>\n",
       "      <td>1.344617</td>\n",
       "      <td>2.941280</td>\n",
       "      <td>0.587525</td>\n",
       "      <td>0.553713</td>\n",
       "      <td>0.652878</td>\n",
       "      <td>2.778725</td>\n",
       "      <td>0.708706</td>\n",
       "      <td>1.379553</td>\n",
       "      <td>1.777043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.014625</td>\n",
       "      <td>0.186212</td>\n",
       "      <td>-1.239583</td>\n",
       "      <td>1.129981</td>\n",
       "      <td>-1.392778</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>1.064422</td>\n",
       "      <td>-0.952660</td>\n",
       "      <td>-0.055741</td>\n",
       "      <td>-0.050936</td>\n",
       "      <td>1.296983</td>\n",
       "      <td>2.108518</td>\n",
       "      <td>-0.857386</td>\n",
       "      <td>-0.714619</td>\n",
       "      <td>-0.131495</td>\n",
       "      <td>0.969540</td>\n",
       "      <td>0.063081</td>\n",
       "      <td>1.850889</td>\n",
       "      <td>1.313378</td>\n",
       "      <td>2.026490</td>\n",
       "      <td>0.902127</td>\n",
       "      <td>1.344002</td>\n",
       "      <td>0.611636</td>\n",
       "      <td>0.129997</td>\n",
       "      <td>0.414243</td>\n",
       "      <td>0.217026</td>\n",
       "      <td>0.345026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.836948</td>\n",
       "      <td>-1.475806</td>\n",
       "      <td>0.854693</td>\n",
       "      <td>2.403334</td>\n",
       "      <td>-1.289730</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>0.352078</td>\n",
       "      <td>0.411301</td>\n",
       "      <td>1.158021</td>\n",
       "      <td>-0.669193</td>\n",
       "      <td>0.629738</td>\n",
       "      <td>-0.837770</td>\n",
       "      <td>0.763617</td>\n",
       "      <td>-0.477577</td>\n",
       "      <td>1.500372</td>\n",
       "      <td>0.974982</td>\n",
       "      <td>-0.110502</td>\n",
       "      <td>0.356920</td>\n",
       "      <td>0.772858</td>\n",
       "      <td>2.591291</td>\n",
       "      <td>1.104855</td>\n",
       "      <td>0.865492</td>\n",
       "      <td>1.335313</td>\n",
       "      <td>0.656397</td>\n",
       "      <td>1.385766</td>\n",
       "      <td>0.819796</td>\n",
       "      <td>1.213709</td>\n",
       "      <td>0.937928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.177572</td>\n",
       "      <td>0.531387</td>\n",
       "      <td>-1.002016</td>\n",
       "      <td>0.825868</td>\n",
       "      <td>0.737725</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>1.490251</td>\n",
       "      <td>-0.738936</td>\n",
       "      <td>-0.952660</td>\n",
       "      <td>-0.136933</td>\n",
       "      <td>1.334005</td>\n",
       "      <td>1.296983</td>\n",
       "      <td>1.046319</td>\n",
       "      <td>-0.080681</td>\n",
       "      <td>0.392877</td>\n",
       "      <td>-0.051790</td>\n",
       "      <td>2.577822</td>\n",
       "      <td>0.874030</td>\n",
       "      <td>0.138219</td>\n",
       "      <td>1.611349</td>\n",
       "      <td>0.954612</td>\n",
       "      <td>0.795040</td>\n",
       "      <td>1.126510</td>\n",
       "      <td>1.901067</td>\n",
       "      <td>2.499051</td>\n",
       "      <td>1.154495</td>\n",
       "      <td>1.196558</td>\n",
       "      <td>1.717747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton_eta  lepton_phi  missing_energy_phi  jet_1_eta  jet_1_phi  \\\n",
       "0   -0.322691    0.863880           -0.572453  -2.141693  -0.523203   \n",
       "1    0.416089    0.695152            0.802544  -0.470049  -0.668134   \n",
       "2   -0.014625    0.186212           -1.239583   1.129981  -1.392778   \n",
       "3   -1.836948   -1.475806            0.854693   2.403334  -1.289730   \n",
       "4    2.177572    0.531387           -1.002016   0.825868   0.737725   \n",
       "\n",
       "   jet_1_btag  jet_2_eta  jet_2_phi  jet_2_btag  jet_3_eta  jet_3_phi  \\\n",
       "0   -0.972454   1.723081  -1.709792   -0.952660   0.025451   1.289919   \n",
       "1   -0.972454  -0.789944  -1.505140    1.158021   0.802190  -0.192560   \n",
       "2   -0.972454   0.016302   1.064422   -0.952660  -0.055741  -0.050936   \n",
       "3   -0.972454   0.352078   0.411301    1.158021  -0.669193   0.629738   \n",
       "4   -0.972454   1.490251  -0.738936   -0.952660  -0.136933   1.334005   \n",
       "\n",
       "   jet_3_btag  jet_4_eta  jet_4_phi  jet_4_btag  lepton_ph  \\\n",
       "0    1.296983  -0.002654  -0.786827   -0.714619   1.174569   \n",
       "1   -0.837770   0.480915   1.476132   -0.714619   0.509711   \n",
       "2    1.296983   2.108518  -0.857386   -0.714619  -0.131495   \n",
       "3   -0.837770   0.763617  -0.477577    1.500372   0.974982   \n",
       "4    1.296983   1.046319  -0.080681    0.392877  -0.051790   \n",
       "\n",
       "   missing_energy_magnitude  jet_1_pt  jet_2_pt  jet_4_pt  jet_3_pt      m_jj  \\\n",
       "0                  0.160122 -0.132040  0.571042  0.458873  0.872105  0.537537   \n",
       "1                  4.281044  1.223397  2.620156  1.344617  2.941280  0.587525   \n",
       "2                  0.969540  0.063081  1.850889  1.313378  2.026490  0.902127   \n",
       "3                 -0.110502  0.356920  0.772858  2.591291  1.104855  0.865492   \n",
       "4                  2.577822  0.874030  0.138219  1.611349  0.954612  0.795040   \n",
       "\n",
       "      m_jjj      m_lv     m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "0  1.043547  0.613459  0.855754  1.105013  1.271511  0.857620  \n",
       "1  0.553713  0.652878  2.778725  0.708706  1.379553  1.777043  \n",
       "2  1.344002  0.611636  0.129997  0.414243  0.217026  0.345026  \n",
       "3  1.335313  0.656397  1.385766  0.819796  1.213709  0.937928  \n",
       "4  1.126510  1.901067  2.499051  1.154495  1.196558  1.717747  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([scaled_train_df1, scaled_train_df2], axis=1, sort=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=500000, random_state=1776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = y_train.to_numpy()\n",
    "# type(np.unique(y)[0])\n",
    "\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100000, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.131178e-17</td>\n",
       "      <td>8.924613e-17</td>\n",
       "      <td>-3.083901e-17</td>\n",
       "      <td>4.807138e-18</td>\n",
       "      <td>-4.718029e-17</td>\n",
       "      <td>9.236995e-15</td>\n",
       "      <td>-8.113305e-17</td>\n",
       "      <td>-2.608358e-17</td>\n",
       "      <td>2.392281e-15</td>\n",
       "      <td>2.121924e-16</td>\n",
       "      <td>-2.010950e-16</td>\n",
       "      <td>1.225050e-14</td>\n",
       "      <td>-8.899181e-17</td>\n",
       "      <td>2.780400e-17</td>\n",
       "      <td>-2.669054e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.414452e+00</td>\n",
       "      <td>-1.731104e+00</td>\n",
       "      <td>-1.731925e+00</td>\n",
       "      <td>-2.942199e+00</td>\n",
       "      <td>-1.730027e+00</td>\n",
       "      <td>-9.724543e-01</td>\n",
       "      <td>-2.885415e+00</td>\n",
       "      <td>-1.732408e+00</td>\n",
       "      <td>-9.526596e-01</td>\n",
       "      <td>-2.705312e+00</td>\n",
       "      <td>-1.730599e+00</td>\n",
       "      <td>-8.377697e-01</td>\n",
       "      <td>-2.479189e+00</td>\n",
       "      <td>-1.731107e+00</td>\n",
       "      <td>-7.146187e-01</td>\n",
       "      <td>-2.695209e-01</td>\n",
       "      <td>-6.625666e-01</td>\n",
       "      <td>-7.968404e-01</td>\n",
       "      <td>-6.073939e-01</td>\n",
       "      <td>-2.264619e-01</td>\n",
       "      <td>-4.952153e-01</td>\n",
       "      <td>-4.221755e-01</td>\n",
       "      <td>-1.084725e+00</td>\n",
       "      <td>-4.670169e+00</td>\n",
       "      <td>-9.966000e-01</td>\n",
       "      <td>-7.556114e-01</td>\n",
       "      <td>-9.412959e-01</td>\n",
       "      <td>-9.336352e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.321584e-01</td>\n",
       "      <td>-8.659604e-01</td>\n",
       "      <td>-8.658190e-01</td>\n",
       "      <td>-6.819477e-01</td>\n",
       "      <td>-8.632094e-01</td>\n",
       "      <td>-9.724543e-01</td>\n",
       "      <td>-6.889230e-01</td>\n",
       "      <td>-8.647062e-01</td>\n",
       "      <td>-9.526596e-01</td>\n",
       "      <td>-6.935506e-01</td>\n",
       "      <td>-8.659706e-01</td>\n",
       "      <td>-8.377697e-01</td>\n",
       "      <td>-7.085818e-01</td>\n",
       "      <td>-8.667571e-01</td>\n",
       "      <td>-7.146187e-01</td>\n",
       "      <td>2.906835e-01</td>\n",
       "      <td>2.969626e-01</td>\n",
       "      <td>3.428450e-01</td>\n",
       "      <td>3.272016e-01</td>\n",
       "      <td>2.719195e-01</td>\n",
       "      <td>2.992355e-01</td>\n",
       "      <td>6.380529e-01</td>\n",
       "      <td>5.307856e-01</td>\n",
       "      <td>6.057191e-01</td>\n",
       "      <td>3.903798e-01</td>\n",
       "      <td>4.299310e-01</td>\n",
       "      <td>4.150297e-01</td>\n",
       "      <td>3.953860e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.386904e-04</td>\n",
       "      <td>-1.365973e-03</td>\n",
       "      <td>2.090639e-04</td>\n",
       "      <td>-1.125500e-03</td>\n",
       "      <td>-7.977719e-04</td>\n",
       "      <td>8.484855e-02</td>\n",
       "      <td>9.082319e-04</td>\n",
       "      <td>8.939700e-04</td>\n",
       "      <td>-9.526596e-01</td>\n",
       "      <td>1.093501e-03</td>\n",
       "      <td>3.138265e-04</td>\n",
       "      <td>-8.377697e-01</td>\n",
       "      <td>6.528141e-04</td>\n",
       "      <td>-1.998949e-04</td>\n",
       "      <td>-7.146187e-01</td>\n",
       "      <td>7.556306e-01</td>\n",
       "      <td>8.220505e-01</td>\n",
       "      <td>7.978708e-01</td>\n",
       "      <td>7.947510e-01</td>\n",
       "      <td>7.664560e-01</td>\n",
       "      <td>8.050897e-01</td>\n",
       "      <td>7.926677e-01</td>\n",
       "      <td>8.047199e-01</td>\n",
       "      <td>6.301876e-01</td>\n",
       "      <td>7.658091e-01</td>\n",
       "      <td>8.108293e-01</td>\n",
       "      <td>7.650499e-01</td>\n",
       "      <td>7.195430e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.318811e-01</td>\n",
       "      <td>8.655338e-01</td>\n",
       "      <td>8.662048e-01</td>\n",
       "      <td>6.806777e-01</td>\n",
       "      <td>8.628153e-01</td>\n",
       "      <td>1.142151e+00</td>\n",
       "      <td>6.888153e-01</td>\n",
       "      <td>8.641829e-01</td>\n",
       "      <td>1.158021e+00</td>\n",
       "      <td>6.939334e-01</td>\n",
       "      <td>8.661466e-01</td>\n",
       "      <td>1.296983e+00</td>\n",
       "      <td>7.082342e-01</td>\n",
       "      <td>8.659056e-01</td>\n",
       "      <td>1.500372e+00</td>\n",
       "      <td>1.432801e+00</td>\n",
       "      <td>1.489800e+00</td>\n",
       "      <td>1.378221e+00</td>\n",
       "      <td>1.418066e+00</td>\n",
       "      <td>1.464286e+00</td>\n",
       "      <td>1.471996e+00</td>\n",
       "      <td>9.855056e-01</td>\n",
       "      <td>1.154184e+00</td>\n",
       "      <td>8.183915e-01</td>\n",
       "      <td>1.333369e+00</td>\n",
       "      <td>1.315954e+00</td>\n",
       "      <td>1.293396e+00</td>\n",
       "      <td>1.316636e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.414175e+00</td>\n",
       "      <td>1.732883e+00</td>\n",
       "      <td>1.732090e+00</td>\n",
       "      <td>2.941910e+00</td>\n",
       "      <td>1.731837e+00</td>\n",
       "      <td>1.142151e+00</td>\n",
       "      <td>2.885307e+00</td>\n",
       "      <td>1.732988e+00</td>\n",
       "      <td>1.158021e+00</td>\n",
       "      <td>2.705695e+00</td>\n",
       "      <td>1.731326e+00</td>\n",
       "      <td>1.296983e+00</td>\n",
       "      <td>2.478841e+00</td>\n",
       "      <td>1.731909e+00</td>\n",
       "      <td>1.500372e+00</td>\n",
       "      <td>1.991209e+01</td>\n",
       "      <td>2.073755e+01</td>\n",
       "      <td>1.696594e+01</td>\n",
       "      <td>1.894311e+01</td>\n",
       "      <td>2.448835e+01</td>\n",
       "      <td>2.914014e+01</td>\n",
       "      <td>4.321080e+01</td>\n",
       "      <td>3.756345e+01</td>\n",
       "      <td>3.609834e+01</td>\n",
       "      <td>2.720432e+01</td>\n",
       "      <td>2.530246e+01</td>\n",
       "      <td>2.221914e+01</td>\n",
       "      <td>1.902509e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lepton_eta    lepton_phi  missing_energy_phi     jet_1_eta  \\\n",
       "count  2.600000e+06  2.600000e+06        2.600000e+06  2.600000e+06   \n",
       "mean   4.131178e-17  8.924613e-17       -3.083901e-17  4.807138e-18   \n",
       "std    1.000000e+00  1.000000e+00        1.000000e+00  1.000000e+00   \n",
       "min   -2.414452e+00 -1.731104e+00       -1.731925e+00 -2.942199e+00   \n",
       "25%   -7.321584e-01 -8.659604e-01       -8.658190e-01 -6.819477e-01   \n",
       "50%   -1.386904e-04 -1.365973e-03        2.090639e-04 -1.125500e-03   \n",
       "75%    7.318811e-01  8.655338e-01        8.662048e-01  6.806777e-01   \n",
       "max    2.414175e+00  1.732883e+00        1.732090e+00  2.941910e+00   \n",
       "\n",
       "          jet_1_phi    jet_1_btag     jet_2_eta     jet_2_phi    jet_2_btag  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean  -4.718029e-17  9.236995e-15 -8.113305e-17 -2.608358e-17  2.392281e-15   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -1.730027e+00 -9.724543e-01 -2.885415e+00 -1.732408e+00 -9.526596e-01   \n",
       "25%   -8.632094e-01 -9.724543e-01 -6.889230e-01 -8.647062e-01 -9.526596e-01   \n",
       "50%   -7.977719e-04  8.484855e-02  9.082319e-04  8.939700e-04 -9.526596e-01   \n",
       "75%    8.628153e-01  1.142151e+00  6.888153e-01  8.641829e-01  1.158021e+00   \n",
       "max    1.731837e+00  1.142151e+00  2.885307e+00  1.732988e+00  1.158021e+00   \n",
       "\n",
       "          jet_3_eta     jet_3_phi    jet_3_btag     jet_4_eta     jet_4_phi  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   2.121924e-16 -2.010950e-16  1.225050e-14 -8.899181e-17  2.780400e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.705312e+00 -1.730599e+00 -8.377697e-01 -2.479189e+00 -1.731107e+00   \n",
       "25%   -6.935506e-01 -8.659706e-01 -8.377697e-01 -7.085818e-01 -8.667571e-01   \n",
       "50%    1.093501e-03  3.138265e-04 -8.377697e-01  6.528141e-04 -1.998949e-04   \n",
       "75%    6.939334e-01  8.661466e-01  1.296983e+00  7.082342e-01  8.659056e-01   \n",
       "max    2.705695e+00  1.731326e+00  1.296983e+00  2.478841e+00  1.731909e+00   \n",
       "\n",
       "         jet_4_btag     lepton_ph  missing_energy_magnitude      jet_1_pt  \\\n",
       "count  2.600000e+06  2.600000e+06              2.600000e+06  2.600000e+06   \n",
       "mean  -2.669054e-16  1.000000e+00              1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00  1.000000e+00              1.000000e+00  1.000000e+00   \n",
       "min   -7.146187e-01 -2.695209e-01             -6.625666e-01 -7.968404e-01   \n",
       "25%   -7.146187e-01  2.906835e-01              2.969626e-01  3.428450e-01   \n",
       "50%   -7.146187e-01  7.556306e-01              8.220505e-01  7.978708e-01   \n",
       "75%    1.500372e+00  1.432801e+00              1.489800e+00  1.378221e+00   \n",
       "max    1.500372e+00  1.991209e+01              2.073755e+01  1.696594e+01   \n",
       "\n",
       "           jet_2_pt      jet_4_pt      jet_3_pt          m_jj         m_jjj  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -6.073939e-01 -2.264619e-01 -4.952153e-01 -4.221755e-01 -1.084725e+00   \n",
       "25%    3.272016e-01  2.719195e-01  2.992355e-01  6.380529e-01  5.307856e-01   \n",
       "50%    7.947510e-01  7.664560e-01  8.050897e-01  7.926677e-01  8.047199e-01   \n",
       "75%    1.418066e+00  1.464286e+00  1.471996e+00  9.855056e-01  1.154184e+00   \n",
       "max    1.894311e+01  2.448835e+01  2.914014e+01  4.321080e+01  3.756345e+01   \n",
       "\n",
       "               m_lv         m_jlv          m_bb         m_wbb        m_wwbb  \n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "min   -4.670169e+00 -9.966000e-01 -7.556114e-01 -9.412959e-01 -9.336352e-01  \n",
       "25%    6.057191e-01  3.903798e-01  4.299310e-01  4.150297e-01  3.953860e-01  \n",
       "50%    6.301876e-01  7.658091e-01  8.108293e-01  7.650499e-01  7.195430e-01  \n",
       "75%    8.183915e-01  1.333369e+00  1.315954e+00  1.293396e+00  1.316636e+00  \n",
       "max    3.609834e+01  2.720432e+01  2.530246e+01  2.221914e+01  1.902509e+01  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# input\n",
    "model.add(tf.keras.Input(shape=(28,)))\n",
    "# hidden\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.1)))  \n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(1,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.001)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=1e-5)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "21000/21000 [==============================] - 62s 3ms/step - loss: 0.5690 - auc: 0.7712 - val_loss: 0.5400 - val_auc: 0.8027\n",
      "Epoch 2/250\n",
      "21000/21000 [==============================] - 32624s 2s/step - loss: 0.5274 - auc: 0.8106 - val_loss: 0.5220 - val_auc: 0.8157\n",
      "Epoch 3/250\n",
      "21000/21000 [==============================] - 43s 2ms/step - loss: 0.5142 - auc: 0.8215 - val_loss: 0.5125 - val_auc: 0.8251\n",
      "Epoch 4/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.5059 - auc: 0.8280 - val_loss: 0.5031 - val_auc: 0.8307\n",
      "Epoch 5/250\n",
      "21000/21000 [==============================] - 57s 3ms/step - loss: 0.5002 - auc: 0.8324 - val_loss: 0.4973 - val_auc: 0.8349\n",
      "Epoch 6/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4958 - auc: 0.8358 - val_loss: 0.4955 - val_auc: 0.8360\n",
      "Epoch 7/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4924 - auc: 0.8383 - val_loss: 0.4926 - val_auc: 0.8387\n",
      "Epoch 8/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4896 - auc: 0.8405 - val_loss: 0.4941 - val_auc: 0.8383\n",
      "Epoch 9/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4873 - auc: 0.8423 - val_loss: 0.4903 - val_auc: 0.8402\n",
      "Epoch 10/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4851 - auc: 0.8438 - val_loss: 0.4880 - val_auc: 0.8415\n",
      "Epoch 11/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4832 - auc: 0.8452 - val_loss: 0.4867 - val_auc: 0.8426\n",
      "Epoch 12/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4815 - auc: 0.8464 - val_loss: 0.4878 - val_auc: 0.8417\n",
      "Epoch 13/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4799 - auc: 0.8476 - val_loss: 0.4880 - val_auc: 0.8435\n",
      "Epoch 14/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4783 - auc: 0.8487 - val_loss: 0.4856 - val_auc: 0.8437\n",
      "Epoch 15/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4769 - auc: 0.8497 - val_loss: 0.4863 - val_auc: 0.8434\n",
      "Epoch 16/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4756 - auc: 0.8507 - val_loss: 0.4861 - val_auc: 0.8438\n",
      "Epoch 17/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4742 - auc: 0.8517 - val_loss: 0.4835 - val_auc: 0.8452\n",
      "Epoch 18/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4729 - auc: 0.8526 - val_loss: 0.4861 - val_auc: 0.8454\n",
      "Epoch 19/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4716 - auc: 0.8535 - val_loss: 0.4828 - val_auc: 0.8456\n",
      "Epoch 20/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4705 - auc: 0.8543 - val_loss: 0.4826 - val_auc: 0.8459\n",
      "Epoch 21/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4694 - auc: 0.8551 - val_loss: 0.4843 - val_auc: 0.8453\n",
      "Epoch 22/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4683 - auc: 0.8558 - val_loss: 0.4844 - val_auc: 0.8450\n",
      "Epoch 23/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4671 - auc: 0.8567 - val_loss: 0.4811 - val_auc: 0.8470\n",
      "Epoch 24/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4661 - auc: 0.8573 - val_loss: 0.4846 - val_auc: 0.8454\n",
      "Epoch 25/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4650 - auc: 0.8581 - val_loss: 0.4832 - val_auc: 0.8460\n",
      "Epoch 26/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4640 - auc: 0.8588 - val_loss: 0.4813 - val_auc: 0.8474\n",
      "Epoch 27/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4630 - auc: 0.8595 - val_loss: 0.4831 - val_auc: 0.8460\n",
      "Epoch 28/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4619 - auc: 0.8602 - val_loss: 0.4824 - val_auc: 0.8466\n",
      "Epoch 29/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4608 - auc: 0.8610 - val_loss: 0.4830 - val_auc: 0.8459\n",
      "Epoch 30/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4599 - auc: 0.8616 - val_loss: 0.4828 - val_auc: 0.8467\n",
      "Epoch 31/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4588 - auc: 0.8623 - val_loss: 0.4825 - val_auc: 0.8464\n",
      "Epoch 32/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4579 - auc: 0.8629 - val_loss: 0.4855 - val_auc: 0.8455\n",
      "Epoch 33/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4570 - auc: 0.8635 - val_loss: 0.4844 - val_auc: 0.8457\n",
      "Epoch 34/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4560 - auc: 0.8642 - val_loss: 0.4831 - val_auc: 0.8464\n",
      "Epoch 35/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4550 - auc: 0.8649 - val_loss: 0.4841 - val_auc: 0.8458\n",
      "Epoch 36/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4539 - auc: 0.8656 - val_loss: 0.4874 - val_auc: 0.8440\n",
      "Epoch 37/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4531 - auc: 0.8661 - val_loss: 0.4846 - val_auc: 0.8459\n",
      "Epoch 38/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4520 - auc: 0.8669 - val_loss: 0.4869 - val_auc: 0.8448\n",
      "Epoch 39/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4510 - auc: 0.8675 - val_loss: 0.4850 - val_auc: 0.8454\n",
      "Epoch 40/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4500 - auc: 0.8681 - val_loss: 0.4880 - val_auc: 0.8436\n",
      "Epoch 41/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4490 - auc: 0.8689 - val_loss: 0.4867 - val_auc: 0.8445\n",
      "Epoch 42/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4480 - auc: 0.8695 - val_loss: 0.4897 - val_auc: 0.8431\n",
      "Epoch 43/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4470 - auc: 0.8701 - val_loss: 0.4894 - val_auc: 0.8441\n",
      "Epoch 44/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4459 - auc: 0.8708 - val_loss: 0.4894 - val_auc: 0.8433\n",
      "Epoch 45/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4449 - auc: 0.8715 - val_loss: 0.4892 - val_auc: 0.8428\n",
      "Epoch 46/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4439 - auc: 0.8721 - val_loss: 0.4900 - val_auc: 0.8425\n",
      "Epoch 47/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4429 - auc: 0.8727 - val_loss: 0.4909 - val_auc: 0.8424\n",
      "Epoch 48/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4419 - auc: 0.8734 - val_loss: 0.4915 - val_auc: 0.8427\n",
      "Epoch 49/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4409 - auc: 0.8740 - val_loss: 0.4929 - val_auc: 0.8413\n",
      "Epoch 50/250\n",
      "21000/21000 [==============================] - 73s 3ms/step - loss: 0.4397 - auc: 0.8748 - val_loss: 0.4938 - val_auc: 0.8406\n",
      "Epoch 51/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4388 - auc: 0.8754 - val_loss: 0.4948 - val_auc: 0.8401\n",
      "Epoch 52/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4376 - auc: 0.8761 - val_loss: 0.4967 - val_auc: 0.8407\n",
      "Epoch 53/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4368 - auc: 0.8766 - val_loss: 0.4964 - val_auc: 0.8403\n",
      "Epoch 54/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4356 - auc: 0.8773 - val_loss: 0.5004 - val_auc: 0.8379\n",
      "Epoch 55/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4346 - auc: 0.8780 - val_loss: 0.5001 - val_auc: 0.8397\n",
      "Epoch 56/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4336 - auc: 0.8786 - val_loss: 0.4989 - val_auc: 0.8390\n",
      "Epoch 57/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4326 - auc: 0.8792 - val_loss: 0.4983 - val_auc: 0.8384\n",
      "Epoch 58/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4315 - auc: 0.8799 - val_loss: 0.5008 - val_auc: 0.8384\n",
      "Epoch 59/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4306 - auc: 0.8805 - val_loss: 0.5008 - val_auc: 0.8384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4295 - auc: 0.8812 - val_loss: 0.5029 - val_auc: 0.8377\n",
      "Epoch 61/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4285 - auc: 0.8817 - val_loss: 0.5040 - val_auc: 0.8367\n",
      "Epoch 62/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4274 - auc: 0.8824 - val_loss: 0.5059 - val_auc: 0.8362\n",
      "Epoch 63/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4264 - auc: 0.8830 - val_loss: 0.5078 - val_auc: 0.8348\n",
      "Epoch 64/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4254 - auc: 0.8836 - val_loss: 0.5062 - val_auc: 0.8362\n",
      "Epoch 65/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4243 - auc: 0.8843 - val_loss: 0.5096 - val_auc: 0.8353\n",
      "Epoch 66/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4234 - auc: 0.8848 - val_loss: 0.5091 - val_auc: 0.8352\n",
      "Epoch 67/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4223 - auc: 0.8855 - val_loss: 0.5117 - val_auc: 0.8341\n",
      "Epoch 68/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4213 - auc: 0.8860 - val_loss: 0.5137 - val_auc: 0.8339\n",
      "Epoch 69/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4203 - auc: 0.8867 - val_loss: 0.5132 - val_auc: 0.8333\n",
      "Epoch 70/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4193 - auc: 0.8872 - val_loss: 0.5127 - val_auc: 0.8329\n",
      "Epoch 71/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4184 - auc: 0.8878 - val_loss: 0.5136 - val_auc: 0.8324\n",
      "Epoch 72/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4173 - auc: 0.8884 - val_loss: 0.5176 - val_auc: 0.8312\n",
      "Epoch 73/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4163 - auc: 0.8890 - val_loss: 0.5165 - val_auc: 0.8310\n",
      "Epoch 74/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4155 - auc: 0.8894 - val_loss: 0.5216 - val_auc: 0.8301\n",
      "Epoch 75/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4145 - auc: 0.8900 - val_loss: 0.5210 - val_auc: 0.8300\n",
      "Epoch 76/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4136 - auc: 0.8906 - val_loss: 0.5215 - val_auc: 0.8291\n",
      "Epoch 77/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4127 - auc: 0.8911 - val_loss: 0.5234 - val_auc: 0.8303\n",
      "Epoch 78/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4117 - auc: 0.8917 - val_loss: 0.5251 - val_auc: 0.8283\n",
      "Epoch 79/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4109 - auc: 0.8921 - val_loss: 0.5270 - val_auc: 0.8302\n",
      "Epoch 80/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4100 - auc: 0.8927 - val_loss: 0.5260 - val_auc: 0.8273\n",
      "Epoch 81/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4090 - auc: 0.8932 - val_loss: 0.5259 - val_auc: 0.8277\n",
      "Epoch 82/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4082 - auc: 0.8937 - val_loss: 0.5269 - val_auc: 0.8258\n",
      "Epoch 83/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4073 - auc: 0.8942 - val_loss: 0.5314 - val_auc: 0.8285\n",
      "Epoch 84/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4064 - auc: 0.8947 - val_loss: 0.5302 - val_auc: 0.8270\n",
      "Epoch 85/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4056 - auc: 0.8951 - val_loss: 0.5326 - val_auc: 0.8270\n",
      "Epoch 86/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4047 - auc: 0.8956 - val_loss: 0.5311 - val_auc: 0.8263\n",
      "Epoch 87/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4038 - auc: 0.8961 - val_loss: 0.5347 - val_auc: 0.8263\n",
      "Epoch 88/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4031 - auc: 0.8965 - val_loss: 0.5365 - val_auc: 0.8263\n",
      "Epoch 89/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4023 - auc: 0.8969 - val_loss: 0.5363 - val_auc: 0.8259\n",
      "Epoch 90/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4016 - auc: 0.8974 - val_loss: 0.5375 - val_auc: 0.8249\n",
      "Epoch 91/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4008 - auc: 0.8978 - val_loss: 0.5385 - val_auc: 0.8247\n",
      "Epoch 92/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4000 - auc: 0.8983 - val_loss: 0.5419 - val_auc: 0.8225\n",
      "Epoch 93/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3993 - auc: 0.8986 - val_loss: 0.5408 - val_auc: 0.8237\n",
      "Epoch 94/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3986 - auc: 0.8990 - val_loss: 0.5406 - val_auc: 0.8233\n",
      "Epoch 95/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3977 - auc: 0.8995 - val_loss: 0.5428 - val_auc: 0.8212\n",
      "Epoch 96/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3972 - auc: 0.8998 - val_loss: 0.5426 - val_auc: 0.8220\n",
      "Epoch 97/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3964 - auc: 0.9002 - val_loss: 0.5459 - val_auc: 0.8218\n",
      "Epoch 98/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3958 - auc: 0.9005 - val_loss: 0.5481 - val_auc: 0.8225\n",
      "Epoch 99/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3951 - auc: 0.9009 - val_loss: 0.5462 - val_auc: 0.8222\n",
      "Epoch 100/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3944 - auc: 0.9013 - val_loss: 0.5493 - val_auc: 0.8214\n",
      "Epoch 101/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3937 - auc: 0.9017 - val_loss: 0.5476 - val_auc: 0.8210\n",
      "Epoch 102/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3929 - auc: 0.9021 - val_loss: 0.5490 - val_auc: 0.8197\n",
      "Epoch 103/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3924 - auc: 0.9024 - val_loss: 0.5506 - val_auc: 0.8193\n",
      "Epoch 104/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3918 - auc: 0.9026 - val_loss: 0.5526 - val_auc: 0.8190\n",
      "Epoch 105/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3912 - auc: 0.9030 - val_loss: 0.5524 - val_auc: 0.8193\n",
      "Epoch 106/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3905 - auc: 0.9034 - val_loss: 0.5548 - val_auc: 0.8210\n",
      "Epoch 107/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3900 - auc: 0.9036 - val_loss: 0.5559 - val_auc: 0.8182\n",
      "Epoch 108/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3894 - auc: 0.9039 - val_loss: 0.5575 - val_auc: 0.8187\n",
      "Epoch 109/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3888 - auc: 0.9043 - val_loss: 0.5589 - val_auc: 0.8165\n",
      "Epoch 110/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3883 - auc: 0.9046 - val_loss: 0.5590 - val_auc: 0.8190\n",
      "Epoch 111/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3878 - auc: 0.9048 - val_loss: 0.5584 - val_auc: 0.8193\n",
      "Epoch 112/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3869 - auc: 0.9052 - val_loss: 0.5607 - val_auc: 0.8149\n",
      "Epoch 113/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3865 - auc: 0.9055 - val_loss: 0.5618 - val_auc: 0.8171\n",
      "Epoch 114/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3861 - auc: 0.9057 - val_loss: 0.5629 - val_auc: 0.8176\n",
      "Epoch 115/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3855 - auc: 0.9060 - val_loss: 0.5620 - val_auc: 0.8164\n",
      "Epoch 116/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3850 - auc: 0.9062 - val_loss: 0.5629 - val_auc: 0.8166\n",
      "Epoch 117/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3845 - auc: 0.9065 - val_loss: 0.5666 - val_auc: 0.8160\n",
      "Epoch 118/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3839 - auc: 0.9068 - val_loss: 0.5646 - val_auc: 0.8170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3836 - auc: 0.9070 - val_loss: 0.5666 - val_auc: 0.8152\n",
      "Epoch 120/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3832 - auc: 0.9072 - val_loss: 0.5690 - val_auc: 0.8141\n",
      "Epoch 121/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3825 - auc: 0.9076 - val_loss: 0.5699 - val_auc: 0.8160\n",
      "Epoch 122/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3822 - auc: 0.9077 - val_loss: 0.5694 - val_auc: 0.8157\n",
      "Epoch 123/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3817 - auc: 0.9079 - val_loss: 0.5707 - val_auc: 0.8143\n",
      "Epoch 124/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3812 - auc: 0.9082 - val_loss: 0.5706 - val_auc: 0.8146\n",
      "Epoch 125/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3809 - auc: 0.9084 - val_loss: 0.5711 - val_auc: 0.8150\n",
      "Epoch 126/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3805 - auc: 0.9086 - val_loss: 0.5732 - val_auc: 0.8155\n",
      "Epoch 127/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3801 - auc: 0.9088 - val_loss: 0.5728 - val_auc: 0.8136\n",
      "Epoch 128/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3795 - auc: 0.9091 - val_loss: 0.5737 - val_auc: 0.8130\n",
      "Epoch 129/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3794 - auc: 0.9092 - val_loss: 0.5735 - val_auc: 0.8145\n",
      "Epoch 130/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3787 - auc: 0.9095 - val_loss: 0.5745 - val_auc: 0.8142\n",
      "Epoch 131/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3784 - auc: 0.9097 - val_loss: 0.5762 - val_auc: 0.8120\n",
      "Epoch 132/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.3781 - auc: 0.9098 - val_loss: 0.5745 - val_auc: 0.8131\n",
      "Epoch 133/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3776 - auc: 0.9101 - val_loss: 0.5758 - val_auc: 0.8131\n",
      "Epoch 134/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3774 - auc: 0.9101 - val_loss: 0.5803 - val_auc: 0.8129\n",
      "Epoch 135/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3769 - auc: 0.9104 - val_loss: 0.5780 - val_auc: 0.8143\n",
      "Epoch 136/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3768 - auc: 0.9105 - val_loss: 0.5803 - val_auc: 0.8129\n",
      "Epoch 137/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3762 - auc: 0.9108 - val_loss: 0.5792 - val_auc: 0.8111\n",
      "Epoch 138/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3759 - auc: 0.9109 - val_loss: 0.5803 - val_auc: 0.8109\n",
      "Epoch 139/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3757 - auc: 0.9110 - val_loss: 0.5795 - val_auc: 0.8129\n",
      "Epoch 140/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3753 - auc: 0.9112 - val_loss: 0.5830 - val_auc: 0.8106\n",
      "Epoch 141/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3750 - auc: 0.9114 - val_loss: 0.5841 - val_auc: 0.8125\n",
      "Epoch 142/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3746 - auc: 0.9116 - val_loss: 0.5818 - val_auc: 0.8123\n",
      "Epoch 143/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3742 - auc: 0.9118 - val_loss: 0.5852 - val_auc: 0.8122\n",
      "Epoch 144/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3739 - auc: 0.9119 - val_loss: 0.5845 - val_auc: 0.8109\n",
      "Epoch 145/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3738 - auc: 0.9119 - val_loss: 0.5832 - val_auc: 0.8104\n",
      "Epoch 146/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3736 - auc: 0.9121 - val_loss: 0.5820 - val_auc: 0.8093\n",
      "Epoch 147/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3731 - auc: 0.9123 - val_loss: 0.5898 - val_auc: 0.8104\n",
      "Epoch 148/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3728 - auc: 0.9124 - val_loss: 0.5890 - val_auc: 0.8105\n",
      "Epoch 149/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3724 - auc: 0.9126 - val_loss: 0.5880 - val_auc: 0.8094\n",
      "Epoch 150/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3723 - auc: 0.9127 - val_loss: 0.5860 - val_auc: 0.8106\n",
      "Epoch 151/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3720 - auc: 0.9129 - val_loss: 0.5877 - val_auc: 0.8102\n",
      "Epoch 152/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3717 - auc: 0.9130 - val_loss: 0.5903 - val_auc: 0.8103\n",
      "Epoch 153/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3716 - auc: 0.9131 - val_loss: 0.5885 - val_auc: 0.8099\n",
      "Epoch 154/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3712 - auc: 0.9132 - val_loss: 0.5894 - val_auc: 0.8102\n",
      "Epoch 155/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3709 - auc: 0.9134 - val_loss: 0.5921 - val_auc: 0.8099\n",
      "Epoch 156/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3708 - auc: 0.9135 - val_loss: 0.5901 - val_auc: 0.8082\n",
      "Epoch 157/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3705 - auc: 0.9136 - val_loss: 0.5904 - val_auc: 0.8096\n",
      "Epoch 158/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3703 - auc: 0.9137 - val_loss: 0.5909 - val_auc: 0.8083\n",
      "Epoch 159/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3699 - auc: 0.9139 - val_loss: 0.5926 - val_auc: 0.8082\n",
      "Epoch 160/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3697 - auc: 0.9140 - val_loss: 0.5910 - val_auc: 0.8084\n",
      "Epoch 161/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3696 - auc: 0.9140 - val_loss: 0.5923 - val_auc: 0.8095\n",
      "Epoch 162/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3691 - auc: 0.9143 - val_loss: 0.5946 - val_auc: 0.8094\n",
      "Epoch 163/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3691 - auc: 0.9143 - val_loss: 0.5911 - val_auc: 0.8072\n",
      "Epoch 164/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3689 - auc: 0.9144 - val_loss: 0.5921 - val_auc: 0.8090\n",
      "Epoch 165/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3686 - auc: 0.9145 - val_loss: 0.5916 - val_auc: 0.8094\n",
      "Epoch 166/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3685 - auc: 0.9146 - val_loss: 0.5949 - val_auc: 0.8092\n",
      "Epoch 167/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3681 - auc: 0.9147 - val_loss: 0.5944 - val_auc: 0.8079\n",
      "Epoch 168/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3680 - auc: 0.9148 - val_loss: 0.5938 - val_auc: 0.8066\n",
      "Epoch 169/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3679 - auc: 0.9148 - val_loss: 0.5980 - val_auc: 0.8075\n",
      "Epoch 170/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3675 - auc: 0.9151 - val_loss: 0.5969 - val_auc: 0.8076\n",
      "Epoch 171/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3673 - auc: 0.9152 - val_loss: 0.5970 - val_auc: 0.8065\n",
      "Epoch 172/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3674 - auc: 0.9151 - val_loss: 0.5965 - val_auc: 0.8075\n",
      "Epoch 173/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3670 - auc: 0.9153 - val_loss: 0.5958 - val_auc: 0.8070\n",
      "Epoch 174/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3669 - auc: 0.9153 - val_loss: 0.5991 - val_auc: 0.8068\n",
      "Epoch 175/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3665 - auc: 0.9156 - val_loss: 0.5990 - val_auc: 0.8054\n",
      "Epoch 176/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3664 - auc: 0.9156 - val_loss: 0.5972 - val_auc: 0.8068\n",
      "Epoch 177/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3663 - auc: 0.9156 - val_loss: 0.6020 - val_auc: 0.8068\n",
      "Epoch 178/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3662 - auc: 0.9157 - val_loss: 0.6006 - val_auc: 0.8062\n",
      "Epoch 179/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3660 - auc: 0.9158 - val_loss: 0.5993 - val_auc: 0.8053\n",
      "Epoch 180/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3658 - auc: 0.9159 - val_loss: 0.6038 - val_auc: 0.8080\n",
      "Epoch 181/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3657 - auc: 0.9160 - val_loss: 0.6008 - val_auc: 0.8070\n",
      "Epoch 182/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3653 - auc: 0.9161 - val_loss: 0.6002 - val_auc: 0.8063\n",
      "Epoch 183/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3651 - auc: 0.9162 - val_loss: 0.6040 - val_auc: 0.8072\n",
      "Epoch 184/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3649 - auc: 0.9163 - val_loss: 0.6025 - val_auc: 0.8061\n",
      "Epoch 185/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3650 - auc: 0.9163 - val_loss: 0.6025 - val_auc: 0.8059\n",
      "Epoch 186/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3648 - auc: 0.9164 - val_loss: 0.6026 - val_auc: 0.8064\n",
      "Epoch 187/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3647 - auc: 0.9164 - val_loss: 0.6023 - val_auc: 0.8051\n",
      "Epoch 188/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3644 - auc: 0.9165 - val_loss: 0.6058 - val_auc: 0.8076\n",
      "Epoch 189/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3643 - auc: 0.9166 - val_loss: 0.6058 - val_auc: 0.8049\n",
      "Epoch 190/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3641 - auc: 0.9167 - val_loss: 0.6056 - val_auc: 0.8048\n",
      "Epoch 191/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3640 - auc: 0.9168 - val_loss: 0.6039 - val_auc: 0.8053\n",
      "Epoch 192/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3639 - auc: 0.9168 - val_loss: 0.6064 - val_auc: 0.8046\n",
      "Epoch 193/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3636 - auc: 0.9169 - val_loss: 0.6039 - val_auc: 0.8048\n",
      "Epoch 194/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3634 - auc: 0.9170 - val_loss: 0.6063 - val_auc: 0.8056\n",
      "Epoch 195/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3633 - auc: 0.9171 - val_loss: 0.6057 - val_auc: 0.8055\n",
      "Epoch 196/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3632 - auc: 0.9171 - val_loss: 0.6046 - val_auc: 0.8045\n",
      "Epoch 197/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3631 - auc: 0.9172 - val_loss: 0.6060 - val_auc: 0.8051\n",
      "Epoch 198/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3630 - auc: 0.9172 - val_loss: 0.6060 - val_auc: 0.8055\n",
      "Epoch 199/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3628 - auc: 0.9173 - val_loss: 0.6085 - val_auc: 0.8047\n",
      "Epoch 200/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3626 - auc: 0.9174 - val_loss: 0.6062 - val_auc: 0.8046\n",
      "Epoch 201/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3625 - auc: 0.9175 - val_loss: 0.6074 - val_auc: 0.8063\n",
      "Epoch 202/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3624 - auc: 0.9175 - val_loss: 0.6085 - val_auc: 0.8042\n",
      "Epoch 203/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3622 - auc: 0.9176 - val_loss: 0.6091 - val_auc: 0.8050\n",
      "Epoch 204/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3622 - auc: 0.9176 - val_loss: 0.6103 - val_auc: 0.8036\n",
      "Epoch 205/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3620 - auc: 0.9177 - val_loss: 0.6110 - val_auc: 0.8046\n",
      "Epoch 206/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3619 - auc: 0.9177 - val_loss: 0.6081 - val_auc: 0.8053\n",
      "Epoch 207/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3618 - auc: 0.9178 - val_loss: 0.6110 - val_auc: 0.8042\n",
      "Epoch 208/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3617 - auc: 0.9179 - val_loss: 0.6110 - val_auc: 0.8043\n",
      "Epoch 209/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3614 - auc: 0.9180 - val_loss: 0.6093 - val_auc: 0.8041\n",
      "Epoch 210/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3614 - auc: 0.9180 - val_loss: 0.6112 - val_auc: 0.8042\n",
      "Epoch 211/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3611 - auc: 0.9181 - val_loss: 0.6117 - val_auc: 0.8036\n",
      "Epoch 212/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3612 - auc: 0.9181 - val_loss: 0.6119 - val_auc: 0.8038\n",
      "Epoch 213/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3609 - auc: 0.9182 - val_loss: 0.6123 - val_auc: 0.8037\n",
      "Epoch 214/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3609 - auc: 0.9183 - val_loss: 0.6092 - val_auc: 0.8045\n",
      "Epoch 215/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3605 - auc: 0.9184 - val_loss: 0.6124 - val_auc: 0.8032\n",
      "Epoch 216/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3610 - auc: 0.9182 - val_loss: 0.6130 - val_auc: 0.8050\n",
      "Epoch 217/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3606 - auc: 0.9184 - val_loss: 0.6102 - val_auc: 0.8029\n",
      "Epoch 218/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3604 - auc: 0.9185 - val_loss: 0.6129 - val_auc: 0.8043\n",
      "Epoch 219/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3603 - auc: 0.9185 - val_loss: 0.6136 - val_auc: 0.8022\n",
      "Epoch 220/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3601 - auc: 0.9186 - val_loss: 0.6133 - val_auc: 0.8037\n",
      "Epoch 221/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3601 - auc: 0.9186 - val_loss: 0.6111 - val_auc: 0.8045\n",
      "Epoch 222/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3601 - auc: 0.9186 - val_loss: 0.6099 - val_auc: 0.8035\n",
      "Epoch 223/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3598 - auc: 0.9187 - val_loss: 0.6108 - val_auc: 0.8035\n",
      "Epoch 224/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3595 - auc: 0.9189 - val_loss: 0.6154 - val_auc: 0.8036\n",
      "Epoch 225/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3597 - auc: 0.9188 - val_loss: 0.6161 - val_auc: 0.8022\n",
      "Epoch 226/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3595 - auc: 0.9189 - val_loss: 0.6147 - val_auc: 0.8035\n",
      "Epoch 227/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3596 - auc: 0.9189 - val_loss: 0.6142 - val_auc: 0.8026\n",
      "Epoch 228/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3595 - auc: 0.9189 - val_loss: 0.6142 - val_auc: 0.8031\n",
      "Epoch 229/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3593 - auc: 0.9190 - val_loss: 0.6135 - val_auc: 0.8016\n",
      "Epoch 230/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3592 - auc: 0.9191 - val_loss: 0.6151 - val_auc: 0.8025\n",
      "Epoch 231/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3590 - auc: 0.9192 - val_loss: 0.6170 - val_auc: 0.8037\n",
      "Epoch 232/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3590 - auc: 0.9191 - val_loss: 0.6145 - val_auc: 0.8028\n",
      "Epoch 233/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3589 - auc: 0.9192 - val_loss: 0.6157 - val_auc: 0.8028\n",
      "Epoch 234/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3589 - auc: 0.9192 - val_loss: 0.6173 - val_auc: 0.8031\n",
      "Epoch 235/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.3588 - auc: 0.9193 - val_loss: 0.6181 - val_auc: 0.8037\n",
      "Epoch 236/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3587 - auc: 0.9193 - val_loss: 0.6164 - val_auc: 0.8035\n",
      "Epoch 237/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3586 - auc: 0.9193 - val_loss: 0.6173 - val_auc: 0.8025\n",
      "Epoch 238/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3586 - auc: 0.9194 - val_loss: 0.6162 - val_auc: 0.8033\n",
      "Epoch 239/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3582 - auc: 0.9195 - val_loss: 0.6126 - val_auc: 0.8025\n",
      "Epoch 240/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3581 - auc: 0.9195 - val_loss: 0.6186 - val_auc: 0.8014\n",
      "Epoch 241/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3583 - auc: 0.9195 - val_loss: 0.6148 - val_auc: 0.8019\n",
      "Epoch 242/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3579 - auc: 0.9197 - val_loss: 0.6162 - val_auc: 0.8015\n",
      "Epoch 243/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3577 - auc: 0.9198 - val_loss: 0.6191 - val_auc: 0.8019\n",
      "Epoch 244/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3581 - auc: 0.9196 - val_loss: 0.6169 - val_auc: 0.8022\n",
      "Epoch 245/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3575 - auc: 0.9199 - val_loss: 0.6191 - val_auc: 0.8023\n",
      "Epoch 246/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3577 - auc: 0.9198 - val_loss: 0.6192 - val_auc: 0.8018\n",
      "Epoch 247/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3577 - auc: 0.9198 - val_loss: 0.6186 - val_auc: 0.8021\n",
      "Epoch 248/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3575 - auc: 0.9199 - val_loss: 0.6219 - val_auc: 0.8034\n",
      "Epoch 249/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3575 - auc: 0.9199 - val_loss: 0.6190 - val_auc: 0.8030\n",
      "Epoch 250/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.3574 - auc: 0.9199 - val_loss: 0.6199 - val_auc: 0.8023\n",
      "Wall time: 12h 17min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bcc59bb348>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(x_train, y_train, epochs=250, validation_data=(x_test,y_test), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100000 samples, validate on 500000 samples\n",
      "Epoch 1/250\n",
      "2100000/2100000 [==============================] - 163s 77us/sample - loss: 0.5790 - AUC: 0.7607 - val_loss: 0.5737 - val_AUC: 0.7663\n",
      "Epoch 2/250\n",
      "2100000/2100000 [==============================] - 171s 82us/sample - loss: 0.5637 - AUC: 0.7762 - val_loss: 0.5561 - val_AUC: 0.7838\n",
      "Epoch 3/250\n",
      "2100000/2100000 [==============================] - 211s 100us/sample - loss: 0.5503 - AUC: 0.7893 - val_loss: 0.5496 - val_AUC: 0.7898\n",
      "Epoch 4/250\n",
      "2100000/2100000 [==============================] - 218s 104us/sample - loss: 0.5428 - AUC: 0.7963 - val_loss: 0.5443 - val_AUC: 0.7955\n",
      "Epoch 5/250\n",
      "2100000/2100000 [==============================] - 248s 118us/sample - loss: 0.5374 - AUC: 0.8011 - val_loss: 0.5422 - val_AUC: 0.7986\n",
      "Epoch 6/250\n",
      "2100000/2100000 [==============================] - 233s 111us/sample - loss: 0.5331 - AUC: 0.8049 - val_loss: 0.5412 - val_AUC: 0.8005\n",
      "Epoch 7/250\n",
      "2100000/2100000 [==============================] - 265s 126us/sample - loss: 0.5294 - AUC: 0.8082 - val_loss: 0.5364 - val_AUC: 0.8019\n",
      "Epoch 8/250\n",
      "2100000/2100000 [==============================] - 296s 141us/sample - loss: 0.5262 - AUC: 0.8110 - val_loss: 0.5348 - val_AUC: 0.8038\n",
      "Epoch 9/250\n",
      "2100000/2100000 [==============================] - 237s 113us/sample - loss: 0.5232 - AUC: 0.8135 - val_loss: 0.5344 - val_AUC: 0.8053\n",
      "Epoch 10/250\n",
      "2100000/2100000 [==============================] - 236s 112us/sample - loss: 0.5204 - AUC: 0.8159 - val_loss: 0.5324 - val_AUC: 0.8059\n",
      "Epoch 11/250\n",
      "2100000/2100000 [==============================] - 224s 107us/sample - loss: 0.5179 - AUC: 0.8180 - val_loss: 0.5311 - val_AUC: 0.8075\n",
      "Epoch 12/250\n",
      "2100000/2100000 [==============================] - 291s 138us/sample - loss: 0.5154 - AUC: 0.8201 - val_loss: 0.5292 - val_AUC: 0.8087\n",
      "Epoch 13/250\n",
      "2100000/2100000 [==============================] - 226s 108us/sample - loss: 0.5130 - AUC: 0.8221 - val_loss: 0.5307 - val_AUC: 0.8091\n",
      "Epoch 14/250\n",
      "2100000/2100000 [==============================] - 243s 116us/sample - loss: 0.5109 - AUC: 0.8238 - val_loss: 0.5286 - val_AUC: 0.8105\n",
      "Epoch 15/250\n",
      "2100000/2100000 [==============================] - 214s 102us/sample - loss: 0.5087 - AUC: 0.8255 - val_loss: 0.5288 - val_AUC: 0.8109\n",
      "Epoch 16/250\n",
      "2100000/2100000 [==============================] - 245s 117us/sample - loss: 0.5065 - AUC: 0.8273 - val_loss: 0.5288 - val_AUC: 0.8108\n",
      "Epoch 17/250\n",
      "2100000/2100000 [==============================] - 321s 153us/sample - loss: 0.5046 - AUC: 0.8288 - val_loss: 0.5283 - val_AUC: 0.8109\n",
      "Epoch 18/250\n",
      "2100000/2100000 [==============================] - 257s 122us/sample - loss: 0.5026 - AUC: 0.8304 - val_loss: 0.5276 - val_AUC: 0.8121\n",
      "Epoch 19/250\n",
      "2100000/2100000 [==============================] - 411s 196us/sample - loss: 0.5008 - AUC: 0.8318 - val_loss: 0.5276 - val_AUC: 0.8124\n",
      "Epoch 20/250\n",
      "2100000/2100000 [==============================] - 312s 149us/sample - loss: 0.4989 - AUC: 0.8333 - val_loss: 0.5267 - val_AUC: 0.8132\n",
      "Epoch 21/250\n",
      "2100000/2100000 [==============================] - 385s 183us/sample - loss: 0.4970 - AUC: 0.8348 - val_loss: 0.5271 - val_AUC: 0.8128\n",
      "Epoch 22/250\n",
      "2100000/2100000 [==============================] - 268s 127us/sample - loss: 0.4953 - AUC: 0.8361 - val_loss: 0.5292 - val_AUC: 0.8117\n",
      "Epoch 23/250\n",
      "2100000/2100000 [==============================] - 261s 124us/sample - loss: 0.4935 - AUC: 0.8375 - val_loss: 0.5283 - val_AUC: 0.8131\n",
      "Epoch 24/250\n",
      "2100000/2100000 [==============================] - 257s 122us/sample - loss: 0.4918 - AUC: 0.8388 - val_loss: 0.5310 - val_AUC: 0.8135\n",
      "Epoch 25/250\n",
      "2100000/2100000 [==============================] - 207s 98us/sample - loss: 0.4902 - AUC: 0.8400 - val_loss: 0.5281 - val_AUC: 0.8122\n",
      "Epoch 26/250\n",
      "2100000/2100000 [==============================] - 174s 83us/sample - loss: 0.4885 - AUC: 0.8413 - val_loss: 0.5272 - val_AUC: 0.8124\n",
      "Epoch 27/250\n",
      "2100000/2100000 [==============================] - 151s 72us/sample - loss: 0.4868 - AUC: 0.8425 - val_loss: 0.5289 - val_AUC: 0.8126\n",
      "Epoch 28/250\n",
      "2100000/2100000 [==============================] - 154s 73us/sample - loss: 0.4853 - AUC: 0.8437 - val_loss: 0.5296 - val_AUC: 0.8113\n",
      "Epoch 29/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4837 - AUC: 0.8449 - val_loss: 0.5288 - val_AUC: 0.8124\n",
      "Epoch 30/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4823 - AUC: 0.8459 - val_loss: 0.5287 - val_AUC: 0.8129\n",
      "Epoch 31/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4807 - AUC: 0.8471 - val_loss: 0.5321 - val_AUC: 0.8120\n",
      "Epoch 32/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.4793 - AUC: 0.8481 - val_loss: 0.5292 - val_AUC: 0.8121\n",
      "Epoch 33/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.4779 - AUC: 0.8491 - val_loss: 0.5346 - val_AUC: 0.8113\n",
      "Epoch 34/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4764 - AUC: 0.8502 - val_loss: 0.5336 - val_AUC: 0.8111\n",
      "Epoch 35/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4750 - AUC: 0.8512 - val_loss: 0.5328 - val_AUC: 0.8108\n",
      "Epoch 36/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4736 - AUC: 0.8522 - val_loss: 0.5347 - val_AUC: 0.8101\n",
      "Epoch 37/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4723 - AUC: 0.8532 - val_loss: 0.5354 - val_AUC: 0.8102\n",
      "Epoch 38/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4709 - AUC: 0.8541 - val_loss: 0.5378 - val_AUC: 0.8106\n",
      "Epoch 39/250\n",
      "2100000/2100000 [==============================] - 150s 71us/sample - loss: 0.4696 - AUC: 0.8550 - val_loss: 0.5377 - val_AUC: 0.8096\n",
      "Epoch 40/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4683 - AUC: 0.8560 - val_loss: 0.5406 - val_AUC: 0.8078\n",
      "Epoch 41/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4670 - AUC: 0.8568 - val_loss: 0.5387 - val_AUC: 0.8086\n",
      "Epoch 42/250\n",
      "2100000/2100000 [==============================] - 155s 74us/sample - loss: 0.4657 - AUC: 0.8578 - val_loss: 0.5413 - val_AUC: 0.8079\n",
      "Epoch 43/250\n",
      "2100000/2100000 [==============================] - 150s 72us/sample - loss: 0.4644 - AUC: 0.8586 - val_loss: 0.5424 - val_AUC: 0.8072\n",
      "Epoch 44/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4632 - AUC: 0.8595 - val_loss: 0.5445 - val_AUC: 0.8066\n",
      "Epoch 45/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4620 - AUC: 0.8603 - val_loss: 0.5422 - val_AUC: 0.8067\n",
      "Epoch 46/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4608 - AUC: 0.8611 - val_loss: 0.5491 - val_AUC: 0.8054\n",
      "Epoch 47/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4596 - AUC: 0.8620 - val_loss: 0.5487 - val_AUC: 0.8063\n",
      "Epoch 48/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4584 - AUC: 0.8627 - val_loss: 0.5454 - val_AUC: 0.8065\n",
      "Epoch 49/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4573 - AUC: 0.8635 - val_loss: 0.5496 - val_AUC: 0.8051\n",
      "Epoch 50/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.4561 - AUC: 0.8642 - val_loss: 0.5494 - val_AUC: 0.8028\n",
      "Epoch 51/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4550 - AUC: 0.8650 - val_loss: 0.5488 - val_AUC: 0.8043\n",
      "Epoch 52/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4539 - AUC: 0.8657 - val_loss: 0.5524 - val_AUC: 0.8030\n",
      "Epoch 53/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4529 - AUC: 0.8665 - val_loss: 0.5498 - val_AUC: 0.8024\n",
      "Epoch 54/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4518 - AUC: 0.8672 - val_loss: 0.5534 - val_AUC: 0.8040\n",
      "Epoch 55/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4507 - AUC: 0.8679 - val_loss: 0.5549 - val_AUC: 0.8029\n",
      "Epoch 56/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4496 - AUC: 0.8686 - val_loss: 0.5574 - val_AUC: 0.8017\n",
      "Epoch 57/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4487 - AUC: 0.8692 - val_loss: 0.5559 - val_AUC: 0.8010\n",
      "Epoch 58/250\n",
      "2100000/2100000 [==============================] - 153s 73us/sample - loss: 0.4475 - AUC: 0.8699 - val_loss: 0.5571 - val_AUC: 0.8013\n",
      "Epoch 59/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4466 - AUC: 0.8705 - val_loss: 0.5593 - val_AUC: 0.7991\n",
      "Epoch 60/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4456 - AUC: 0.8712 - val_loss: 0.5589 - val_AUC: 0.8005\n",
      "Epoch 61/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4447 - AUC: 0.8718 - val_loss: 0.5630 - val_AUC: 0.8008\n",
      "Epoch 62/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4437 - AUC: 0.8724 - val_loss: 0.5604 - val_AUC: 0.7982\n",
      "Epoch 63/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4426 - AUC: 0.8731 - val_loss: 0.5634 - val_AUC: 0.7985\n",
      "Epoch 64/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4418 - AUC: 0.8736 - val_loss: 0.5682 - val_AUC: 0.7982\n",
      "Epoch 65/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.4408 - AUC: 0.8742 - val_loss: 0.5657 - val_AUC: 0.7986\n",
      "Epoch 66/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4400 - AUC: 0.8748 - val_loss: 0.5673 - val_AUC: 0.7995\n",
      "Epoch 67/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.4392 - AUC: 0.8753 - val_loss: 0.5683 - val_AUC: 0.7981\n",
      "Epoch 68/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4382 - AUC: 0.8759 - val_loss: 0.5673 - val_AUC: 0.7969\n",
      "Epoch 69/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4374 - AUC: 0.8764 - val_loss: 0.5687 - val_AUC: 0.7961\n",
      "Epoch 70/250\n",
      "2100000/2100000 [==============================] - 153s 73us/sample - loss: 0.4367 - AUC: 0.8769 - val_loss: 0.5768 - val_AUC: 0.7971\n",
      "Epoch 71/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4358 - AUC: 0.8774 - val_loss: 0.5726 - val_AUC: 0.7969\n",
      "Epoch 72/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4350 - AUC: 0.8779 - val_loss: 0.5723 - val_AUC: 0.7952\n",
      "Epoch 73/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4342 - AUC: 0.8784 - val_loss: 0.5728 - val_AUC: 0.7960\n",
      "Epoch 74/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4334 - AUC: 0.8789 - val_loss: 0.5784 - val_AUC: 0.7956\n",
      "Epoch 75/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4326 - AUC: 0.8794 - val_loss: 0.5755 - val_AUC: 0.7952\n",
      "Epoch 76/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4318 - AUC: 0.8799 - val_loss: 0.5785 - val_AUC: 0.7949\n",
      "Epoch 77/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4312 - AUC: 0.8802 - val_loss: 0.5782 - val_AUC: 0.7922\n",
      "Epoch 78/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4304 - AUC: 0.8807 - val_loss: 0.5790 - val_AUC: 0.7964\n",
      "Epoch 79/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4296 - AUC: 0.8812 - val_loss: 0.5794 - val_AUC: 0.7929\n",
      "Epoch 80/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4291 - AUC: 0.8815 - val_loss: 0.5805 - val_AUC: 0.7933\n",
      "Epoch 81/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.4282 - AUC: 0.8821 - val_loss: 0.5825 - val_AUC: 0.7942\n",
      "Epoch 82/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4276 - AUC: 0.8824 - val_loss: 0.5830 - val_AUC: 0.7918\n",
      "Epoch 83/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4270 - AUC: 0.8828 - val_loss: 0.5829 - val_AUC: 0.7918\n",
      "Epoch 84/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4263 - AUC: 0.8833 - val_loss: 0.5856 - val_AUC: 0.7916\n",
      "Epoch 85/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4257 - AUC: 0.8835 - val_loss: 0.5858 - val_AUC: 0.7914\n",
      "Epoch 86/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4249 - AUC: 0.8840 - val_loss: 0.5850 - val_AUC: 0.7929\n",
      "Epoch 87/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4243 - AUC: 0.8844 - val_loss: 0.5910 - val_AUC: 0.7902\n",
      "Epoch 88/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4237 - AUC: 0.8848 - val_loss: 0.5881 - val_AUC: 0.7905\n",
      "Epoch 89/250\n",
      "2100000/2100000 [==============================] - 151s 72us/sample - loss: 0.4232 - AUC: 0.8850 - val_loss: 0.5891 - val_AUC: 0.7910\n",
      "Epoch 90/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4226 - AUC: 0.8854 - val_loss: 0.5885 - val_AUC: 0.7908\n",
      "Epoch 91/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4221 - AUC: 0.8857 - val_loss: 0.5915 - val_AUC: 0.7927\n",
      "Epoch 92/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.4214 - AUC: 0.8862 - val_loss: 0.5928 - val_AUC: 0.7912\n",
      "Epoch 93/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4208 - AUC: 0.8865 - val_loss: 0.5915 - val_AUC: 0.7887\n",
      "Epoch 94/250\n",
      "2100000/2100000 [==============================] - 151s 72us/sample - loss: 0.4203 - AUC: 0.8868 - val_loss: 0.5922 - val_AUC: 0.7893\n",
      "Epoch 95/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.4197 - AUC: 0.8871 - val_loss: 0.5933 - val_AUC: 0.7891\n",
      "Epoch 96/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4192 - AUC: 0.8874 - val_loss: 0.5931 - val_AUC: 0.7879\n",
      "Epoch 97/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4186 - AUC: 0.8878 - val_loss: 0.5969 - val_AUC: 0.7891\n",
      "Epoch 98/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.4182 - AUC: 0.8880 - val_loss: 0.5971 - val_AUC: 0.7882\n",
      "Epoch 99/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4177 - AUC: 0.8883 - val_loss: 0.5976 - val_AUC: 0.7881\n",
      "Epoch 100/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4170 - AUC: 0.8887 - val_loss: 0.5983 - val_AUC: 0.7890\n",
      "Epoch 101/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.4168 - AUC: 0.8888 - val_loss: 0.5974 - val_AUC: 0.7882\n",
      "Epoch 102/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4161 - AUC: 0.8892 - val_loss: 0.5978 - val_AUC: 0.7869\n",
      "Epoch 103/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4158 - AUC: 0.8894 - val_loss: 0.6005 - val_AUC: 0.7876\n",
      "Epoch 104/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4153 - AUC: 0.8897 - val_loss: 0.6027 - val_AUC: 0.7889\n",
      "Epoch 105/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.4148 - AUC: 0.8900 - val_loss: 0.6032 - val_AUC: 0.7879\n",
      "Epoch 106/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4143 - AUC: 0.8902 - val_loss: 0.6037 - val_AUC: 0.7864\n",
      "Epoch 107/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4138 - AUC: 0.8905 - val_loss: 0.6035 - val_AUC: 0.7873\n",
      "Epoch 108/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4135 - AUC: 0.8907 - val_loss: 0.6078 - val_AUC: 0.7881\n",
      "Epoch 109/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4132 - AUC: 0.8909 - val_loss: 0.6050 - val_AUC: 0.7875\n",
      "Epoch 110/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4126 - AUC: 0.8913 - val_loss: 0.6075 - val_AUC: 0.7874\n",
      "Epoch 111/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4121 - AUC: 0.8916 - val_loss: 0.6093 - val_AUC: 0.7871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4117 - AUC: 0.8918 - val_loss: 0.6082 - val_AUC: 0.7864\n",
      "Epoch 113/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.4113 - AUC: 0.8920 - val_loss: 0.6092 - val_AUC: 0.7855\n",
      "Epoch 114/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4111 - AUC: 0.8921 - val_loss: 0.6078 - val_AUC: 0.7852\n",
      "Epoch 115/250\n",
      "2100000/2100000 [==============================] - 150s 71us/sample - loss: 0.4105 - AUC: 0.8924 - val_loss: 0.6113 - val_AUC: 0.7850\n",
      "Epoch 116/250\n",
      "2100000/2100000 [==============================] - 152s 72us/sample - loss: 0.4102 - AUC: 0.8926 - val_loss: 0.6080 - val_AUC: 0.7868\n",
      "Epoch 117/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4099 - AUC: 0.8928 - val_loss: 0.6111 - val_AUC: 0.7865\n",
      "Epoch 118/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.4094 - AUC: 0.8931 - val_loss: 0.6073 - val_AUC: 0.7840\n",
      "Epoch 119/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4091 - AUC: 0.8932 - val_loss: 0.6111 - val_AUC: 0.7852\n",
      "Epoch 120/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4088 - AUC: 0.8934 - val_loss: 0.6132 - val_AUC: 0.7846\n",
      "Epoch 121/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4083 - AUC: 0.8937 - val_loss: 0.6144 - val_AUC: 0.7854\n",
      "Epoch 122/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4078 - AUC: 0.8940 - val_loss: 0.6117 - val_AUC: 0.7835\n",
      "Epoch 123/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4077 - AUC: 0.8940 - val_loss: 0.6164 - val_AUC: 0.7845\n",
      "Epoch 124/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4074 - AUC: 0.8942 - val_loss: 0.6154 - val_AUC: 0.7836\n",
      "Epoch 125/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4069 - AUC: 0.8945 - val_loss: 0.6158 - val_AUC: 0.7825\n",
      "Epoch 126/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4066 - AUC: 0.8946 - val_loss: 0.6164 - val_AUC: 0.7839\n",
      "Epoch 127/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4065 - AUC: 0.8947 - val_loss: 0.6159 - val_AUC: 0.7841\n",
      "Epoch 128/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.4060 - AUC: 0.8949 - val_loss: 0.6178 - val_AUC: 0.7843\n",
      "Epoch 129/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4058 - AUC: 0.8950 - val_loss: 0.6155 - val_AUC: 0.7840\n",
      "Epoch 130/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.4054 - AUC: 0.8953 - val_loss: 0.6172 - val_AUC: 0.7850\n",
      "Epoch 131/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4051 - AUC: 0.8954 - val_loss: 0.6193 - val_AUC: 0.7851\n",
      "Epoch 132/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4047 - AUC: 0.8957 - val_loss: 0.6200 - val_AUC: 0.7817\n",
      "Epoch 133/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4045 - AUC: 0.8958 - val_loss: 0.6206 - val_AUC: 0.7830\n",
      "Epoch 134/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4042 - AUC: 0.8960 - val_loss: 0.6200 - val_AUC: 0.7828\n",
      "Epoch 135/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4041 - AUC: 0.8961 - val_loss: 0.6213 - val_AUC: 0.7822\n",
      "Epoch 136/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4037 - AUC: 0.8962 - val_loss: 0.6200 - val_AUC: 0.7821\n",
      "Epoch 137/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4035 - AUC: 0.8964 - val_loss: 0.6217 - val_AUC: 0.7833\n",
      "Epoch 138/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.4034 - AUC: 0.8964 - val_loss: 0.6219 - val_AUC: 0.7816\n",
      "Epoch 139/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.4028 - AUC: 0.8967 - val_loss: 0.6253 - val_AUC: 0.7832\n",
      "Epoch 140/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.4025 - AUC: 0.8969 - val_loss: 0.6243 - val_AUC: 0.7823\n",
      "Epoch 141/250\n",
      "2100000/2100000 [==============================] - 157s 75us/sample - loss: 0.4023 - AUC: 0.8970 - val_loss: 0.6245 - val_AUC: 0.7817\n",
      "Epoch 142/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.4022 - AUC: 0.8971 - val_loss: 0.6226 - val_AUC: 0.7827\n",
      "Epoch 143/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.4019 - AUC: 0.8972 - val_loss: 0.6254 - val_AUC: 0.7819\n",
      "Epoch 144/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.4018 - AUC: 0.8973 - val_loss: 0.6261 - val_AUC: 0.7820\n",
      "Epoch 145/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4014 - AUC: 0.8975 - val_loss: 0.6250 - val_AUC: 0.7807\n",
      "Epoch 146/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4012 - AUC: 0.8976 - val_loss: 0.6233 - val_AUC: 0.7789\n",
      "Epoch 147/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.4011 - AUC: 0.8977 - val_loss: 0.6251 - val_AUC: 0.7788\n",
      "Epoch 148/250\n",
      "2100000/2100000 [==============================] - 151s 72us/sample - loss: 0.4007 - AUC: 0.8979 - val_loss: 0.6278 - val_AUC: 0.7802\n",
      "Epoch 149/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.4006 - AUC: 0.8979 - val_loss: 0.6270 - val_AUC: 0.7806\n",
      "Epoch 150/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.4003 - AUC: 0.8981 - val_loss: 0.6269 - val_AUC: 0.7792\n",
      "Epoch 151/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3999 - AUC: 0.8983 - val_loss: 0.6274 - val_AUC: 0.7794\n",
      "Epoch 152/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3999 - AUC: 0.8983 - val_loss: 0.6283 - val_AUC: 0.7812\n",
      "Epoch 153/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3998 - AUC: 0.8984 - val_loss: 0.6283 - val_AUC: 0.7805\n",
      "Epoch 154/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3994 - AUC: 0.8986 - val_loss: 0.6319 - val_AUC: 0.7790\n",
      "Epoch 155/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3993 - AUC: 0.8987 - val_loss: 0.6311 - val_AUC: 0.7804\n",
      "Epoch 156/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3990 - AUC: 0.8988 - val_loss: 0.6302 - val_AUC: 0.7797\n",
      "Epoch 157/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3988 - AUC: 0.8989 - val_loss: 0.6315 - val_AUC: 0.7785\n",
      "Epoch 158/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3988 - AUC: 0.8989 - val_loss: 0.6280 - val_AUC: 0.7798\n",
      "Epoch 159/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3984 - AUC: 0.8992 - val_loss: 0.6355 - val_AUC: 0.7804\n",
      "Epoch 160/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3982 - AUC: 0.8993 - val_loss: 0.6322 - val_AUC: 0.7818\n",
      "Epoch 161/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3982 - AUC: 0.8993 - val_loss: 0.6321 - val_AUC: 0.7785\n",
      "Epoch 162/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3978 - AUC: 0.8995 - val_loss: 0.6318 - val_AUC: 0.7783\n",
      "Epoch 163/250\n",
      "2100000/2100000 [==============================] - 148s 71us/sample - loss: 0.3976 - AUC: 0.8996 - val_loss: 0.6333 - val_AUC: 0.7783\n",
      "Epoch 164/250\n",
      "2100000/2100000 [==============================] - 150s 72us/sample - loss: 0.3975 - AUC: 0.8997 - val_loss: 0.6337 - val_AUC: 0.7788\n",
      "Epoch 165/250\n",
      "2100000/2100000 [==============================] - 162s 77us/sample - loss: 0.3974 - AUC: 0.8997 - val_loss: 0.6334 - val_AUC: 0.7802\n",
      "Epoch 166/250\n",
      "2100000/2100000 [==============================] - 165s 79us/sample - loss: 0.3972 - AUC: 0.8998 - val_loss: 0.6331 - val_AUC: 0.7784\n",
      "Epoch 167/250\n",
      "2100000/2100000 [==============================] - 158s 75us/sample - loss: 0.3968 - AUC: 0.9000 - val_loss: 0.6363 - val_AUC: 0.7793\n",
      "Epoch 168/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3968 - AUC: 0.9000 - val_loss: 0.6358 - val_AUC: 0.7790\n",
      "Epoch 169/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.3964 - AUC: 0.9002 - val_loss: 0.6326 - val_AUC: 0.7796\n",
      "Epoch 170/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3965 - AUC: 0.9002 - val_loss: 0.6360 - val_AUC: 0.7794\n",
      "Epoch 171/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3963 - AUC: 0.9003 - val_loss: 0.6386 - val_AUC: 0.7792\n",
      "Epoch 172/250\n",
      "2100000/2100000 [==============================] - 154s 73us/sample - loss: 0.3960 - AUC: 0.9004 - val_loss: 0.6342 - val_AUC: 0.7787\n",
      "Epoch 173/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3959 - AUC: 0.9005 - val_loss: 0.6359 - val_AUC: 0.7796\n",
      "Epoch 174/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3956 - AUC: 0.9007 - val_loss: 0.6352 - val_AUC: 0.7798\n",
      "Epoch 175/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3957 - AUC: 0.9006 - val_loss: 0.6329 - val_AUC: 0.7778\n",
      "Epoch 176/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3955 - AUC: 0.9007 - val_loss: 0.6345 - val_AUC: 0.7786\n",
      "Epoch 177/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3952 - AUC: 0.9009 - val_loss: 0.6387 - val_AUC: 0.7788\n",
      "Epoch 178/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3951 - AUC: 0.9010 - val_loss: 0.6375 - val_AUC: 0.7798\n",
      "Epoch 179/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3949 - AUC: 0.9010 - val_loss: 0.6444 - val_AUC: 0.7779\n",
      "Epoch 180/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3947 - AUC: 0.9011 - val_loss: 0.6394 - val_AUC: 0.7769\n",
      "Epoch 181/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3946 - AUC: 0.9012 - val_loss: 0.6387 - val_AUC: 0.7789\n",
      "Epoch 182/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3945 - AUC: 0.9012 - val_loss: 0.6391 - val_AUC: 0.7777\n",
      "Epoch 183/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3945 - AUC: 0.9012 - val_loss: 0.6376 - val_AUC: 0.7790\n",
      "Epoch 184/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.3942 - AUC: 0.9014 - val_loss: 0.6381 - val_AUC: 0.7784\n",
      "Epoch 185/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3940 - AUC: 0.9015 - val_loss: 0.6415 - val_AUC: 0.7783\n",
      "Epoch 186/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3939 - AUC: 0.9016 - val_loss: 0.6413 - val_AUC: 0.7778\n",
      "Epoch 187/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.3937 - AUC: 0.9016 - val_loss: 0.6432 - val_AUC: 0.7787\n",
      "Epoch 188/250\n",
      "2100000/2100000 [==============================] - 150s 71us/sample - loss: 0.3937 - AUC: 0.9017 - val_loss: 0.6372 - val_AUC: 0.7771\n",
      "Epoch 189/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3933 - AUC: 0.9019 - val_loss: 0.6472 - val_AUC: 0.7765\n",
      "Epoch 190/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3932 - AUC: 0.9019 - val_loss: 0.6418 - val_AUC: 0.7768\n",
      "Epoch 191/250\n",
      "2100000/2100000 [==============================] - 161s 76us/sample - loss: 0.3931 - AUC: 0.9020 - val_loss: 0.6434 - val_AUC: 0.7775\n",
      "Epoch 192/250\n",
      "2100000/2100000 [==============================] - 154s 73us/sample - loss: 0.3930 - AUC: 0.9021 - val_loss: 0.6423 - val_AUC: 0.7772\n",
      "Epoch 193/250\n",
      "2100000/2100000 [==============================] - 150s 72us/sample - loss: 0.3930 - AUC: 0.9020 - val_loss: 0.6410 - val_AUC: 0.7773\n",
      "Epoch 194/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3928 - AUC: 0.9021 - val_loss: 0.6428 - val_AUC: 0.7773\n",
      "Epoch 195/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3926 - AUC: 0.9022 - val_loss: 0.6429 - val_AUC: 0.7773\n",
      "Epoch 196/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3926 - AUC: 0.9023 - val_loss: 0.6394 - val_AUC: 0.7775\n",
      "Epoch 197/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3925 - AUC: 0.9023 - val_loss: 0.6386 - val_AUC: 0.7766\n",
      "Epoch 198/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3922 - AUC: 0.9024 - val_loss: 0.6412 - val_AUC: 0.7772\n",
      "Epoch 199/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3922 - AUC: 0.9024 - val_loss: 0.6438 - val_AUC: 0.7766\n",
      "Epoch 200/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3918 - AUC: 0.9027 - val_loss: 0.6430 - val_AUC: 0.7773\n",
      "Epoch 201/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3920 - AUC: 0.9026 - val_loss: 0.6457 - val_AUC: 0.7764\n",
      "Epoch 202/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.3919 - AUC: 0.9026 - val_loss: 0.6479 - val_AUC: 0.7780\n",
      "Epoch 203/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3917 - AUC: 0.9027 - val_loss: 0.6433 - val_AUC: 0.7762\n",
      "Epoch 204/250\n",
      "2100000/2100000 [==============================] - 142s 67us/sample - loss: 0.3915 - AUC: 0.9028 - val_loss: 0.6452 - val_AUC: 0.7768\n",
      "Epoch 205/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3912 - AUC: 0.9030 - val_loss: 0.6435 - val_AUC: 0.7774\n",
      "Epoch 206/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3913 - AUC: 0.9030 - val_loss: 0.6466 - val_AUC: 0.7772\n",
      "Epoch 207/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3913 - AUC: 0.9029 - val_loss: 0.6446 - val_AUC: 0.7752\n",
      "Epoch 208/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3912 - AUC: 0.9030 - val_loss: 0.6449 - val_AUC: 0.7764\n",
      "Epoch 209/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3909 - AUC: 0.9032 - val_loss: 0.6433 - val_AUC: 0.7755\n",
      "Epoch 210/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3907 - AUC: 0.9033 - val_loss: 0.6477 - val_AUC: 0.7759\n",
      "Epoch 211/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3910 - AUC: 0.9031 - val_loss: 0.6458 - val_AUC: 0.7764\n",
      "Epoch 212/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3907 - AUC: 0.9033 - val_loss: 0.6453 - val_AUC: 0.7747\n",
      "Epoch 213/250\n",
      "2100000/2100000 [==============================] - 141s 67us/sample - loss: 0.3905 - AUC: 0.9034 - val_loss: 0.6459 - val_AUC: 0.7773\n",
      "Epoch 214/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3904 - AUC: 0.9034 - val_loss: 0.6446 - val_AUC: 0.7753\n",
      "Epoch 215/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3904 - AUC: 0.9034 - val_loss: 0.6473 - val_AUC: 0.7769\n",
      "Epoch 216/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3900 - AUC: 0.9036 - val_loss: 0.6463 - val_AUC: 0.7745\n",
      "Epoch 217/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3901 - AUC: 0.9036 - val_loss: 0.6511 - val_AUC: 0.7748\n",
      "Epoch 218/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3899 - AUC: 0.9037 - val_loss: 0.6489 - val_AUC: 0.7759\n",
      "Epoch 219/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3900 - AUC: 0.9036 - val_loss: 0.6456 - val_AUC: 0.7757\n",
      "Epoch 220/250\n",
      "2100000/2100000 [==============================] - 152s 72us/sample - loss: 0.3898 - AUC: 0.9037 - val_loss: 0.6488 - val_AUC: 0.7772\n",
      "Epoch 221/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3896 - AUC: 0.9038 - val_loss: 0.6462 - val_AUC: 0.7752\n",
      "Epoch 222/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3895 - AUC: 0.9039 - val_loss: 0.6521 - val_AUC: 0.7761\n",
      "Epoch 223/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3895 - AUC: 0.9039 - val_loss: 0.6495 - val_AUC: 0.7770\n",
      "Epoch 224/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3895 - AUC: 0.9039 - val_loss: 0.6449 - val_AUC: 0.7756\n",
      "Epoch 225/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3894 - AUC: 0.9040 - val_loss: 0.6482 - val_AUC: 0.7774\n",
      "Epoch 226/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3893 - AUC: 0.9040 - val_loss: 0.6478 - val_AUC: 0.7756\n",
      "Epoch 227/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3891 - AUC: 0.9041 - val_loss: 0.6483 - val_AUC: 0.7749\n",
      "Epoch 228/250\n",
      "2100000/2100000 [==============================] - 144s 68us/sample - loss: 0.3890 - AUC: 0.9041 - val_loss: 0.6487 - val_AUC: 0.7763\n",
      "Epoch 229/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3890 - AUC: 0.9042 - val_loss: 0.6512 - val_AUC: 0.7738\n",
      "Epoch 230/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3889 - AUC: 0.9042 - val_loss: 0.6521 - val_AUC: 0.7772\n",
      "Epoch 231/250\n",
      "2100000/2100000 [==============================] - 149s 71us/sample - loss: 0.3887 - AUC: 0.9043 - val_loss: 0.6505 - val_AUC: 0.7747\n",
      "Epoch 232/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3889 - AUC: 0.9042 - val_loss: 0.6500 - val_AUC: 0.7765\n",
      "Epoch 233/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3887 - AUC: 0.9043 - val_loss: 0.6506 - val_AUC: 0.7747\n",
      "Epoch 234/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3886 - AUC: 0.9043 - val_loss: 0.6481 - val_AUC: 0.7753\n",
      "Epoch 235/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.3886 - AUC: 0.9043 - val_loss: 0.6514 - val_AUC: 0.7755\n",
      "Epoch 236/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3885 - AUC: 0.9044 - val_loss: 0.6496 - val_AUC: 0.7732\n",
      "Epoch 237/250\n",
      "2100000/2100000 [==============================] - 150s 71us/sample - loss: 0.3883 - AUC: 0.9045 - val_loss: 0.6537 - val_AUC: 0.7758\n",
      "Epoch 238/250\n",
      "2100000/2100000 [==============================] - 142s 68us/sample - loss: 0.3883 - AUC: 0.9045 - val_loss: 0.6520 - val_AUC: 0.7765\n",
      "Epoch 239/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3881 - AUC: 0.9047 - val_loss: 0.6542 - val_AUC: 0.7762\n",
      "Epoch 240/250\n",
      "2100000/2100000 [==============================] - 155s 74us/sample - loss: 0.3881 - AUC: 0.9046 - val_loss: 0.6540 - val_AUC: 0.7745\n",
      "Epoch 241/250\n",
      "2100000/2100000 [==============================] - 148s 70us/sample - loss: 0.3882 - AUC: 0.9046 - val_loss: 0.6497 - val_AUC: 0.7762\n",
      "Epoch 242/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3878 - AUC: 0.9048 - val_loss: 0.6507 - val_AUC: 0.7747\n",
      "Epoch 243/250\n",
      "2100000/2100000 [==============================] - 146s 70us/sample - loss: 0.3881 - AUC: 0.9046 - val_loss: 0.6493 - val_AUC: 0.7748\n",
      "Epoch 244/250\n",
      "2100000/2100000 [==============================] - 144s 69us/sample - loss: 0.3878 - AUC: 0.9048 - val_loss: 0.6505 - val_AUC: 0.7745\n",
      "Epoch 245/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3878 - AUC: 0.9048 - val_loss: 0.6518 - val_AUC: 0.7748\n",
      "Epoch 246/250\n",
      "2100000/2100000 [==============================] - 147s 70us/sample - loss: 0.3876 - AUC: 0.9049 - val_loss: 0.6522 - val_AUC: 0.7767\n",
      "Epoch 247/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3877 - AUC: 0.9048 - val_loss: 0.6525 - val_AUC: 0.7745\n",
      "Epoch 248/250\n",
      "2100000/2100000 [==============================] - 146s 69us/sample - loss: 0.3875 - AUC: 0.9049 - val_loss: 0.6540 - val_AUC: 0.7734\n",
      "Epoch 249/250\n",
      "2100000/2100000 [==============================] - 143s 68us/sample - loss: 0.3875 - AUC: 0.9049 - val_loss: 0.6528 - val_AUC: 0.7757\n",
      "Epoch 250/250\n",
      "2100000/2100000 [==============================] - 145s 69us/sample - loss: 0.3874 - AUC: 0.9050 - val_loss: 0.6498 - val_AUC: 0.7750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a75da2fc08>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change learning rate\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch == 1:\n",
    "        return float(0.05)\n",
    "    else:\n",
    "        if lr * (1-1.0000002) < 0.000001:\n",
    "            return float(lr)\n",
    "        else:\n",
    "            return float(lr * (1-1.0000002))\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(momentum=1e-5)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "#set early stopping criteria\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.00001 , patience=10)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=250, validation_data=(x_test,y_test), batch_size=100, callbacks=[lr_callback,es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100000 samples, validate on 500000 samples\n",
      "Epoch 1/10\n",
      "2100000/2100000 [==============================] - 190s 90us/sample - loss: 0.5564 - AUC: 0.7839 - val_loss: 0.5221 - val_AUC: 0.8149\n",
      "Epoch 2/10\n",
      "2100000/2100000 [==============================] - 202s 96us/sample - loss: 0.5313 - AUC: 0.8078 - val_loss: 0.5186 - val_AUC: 0.8196\n",
      "Epoch 3/10\n",
      "2100000/2100000 [==============================] - 223s 106us/sample - loss: 0.5146 - AUC: 0.8217 - val_loss: 0.5101 - val_AUC: 0.8292\n",
      "Epoch 4/10\n",
      "2100000/2100000 [==============================] - 198s 94us/sample - loss: 0.5074 - AUC: 0.8275 - val_loss: 0.5057 - val_AUC: 0.8279\n",
      "Epoch 5/10\n",
      "2100000/2100000 [==============================] - 194s 92us/sample - loss: 0.5024 - AUC: 0.8314 - val_loss: 0.5034 - val_AUC: 0.8325\n",
      "Epoch 6/10\n",
      "2100000/2100000 [==============================] - 201s 96us/sample - loss: 0.4987 - AUC: 0.8342 - val_loss: 0.5132 - val_AUC: 0.8276\n",
      "Epoch 7/10\n",
      "2100000/2100000 [==============================] - 190s 90us/sample - loss: 0.4962 - AUC: 0.8361 - val_loss: 0.5001 - val_AUC: 0.8346\n",
      "Epoch 8/10\n",
      "2100000/2100000 [==============================] - 204s 97us/sample - loss: 0.4936 - AUC: 0.8380 - val_loss: 0.4997 - val_AUC: 0.8335\n",
      "Epoch 9/10\n",
      "2100000/2100000 [==============================] - 192s 91us/sample - loss: 0.4915 - AUC: 0.8396 - val_loss: 0.5027 - val_AUC: 0.8372\n",
      "Epoch 10/10\n",
      "2100000/2100000 [==============================] - 216s 103us/sample - loss: 0.4900 - AUC: 0.8407 - val_loss: 0.5040 - val_AUC: 0.8374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d91295e148>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change learning rate and add stopping criteria\n",
    "#momentum not working here\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch == 1:\n",
    "        return float(0.05)\n",
    "    else:\n",
    "        if lr * (1-1.0000002) < 0.000001:\n",
    "            return float(lr)\n",
    "        else:\n",
    "            return float(lr * (1-1.0000002))\n",
    "        \n",
    "def mmnt(epoch,moment):\n",
    "    if epoch == 1:\n",
    "        return float(0.9)\n",
    "    else:\n",
    "        if epoch <= 200:\n",
    "            return float(moment + 0.00045)\n",
    "        else:\n",
    "            return float(0.99)\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(momentum=.9)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "#set early stopping criteria\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.00001 , patience=10)\n",
    "\n",
    "#opt = tf.compat.v1.train.MomentumOptimizer(learning_rate=.05, momentum=mmnt, use_locking=False, name='Momentum', use_nesterov=False)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test,y_test), batch_size=100, callbacks=[lr_callback,es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = model.history.history['AUC']\n",
    "val_loss = model.history.history['val_AUC']\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Test')\n",
    "plt.legend(title='')\n",
    "plt.title('AUC Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Test')\n",
    "plt.legend(title='')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.5563940400424457,\n",
       "  0.531262514572768,\n",
       "  0.5146266064331645,\n",
       "  0.5073530776727767,\n",
       "  0.5024125940671988,\n",
       "  0.49870970666834286,\n",
       "  0.4961710332220509,\n",
       "  0.49362627341208004,\n",
       "  0.49149636771707306,\n",
       "  0.4899609051204863],\n",
       " 'AUC': [0.78385186,\n",
       "  0.8078157,\n",
       "  0.82174736,\n",
       "  0.82746744,\n",
       "  0.83136535,\n",
       "  0.8342093,\n",
       "  0.83609897,\n",
       "  0.83803195,\n",
       "  0.83964175,\n",
       "  0.8407293],\n",
       " 'val_loss': [0.5221428073346615,\n",
       "  0.5186260644078254,\n",
       "  0.5101012495994568,\n",
       "  0.5056665547072887,\n",
       "  0.5033992579519748,\n",
       "  0.5131608513176441,\n",
       "  0.5001451087474823,\n",
       "  0.4996768009662628,\n",
       "  0.5026775236308575,\n",
       "  0.5040172001361847],\n",
       " 'val_AUC': [0.8148689,\n",
       "  0.81958205,\n",
       "  0.82922405,\n",
       "  0.8279091,\n",
       "  0.832528,\n",
       "  0.8276216,\n",
       "  0.83462936,\n",
       "  0.83346367,\n",
       "  0.8371858,\n",
       "  0.8373918],\n",
       " 'lr': [0.01, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
