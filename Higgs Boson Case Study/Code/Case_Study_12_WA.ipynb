{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = tf.ConfigProto(\n",
    "#        device_count = {'GPU': 0}\n",
    "#    )\n",
    "#sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# import shutil\n",
    "# # C:\\Users\\allro\\JupyterNotebook\\QTW\\Data\n",
    "# with gzip.open('C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv.gz', 'rb') as f_in:\n",
    "#    with open('C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv', 'wb') as f_out:\n",
    "#        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/ml/datasets/HIGGS#\n",
    "df = pd.read_csv(\"../../../HIGGS/HIGGS.csv\", header=None)\n",
    "# df = pd.read_csv(\"../../HIGGS.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns =['target', 'lepton_ph', 'lepton_eta', 'lepton_phi','missing_energy_magnitude','missing_energy_phi',\n",
    "             'jet_1_pt','jet_1_eta','jet_1_phi','jet_1_btag','jet_2_pt','jet_2_eta','jet_2_phi','jet_2_btag',\n",
    "             'jet_3_pt','jet_3_eta','jet_3_phi','jet_3_btag','jet_4_pt','jet_4_eta','jet_4_phi','jet_4_btag',\n",
    "             'm_jj','m_jjj','m_lv','m_jlv','m_bb','m_wbb','m_wwbb'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000000 entries, 0 to 10999999\n",
      "Data columns (total 29 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   target                    float64\n",
      " 1   lepton_ph                 float64\n",
      " 2   lepton_eta                float64\n",
      " 3   lepton_phi                float64\n",
      " 4   missing_energy_magnitude  float64\n",
      " 5   missing_energy_phi        float64\n",
      " 6   jet_1_pt                  float64\n",
      " 7   jet_1_eta                 float64\n",
      " 8   jet_1_phi                 float64\n",
      " 9   jet_1_btag                float64\n",
      " 10  jet_2_pt                  float64\n",
      " 11  jet_2_eta                 float64\n",
      " 12  jet_2_phi                 float64\n",
      " 13  jet_2_btag                float64\n",
      " 14  jet_3_pt                  float64\n",
      " 15  jet_3_eta                 float64\n",
      " 16  jet_3_phi                 float64\n",
      " 17  jet_3_btag                float64\n",
      " 18  jet_4_pt                  float64\n",
      " 19  jet_4_eta                 float64\n",
      " 20  jet_4_phi                 float64\n",
      " 21  jet_4_btag                float64\n",
      " 22  m_jj                      float64\n",
      " 23  m_jjj                     float64\n",
      " 24  m_lv                      float64\n",
      " 25  m_jlv                     float64\n",
      " 26  m_bb                      float64\n",
      " 27  m_wbb                     float64\n",
      " 28  m_wwbb                    float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "# Print out the data types\n",
    "df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  lepton_ph  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0     1.0   0.869293   -0.635082    0.225690                  0.327470   \n",
       "1     1.0   0.907542    0.329147    0.359412                  1.497970   \n",
       "2     1.0   0.798835    1.470639   -1.635975                  0.453773   \n",
       "3     0.0   1.344385   -0.876626    0.935913                  1.992050   \n",
       "4     1.0   1.105009    0.321356    1.522401                  0.882808   \n",
       "\n",
       "   missing_energy_phi  jet_1_pt  jet_1_eta  jet_1_phi  jet_1_btag  ...  \\\n",
       "0           -0.689993  0.754202  -0.248573  -1.092064    0.000000  ...   \n",
       "1           -0.313010  1.095531  -0.557525  -1.588230    2.173076  ...   \n",
       "2            0.425629  1.104875   1.282322   1.381664    0.000000  ...   \n",
       "3            0.882454  1.786066  -1.646778  -0.942383    0.000000  ...   \n",
       "4           -1.205349  0.681466  -1.070464  -0.921871    0.000000  ...   \n",
       "\n",
       "   jet_4_eta  jet_4_phi  jet_4_btag      m_jj     m_jjj      m_lv     m_jlv  \\\n",
       "0  -0.010455  -0.045767    3.101961  1.353760  0.979563  0.978076  0.920005   \n",
       "1  -1.138930  -0.000819    0.000000  0.302220  0.833048  0.985700  0.978098   \n",
       "2   1.128848   0.900461    0.000000  0.909753  1.108330  0.985692  0.951331   \n",
       "3  -0.678379  -1.360356    0.000000  0.946652  1.028704  0.998656  0.728281   \n",
       "4  -0.373566   0.113041    0.000000  0.755856  1.361057  0.986610  0.838085   \n",
       "\n",
       "       m_bb     m_wbb    m_wwbb  \n",
       "0  0.721657  0.988751  0.876678  \n",
       "1  0.779732  0.992356  0.798343  \n",
       "2  0.803252  0.865924  0.780118  \n",
       "3  0.869200  1.026736  0.957904  \n",
       "4  1.133295  0.872245  0.808487  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample the data\n",
    "train = df.sample(n=2600000, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.299203e-01</td>\n",
       "      <td>9.914658e-01</td>\n",
       "      <td>-8.297618e-06</td>\n",
       "      <td>-1.327225e-05</td>\n",
       "      <td>9.985364e-01</td>\n",
       "      <td>2.613459e-05</td>\n",
       "      <td>9.909152e-01</td>\n",
       "      <td>-2.027520e-05</td>\n",
       "      <td>7.716199e-06</td>\n",
       "      <td>9.999687e-01</td>\n",
       "      <td>9.927294e-01</td>\n",
       "      <td>-1.026444e-05</td>\n",
       "      <td>-2.076887e-05</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>9.922591e-01</td>\n",
       "      <td>1.459561e-05</td>\n",
       "      <td>3.678632e-06</td>\n",
       "      <td>1.000011e+00</td>\n",
       "      <td>9.861087e-01</td>\n",
       "      <td>-5.756954e-06</td>\n",
       "      <td>1.744903e-05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.034290e+00</td>\n",
       "      <td>1.024805e+00</td>\n",
       "      <td>1.050554e+00</td>\n",
       "      <td>1.009742e+00</td>\n",
       "      <td>9.729596e-01</td>\n",
       "      <td>1.033036e+00</td>\n",
       "      <td>9.598120e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.991040e-01</td>\n",
       "      <td>5.653777e-01</td>\n",
       "      <td>1.008827e+00</td>\n",
       "      <td>1.006346e+00</td>\n",
       "      <td>6.000185e-01</td>\n",
       "      <td>1.006326e+00</td>\n",
       "      <td>4.749747e-01</td>\n",
       "      <td>1.009303e+00</td>\n",
       "      <td>1.005901e+00</td>\n",
       "      <td>1.027808e+00</td>\n",
       "      <td>4.999939e-01</td>\n",
       "      <td>1.009331e+00</td>\n",
       "      <td>1.006154e+00</td>\n",
       "      <td>1.049398e+00</td>\n",
       "      <td>4.876623e-01</td>\n",
       "      <td>1.008747e+00</td>\n",
       "      <td>1.006305e+00</td>\n",
       "      <td>1.193676e+00</td>\n",
       "      <td>5.057777e-01</td>\n",
       "      <td>1.007694e+00</td>\n",
       "      <td>1.006366e+00</td>\n",
       "      <td>1.400209e+00</td>\n",
       "      <td>6.746354e-01</td>\n",
       "      <td>3.808074e-01</td>\n",
       "      <td>1.645763e-01</td>\n",
       "      <td>3.974453e-01</td>\n",
       "      <td>5.254063e-01</td>\n",
       "      <td>3.652556e-01</td>\n",
       "      <td>3.133378e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.746966e-01</td>\n",
       "      <td>-2.434976e+00</td>\n",
       "      <td>-1.742508e+00</td>\n",
       "      <td>2.370088e-04</td>\n",
       "      <td>-1.743944e+00</td>\n",
       "      <td>1.375024e-01</td>\n",
       "      <td>-2.969725e+00</td>\n",
       "      <td>-1.741237e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.889811e-01</td>\n",
       "      <td>-2.913090e+00</td>\n",
       "      <td>-1.742372e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.636076e-01</td>\n",
       "      <td>-2.729663e+00</td>\n",
       "      <td>-1.742069e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.653542e-01</td>\n",
       "      <td>-2.497265e+00</td>\n",
       "      <td>-1.742691e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.507046e-02</td>\n",
       "      <td>1.986757e-01</td>\n",
       "      <td>8.304866e-02</td>\n",
       "      <td>1.320062e-01</td>\n",
       "      <td>4.786215e-02</td>\n",
       "      <td>2.951122e-01</td>\n",
       "      <td>3.307214e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.907533e-01</td>\n",
       "      <td>-7.383225e-01</td>\n",
       "      <td>-8.719308e-01</td>\n",
       "      <td>5.768156e-01</td>\n",
       "      <td>-8.712081e-01</td>\n",
       "      <td>6.789927e-01</td>\n",
       "      <td>-6.872450e-01</td>\n",
       "      <td>-8.680962e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.564608e-01</td>\n",
       "      <td>-6.944718e-01</td>\n",
       "      <td>-8.701791e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.508527e-01</td>\n",
       "      <td>-6.998083e-01</td>\n",
       "      <td>-8.711343e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.177673e-01</td>\n",
       "      <td>-7.141902e-01</td>\n",
       "      <td>-8.714789e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.906095e-01</td>\n",
       "      <td>8.462266e-01</td>\n",
       "      <td>9.857525e-01</td>\n",
       "      <td>7.675732e-01</td>\n",
       "      <td>6.738168e-01</td>\n",
       "      <td>8.193964e-01</td>\n",
       "      <td>7.703901e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.533714e-01</td>\n",
       "      <td>-5.415563e-05</td>\n",
       "      <td>-2.410638e-04</td>\n",
       "      <td>8.916277e-01</td>\n",
       "      <td>2.125454e-04</td>\n",
       "      <td>8.948193e-01</td>\n",
       "      <td>-2.543566e-05</td>\n",
       "      <td>5.813991e-05</td>\n",
       "      <td>1.086538e+00</td>\n",
       "      <td>8.901377e-01</td>\n",
       "      <td>6.027267e-05</td>\n",
       "      <td>3.514990e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.972494e-01</td>\n",
       "      <td>1.728937e-04</td>\n",
       "      <td>-7.519117e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.682333e-01</td>\n",
       "      <td>3.721330e-04</td>\n",
       "      <td>-2.642369e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.949304e-01</td>\n",
       "      <td>9.506853e-01</td>\n",
       "      <td>9.897798e-01</td>\n",
       "      <td>9.165110e-01</td>\n",
       "      <td>8.733798e-01</td>\n",
       "      <td>9.473447e-01</td>\n",
       "      <td>8.719701e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.236226e+00</td>\n",
       "      <td>7.382142e-01</td>\n",
       "      <td>8.709940e-01</td>\n",
       "      <td>1.293056e+00</td>\n",
       "      <td>8.714708e-01</td>\n",
       "      <td>1.170740e+00</td>\n",
       "      <td>6.871941e-01</td>\n",
       "      <td>8.683126e-01</td>\n",
       "      <td>2.173076e+00</td>\n",
       "      <td>1.201875e+00</td>\n",
       "      <td>6.945924e-01</td>\n",
       "      <td>8.698727e-01</td>\n",
       "      <td>2.214872e+00</td>\n",
       "      <td>1.221798e+00</td>\n",
       "      <td>7.001541e-01</td>\n",
       "      <td>8.713947e-01</td>\n",
       "      <td>2.548224e+00</td>\n",
       "      <td>1.220930e+00</td>\n",
       "      <td>7.141017e-01</td>\n",
       "      <td>8.716055e-01</td>\n",
       "      <td>3.101961e+00</td>\n",
       "      <td>1.024730e+00</td>\n",
       "      <td>1.083493e+00</td>\n",
       "      <td>1.020528e+00</td>\n",
       "      <td>1.142226e+00</td>\n",
       "      <td>1.138439e+00</td>\n",
       "      <td>1.140458e+00</td>\n",
       "      <td>1.059248e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.209891e+01</td>\n",
       "      <td>2.434868e+00</td>\n",
       "      <td>1.743236e+00</td>\n",
       "      <td>1.539682e+01</td>\n",
       "      <td>1.743257e+00</td>\n",
       "      <td>9.940391e+00</td>\n",
       "      <td>2.969674e+00</td>\n",
       "      <td>1.741454e+00</td>\n",
       "      <td>2.173076e+00</td>\n",
       "      <td>1.164708e+01</td>\n",
       "      <td>2.913210e+00</td>\n",
       "      <td>1.743175e+00</td>\n",
       "      <td>2.214872e+00</td>\n",
       "      <td>1.470899e+01</td>\n",
       "      <td>2.730009e+00</td>\n",
       "      <td>1.742884e+00</td>\n",
       "      <td>2.548224e+00</td>\n",
       "      <td>1.288257e+01</td>\n",
       "      <td>2.498009e+00</td>\n",
       "      <td>1.743372e+00</td>\n",
       "      <td>3.101961e+00</td>\n",
       "      <td>4.019237e+01</td>\n",
       "      <td>2.037278e+01</td>\n",
       "      <td>7.992739e+00</td>\n",
       "      <td>1.426244e+01</td>\n",
       "      <td>1.776285e+01</td>\n",
       "      <td>1.149652e+01</td>\n",
       "      <td>8.374498e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target     lepton_ph    lepton_eta    lepton_phi  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   5.299203e-01  9.914658e-01 -8.297618e-06 -1.327225e-05   \n",
       "std    4.991040e-01  5.653777e-01  1.008827e+00  1.006346e+00   \n",
       "min    0.000000e+00  2.746966e-01 -2.434976e+00 -1.742508e+00   \n",
       "25%    0.000000e+00  5.907533e-01 -7.383225e-01 -8.719308e-01   \n",
       "50%    1.000000e+00  8.533714e-01 -5.415563e-05 -2.410638e-04   \n",
       "75%    1.000000e+00  1.236226e+00  7.382142e-01  8.709940e-01   \n",
       "max    1.000000e+00  1.209891e+01  2.434868e+00  1.743236e+00   \n",
       "\n",
       "       missing_energy_magnitude  missing_energy_phi      jet_1_pt  \\\n",
       "count              1.100000e+07        1.100000e+07  1.100000e+07   \n",
       "mean               9.985364e-01        2.613459e-05  9.909152e-01   \n",
       "std                6.000185e-01        1.006326e+00  4.749747e-01   \n",
       "min                2.370088e-04       -1.743944e+00  1.375024e-01   \n",
       "25%                5.768156e-01       -8.712081e-01  6.789927e-01   \n",
       "50%                8.916277e-01        2.125454e-04  8.948193e-01   \n",
       "75%                1.293056e+00        8.714708e-01  1.170740e+00   \n",
       "max                1.539682e+01        1.743257e+00  9.940391e+00   \n",
       "\n",
       "          jet_1_eta     jet_1_phi    jet_1_btag      jet_2_pt     jet_2_eta  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean  -2.027520e-05  7.716199e-06  9.999687e-01  9.927294e-01 -1.026444e-05   \n",
       "std    1.009303e+00  1.005901e+00  1.027808e+00  4.999939e-01  1.009331e+00   \n",
       "min   -2.969725e+00 -1.741237e+00  0.000000e+00  1.889811e-01 -2.913090e+00   \n",
       "25%   -6.872450e-01 -8.680962e-01  0.000000e+00  6.564608e-01 -6.944718e-01   \n",
       "50%   -2.543566e-05  5.813991e-05  1.086538e+00  8.901377e-01  6.027267e-05   \n",
       "75%    6.871941e-01  8.683126e-01  2.173076e+00  1.201875e+00  6.945924e-01   \n",
       "max    2.969674e+00  1.741454e+00  2.173076e+00  1.164708e+01  2.913210e+00   \n",
       "\n",
       "          jet_2_phi    jet_2_btag      jet_3_pt     jet_3_eta     jet_3_phi  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean  -2.076887e-05  1.000008e+00  9.922591e-01  1.459561e-05  3.678632e-06   \n",
       "std    1.006154e+00  1.049398e+00  4.876623e-01  1.008747e+00  1.006305e+00   \n",
       "min   -1.742372e+00  0.000000e+00  2.636076e-01 -2.729663e+00 -1.742069e+00   \n",
       "25%   -8.701791e-01  0.000000e+00  6.508527e-01 -6.998083e-01 -8.711343e-01   \n",
       "50%    3.514990e-04  0.000000e+00  8.972494e-01  1.728937e-04 -7.519117e-04   \n",
       "75%    8.698727e-01  2.214872e+00  1.221798e+00  7.001541e-01  8.713947e-01   \n",
       "max    1.743175e+00  2.214872e+00  1.470899e+01  2.730009e+00  1.742884e+00   \n",
       "\n",
       "         jet_3_btag      jet_4_pt     jet_4_eta     jet_4_phi    jet_4_btag  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   1.000011e+00  9.861087e-01 -5.756954e-06  1.744903e-05  1.000000e+00   \n",
       "std    1.193676e+00  5.057777e-01  1.007694e+00  1.006366e+00  1.400209e+00   \n",
       "min    0.000000e+00  3.653542e-01 -2.497265e+00 -1.742691e+00  0.000000e+00   \n",
       "25%    0.000000e+00  6.177673e-01 -7.141902e-01 -8.714789e-01  0.000000e+00   \n",
       "50%    0.000000e+00  8.682333e-01  3.721330e-04 -2.642369e-04  0.000000e+00   \n",
       "75%    2.548224e+00  1.220930e+00  7.141017e-01  8.716055e-01  3.101961e+00   \n",
       "max    2.548224e+00  1.288257e+01  2.498009e+00  1.743372e+00  3.101961e+00   \n",
       "\n",
       "               m_jj         m_jjj          m_lv         m_jlv          m_bb  \\\n",
       "count  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07  1.100000e+07   \n",
       "mean   1.034290e+00  1.024805e+00  1.050554e+00  1.009742e+00  9.729596e-01   \n",
       "std    6.746354e-01  3.808074e-01  1.645763e-01  3.974453e-01  5.254063e-01   \n",
       "min    7.507046e-02  1.986757e-01  8.304866e-02  1.320062e-01  4.786215e-02   \n",
       "25%    7.906095e-01  8.462266e-01  9.857525e-01  7.675732e-01  6.738168e-01   \n",
       "50%    8.949304e-01  9.506853e-01  9.897798e-01  9.165110e-01  8.733798e-01   \n",
       "75%    1.024730e+00  1.083493e+00  1.020528e+00  1.142226e+00  1.138439e+00   \n",
       "max    4.019237e+01  2.037278e+01  7.992739e+00  1.426244e+01  1.776285e+01   \n",
       "\n",
       "              m_wbb        m_wwbb  \n",
       "count  1.100000e+07  1.100000e+07  \n",
       "mean   1.033036e+00  9.598120e-01  \n",
       "std    3.652556e-01  3.133378e-01  \n",
       "min    2.951122e-01  3.307214e-01  \n",
       "25%    8.193964e-01  7.703901e-01  \n",
       "50%    9.473447e-01  8.719701e-01  \n",
       "75%    1.140458e+00  1.059248e+00  \n",
       "max    1.149652e+01  8.374498e+00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#figure out which columns have values strictly greater than 0, for scaler purposes\n",
    "pd.set_option(\"display.max_rows\", 500, \"display.max_columns\", None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600000, 29)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "pre_X = train.loc[:, df.columns != 'target']\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_train = scaler.fit_transform(pre_X)\n",
    "# X = pd.DataFrame(data=scaled_train, columns=pre_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.695209e-01</td>\n",
       "      <td>-6.625666e-01</td>\n",
       "      <td>-7.968404e-01</td>\n",
       "      <td>-6.073939e-01</td>\n",
       "      <td>-2.264619e-01</td>\n",
       "      <td>-4.952153e-01</td>\n",
       "      <td>-4.221755e-01</td>\n",
       "      <td>-1.084725e+00</td>\n",
       "      <td>-4.670169e+00</td>\n",
       "      <td>-9.966000e-01</td>\n",
       "      <td>-7.556114e-01</td>\n",
       "      <td>-9.412959e-01</td>\n",
       "      <td>-9.336352e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.906835e-01</td>\n",
       "      <td>2.969626e-01</td>\n",
       "      <td>3.428450e-01</td>\n",
       "      <td>3.272016e-01</td>\n",
       "      <td>2.719195e-01</td>\n",
       "      <td>2.992355e-01</td>\n",
       "      <td>6.380529e-01</td>\n",
       "      <td>5.307856e-01</td>\n",
       "      <td>6.057191e-01</td>\n",
       "      <td>3.903798e-01</td>\n",
       "      <td>4.299310e-01</td>\n",
       "      <td>4.150297e-01</td>\n",
       "      <td>3.953860e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.556306e-01</td>\n",
       "      <td>8.220505e-01</td>\n",
       "      <td>7.978708e-01</td>\n",
       "      <td>7.947510e-01</td>\n",
       "      <td>7.664560e-01</td>\n",
       "      <td>8.050897e-01</td>\n",
       "      <td>7.926677e-01</td>\n",
       "      <td>8.047199e-01</td>\n",
       "      <td>6.301876e-01</td>\n",
       "      <td>7.658091e-01</td>\n",
       "      <td>8.108293e-01</td>\n",
       "      <td>7.650499e-01</td>\n",
       "      <td>7.195430e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.432801e+00</td>\n",
       "      <td>1.489800e+00</td>\n",
       "      <td>1.378221e+00</td>\n",
       "      <td>1.418066e+00</td>\n",
       "      <td>1.464286e+00</td>\n",
       "      <td>1.471996e+00</td>\n",
       "      <td>9.855056e-01</td>\n",
       "      <td>1.154184e+00</td>\n",
       "      <td>8.183915e-01</td>\n",
       "      <td>1.333369e+00</td>\n",
       "      <td>1.315954e+00</td>\n",
       "      <td>1.293396e+00</td>\n",
       "      <td>1.316636e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.991209e+01</td>\n",
       "      <td>2.073755e+01</td>\n",
       "      <td>1.696594e+01</td>\n",
       "      <td>1.894311e+01</td>\n",
       "      <td>2.448835e+01</td>\n",
       "      <td>2.914014e+01</td>\n",
       "      <td>4.321080e+01</td>\n",
       "      <td>3.756345e+01</td>\n",
       "      <td>3.609834e+01</td>\n",
       "      <td>2.720432e+01</td>\n",
       "      <td>2.530246e+01</td>\n",
       "      <td>2.221914e+01</td>\n",
       "      <td>1.902509e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lepton_ph  missing_energy_magnitude      jet_1_pt      jet_2_pt  \\\n",
       "count  2.600000e+06              2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.000000e+00              1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00              1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.695209e-01             -6.625666e-01 -7.968404e-01 -6.073939e-01   \n",
       "25%    2.906835e-01              2.969626e-01  3.428450e-01  3.272016e-01   \n",
       "50%    7.556306e-01              8.220505e-01  7.978708e-01  7.947510e-01   \n",
       "75%    1.432801e+00              1.489800e+00  1.378221e+00  1.418066e+00   \n",
       "max    1.991209e+01              2.073755e+01  1.696594e+01  1.894311e+01   \n",
       "\n",
       "           jet_4_pt      jet_3_pt          m_jj         m_jjj          m_lv  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.264619e-01 -4.952153e-01 -4.221755e-01 -1.084725e+00 -4.670169e+00   \n",
       "25%    2.719195e-01  2.992355e-01  6.380529e-01  5.307856e-01  6.057191e-01   \n",
       "50%    7.664560e-01  8.050897e-01  7.926677e-01  8.047199e-01  6.301876e-01   \n",
       "75%    1.464286e+00  1.471996e+00  9.855056e-01  1.154184e+00  8.183915e-01   \n",
       "max    2.448835e+01  2.914014e+01  4.321080e+01  3.756345e+01  3.609834e+01   \n",
       "\n",
       "              m_jlv          m_bb         m_wbb        m_wwbb  \n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "min   -9.966000e-01 -7.556114e-01 -9.412959e-01 -9.336352e-01  \n",
       "25%    3.903798e-01  4.299310e-01  4.150297e-01  3.953860e-01  \n",
       "50%    7.658091e-01  8.108293e-01  7.650499e-01  7.195430e-01  \n",
       "75%    1.333369e+00  1.315954e+00  1.293396e+00  1.316636e+00  \n",
       "max    2.720432e+01  2.530246e+01  2.221914e+01  1.902509e+01  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not complete, need columns where mean=1 and stdev=1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#NOT strictly greater than 0 columns:\n",
    "not_greater_than_0 = ['lepton_eta','lepton_phi','missing_energy_phi','jet_1_eta','jet_1_phi','jet_1_btag',\n",
    "                      'jet_2_eta','jet_2_phi','jet_2_btag','jet_3_eta','jet_3_phi','jet_3_btag','jet_4_eta',\n",
    "                      'jet_4_phi','jet_4_btag']\n",
    "\n",
    "#strictly greater than 0 columns:\n",
    "greater_than_0 = ['lepton_ph','missing_energy_magnitude','jet_1_pt','jet_2_pt','jet_4_pt','jet_3_pt','m_jj','m_jjj','m_lv',\n",
    "                  'm_jlv','m_bb','m_wbb','m_wwbb']\n",
    "\n",
    "\n",
    "#these columns scale where mean=0 and stdev=1\n",
    "to_scale1 = pre_X[not_greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train1 = scaler.fit_transform(to_scale1)\n",
    "scaled_train_df1 = pd.DataFrame(scaled_train1, columns=not_greater_than_0)\n",
    "\n",
    "#these columns scale where mean=1 and stdev=1\n",
    "to_scale2 = pre_X[greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train2 = scaler.fit_transform(to_scale2)\n",
    "scaled_train_df2 = pd.DataFrame(scaled_train2 + 1, columns=greater_than_0)\n",
    "scaled_train_df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600000, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled_train2 + 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_X[greater_than_0].to_numpy().shape\n",
    "\n",
    "# np_greater_than_0 = pre_X[greater_than_0].to_numpy()\n",
    "\n",
    "def manual_scaling(x, desired_mean=0, desired_std=1):\n",
    "    new_x = np.empty(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        mean = np.mean(x[:, i])\n",
    "        std = np.std(x[:, i])\n",
    "        new_x[:, i] = desired_mean + (x[:, i] - mean) * (desired_std / std)\n",
    "    return new_x\n",
    "\n",
    "np_greater_than_0 = manual_scaling(pre_X[greater_than_0].to_numpy(), desired_mean=1, desired_std=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.175,  0.16 , -0.132, ...,  1.105,  1.272,  0.858],\n",
       "       [ 0.51 ,  4.281,  1.223, ...,  0.709,  1.38 ,  1.777],\n",
       "       [-0.131,  0.97 ,  0.063, ...,  0.414,  0.217,  0.345],\n",
       "       ...,\n",
       "       [ 0.141,  0.259,  1.578, ...,  0.017,  1.233,  0.923],\n",
       "       [ 1.813,  0.645,  1.675, ...,  1.379,  1.363,  1.836],\n",
       "       [ 0.164,  0.846,  1.208, ...,  1.776,  0.987,  0.697]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round((np_greater_than_0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.175,  0.16 , -0.132, ...,  1.105,  1.272,  0.858],\n",
       "       [ 0.51 ,  4.281,  1.223, ...,  0.709,  1.38 ,  1.777],\n",
       "       [-0.131,  0.97 ,  0.063, ...,  0.414,  0.217,  0.345],\n",
       "       ...,\n",
       "       [ 0.141,  0.259,  1.578, ...,  0.017,  1.233,  0.923],\n",
       "       [ 1.813,  0.645,  1.675, ...,  1.379,  1.363,  1.836],\n",
       "       [ 0.164,  0.846,  1.208, ...,  1.776,  0.987,  0.697]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round((scaled_train2 + 1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_they_the_same = np.round((np_greater_than_0), 3) == np.round((scaled_train2 + 1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2600000, 2600000, 2600000, 2600000, 2600000, 2600000, 2600000,\n",
       "       2600000, 2600000, 2600000, 2600000, 2600000, 2600000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(are_they_the_same == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.322691</td>\n",
       "      <td>0.863880</td>\n",
       "      <td>-0.572453</td>\n",
       "      <td>-2.141693</td>\n",
       "      <td>-0.523203</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>1.723081</td>\n",
       "      <td>-1.709792</td>\n",
       "      <td>-0.952660</td>\n",
       "      <td>0.025451</td>\n",
       "      <td>1.289919</td>\n",
       "      <td>1.296983</td>\n",
       "      <td>-0.002654</td>\n",
       "      <td>-0.786827</td>\n",
       "      <td>-0.714619</td>\n",
       "      <td>1.174569</td>\n",
       "      <td>0.160122</td>\n",
       "      <td>-0.132040</td>\n",
       "      <td>0.571042</td>\n",
       "      <td>0.458873</td>\n",
       "      <td>0.872105</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>1.043547</td>\n",
       "      <td>0.613459</td>\n",
       "      <td>0.855754</td>\n",
       "      <td>1.105013</td>\n",
       "      <td>1.271511</td>\n",
       "      <td>0.857620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416089</td>\n",
       "      <td>0.695152</td>\n",
       "      <td>0.802544</td>\n",
       "      <td>-0.470049</td>\n",
       "      <td>-0.668134</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>-0.789944</td>\n",
       "      <td>-1.505140</td>\n",
       "      <td>1.158021</td>\n",
       "      <td>0.802190</td>\n",
       "      <td>-0.192560</td>\n",
       "      <td>-0.837770</td>\n",
       "      <td>0.480915</td>\n",
       "      <td>1.476132</td>\n",
       "      <td>-0.714619</td>\n",
       "      <td>0.509711</td>\n",
       "      <td>4.281044</td>\n",
       "      <td>1.223397</td>\n",
       "      <td>2.620156</td>\n",
       "      <td>1.344617</td>\n",
       "      <td>2.941280</td>\n",
       "      <td>0.587525</td>\n",
       "      <td>0.553713</td>\n",
       "      <td>0.652878</td>\n",
       "      <td>2.778725</td>\n",
       "      <td>0.708706</td>\n",
       "      <td>1.379553</td>\n",
       "      <td>1.777043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.014625</td>\n",
       "      <td>0.186212</td>\n",
       "      <td>-1.239583</td>\n",
       "      <td>1.129981</td>\n",
       "      <td>-1.392778</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>1.064422</td>\n",
       "      <td>-0.952660</td>\n",
       "      <td>-0.055741</td>\n",
       "      <td>-0.050936</td>\n",
       "      <td>1.296983</td>\n",
       "      <td>2.108518</td>\n",
       "      <td>-0.857386</td>\n",
       "      <td>-0.714619</td>\n",
       "      <td>-0.131495</td>\n",
       "      <td>0.969540</td>\n",
       "      <td>0.063081</td>\n",
       "      <td>1.850889</td>\n",
       "      <td>1.313378</td>\n",
       "      <td>2.026490</td>\n",
       "      <td>0.902127</td>\n",
       "      <td>1.344002</td>\n",
       "      <td>0.611636</td>\n",
       "      <td>0.129997</td>\n",
       "      <td>0.414243</td>\n",
       "      <td>0.217026</td>\n",
       "      <td>0.345026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.836948</td>\n",
       "      <td>-1.475806</td>\n",
       "      <td>0.854693</td>\n",
       "      <td>2.403334</td>\n",
       "      <td>-1.289730</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>0.352078</td>\n",
       "      <td>0.411301</td>\n",
       "      <td>1.158021</td>\n",
       "      <td>-0.669193</td>\n",
       "      <td>0.629738</td>\n",
       "      <td>-0.837770</td>\n",
       "      <td>0.763617</td>\n",
       "      <td>-0.477577</td>\n",
       "      <td>1.500372</td>\n",
       "      <td>0.974982</td>\n",
       "      <td>-0.110502</td>\n",
       "      <td>0.356920</td>\n",
       "      <td>0.772858</td>\n",
       "      <td>2.591291</td>\n",
       "      <td>1.104855</td>\n",
       "      <td>0.865492</td>\n",
       "      <td>1.335313</td>\n",
       "      <td>0.656397</td>\n",
       "      <td>1.385766</td>\n",
       "      <td>0.819796</td>\n",
       "      <td>1.213709</td>\n",
       "      <td>0.937928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.177572</td>\n",
       "      <td>0.531387</td>\n",
       "      <td>-1.002016</td>\n",
       "      <td>0.825868</td>\n",
       "      <td>0.737725</td>\n",
       "      <td>-0.972454</td>\n",
       "      <td>1.490251</td>\n",
       "      <td>-0.738936</td>\n",
       "      <td>-0.952660</td>\n",
       "      <td>-0.136933</td>\n",
       "      <td>1.334005</td>\n",
       "      <td>1.296983</td>\n",
       "      <td>1.046319</td>\n",
       "      <td>-0.080681</td>\n",
       "      <td>0.392877</td>\n",
       "      <td>-0.051790</td>\n",
       "      <td>2.577822</td>\n",
       "      <td>0.874030</td>\n",
       "      <td>0.138219</td>\n",
       "      <td>1.611349</td>\n",
       "      <td>0.954612</td>\n",
       "      <td>0.795040</td>\n",
       "      <td>1.126510</td>\n",
       "      <td>1.901067</td>\n",
       "      <td>2.499051</td>\n",
       "      <td>1.154495</td>\n",
       "      <td>1.196558</td>\n",
       "      <td>1.717747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton_eta  lepton_phi  missing_energy_phi  jet_1_eta  jet_1_phi  \\\n",
       "0   -0.322691    0.863880           -0.572453  -2.141693  -0.523203   \n",
       "1    0.416089    0.695152            0.802544  -0.470049  -0.668134   \n",
       "2   -0.014625    0.186212           -1.239583   1.129981  -1.392778   \n",
       "3   -1.836948   -1.475806            0.854693   2.403334  -1.289730   \n",
       "4    2.177572    0.531387           -1.002016   0.825868   0.737725   \n",
       "\n",
       "   jet_1_btag  jet_2_eta  jet_2_phi  jet_2_btag  jet_3_eta  jet_3_phi  \\\n",
       "0   -0.972454   1.723081  -1.709792   -0.952660   0.025451   1.289919   \n",
       "1   -0.972454  -0.789944  -1.505140    1.158021   0.802190  -0.192560   \n",
       "2   -0.972454   0.016302   1.064422   -0.952660  -0.055741  -0.050936   \n",
       "3   -0.972454   0.352078   0.411301    1.158021  -0.669193   0.629738   \n",
       "4   -0.972454   1.490251  -0.738936   -0.952660  -0.136933   1.334005   \n",
       "\n",
       "   jet_3_btag  jet_4_eta  jet_4_phi  jet_4_btag  lepton_ph  \\\n",
       "0    1.296983  -0.002654  -0.786827   -0.714619   1.174569   \n",
       "1   -0.837770   0.480915   1.476132   -0.714619   0.509711   \n",
       "2    1.296983   2.108518  -0.857386   -0.714619  -0.131495   \n",
       "3   -0.837770   0.763617  -0.477577    1.500372   0.974982   \n",
       "4    1.296983   1.046319  -0.080681    0.392877  -0.051790   \n",
       "\n",
       "   missing_energy_magnitude  jet_1_pt  jet_2_pt  jet_4_pt  jet_3_pt      m_jj  \\\n",
       "0                  0.160122 -0.132040  0.571042  0.458873  0.872105  0.537537   \n",
       "1                  4.281044  1.223397  2.620156  1.344617  2.941280  0.587525   \n",
       "2                  0.969540  0.063081  1.850889  1.313378  2.026490  0.902127   \n",
       "3                 -0.110502  0.356920  0.772858  2.591291  1.104855  0.865492   \n",
       "4                  2.577822  0.874030  0.138219  1.611349  0.954612  0.795040   \n",
       "\n",
       "      m_jjj      m_lv     m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "0  1.043547  0.613459  0.855754  1.105013  1.271511  0.857620  \n",
       "1  0.553713  0.652878  2.778725  0.708706  1.379553  1.777043  \n",
       "2  1.344002  0.611636  0.129997  0.414243  0.217026  0.345026  \n",
       "3  1.335313  0.656397  1.385766  0.819796  1.213709  0.937928  \n",
       "4  1.126510  1.901067  2.499051  1.154495  1.196558  1.717747  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([scaled_train_df1, scaled_train_df2], axis=1, sort=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=500000, random_state=1776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = y_train.to_numpy()\n",
    "# type(np.unique(y)[0])\n",
    "\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100000, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "      <td>2.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.470954e-18</td>\n",
       "      <td>1.700110e-17</td>\n",
       "      <td>-2.381958e-17</td>\n",
       "      <td>-9.263701e-18</td>\n",
       "      <td>-3.566378e-17</td>\n",
       "      <td>7.213785e-17</td>\n",
       "      <td>-1.918192e-17</td>\n",
       "      <td>-1.036573e-17</td>\n",
       "      <td>3.512711e-17</td>\n",
       "      <td>7.341819e-18</td>\n",
       "      <td>2.970888e-17</td>\n",
       "      <td>-1.521518e-17</td>\n",
       "      <td>-1.473556e-17</td>\n",
       "      <td>2.121790e-17</td>\n",
       "      <td>1.082293e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.414452e+00</td>\n",
       "      <td>-1.731104e+00</td>\n",
       "      <td>-1.731925e+00</td>\n",
       "      <td>-2.942199e+00</td>\n",
       "      <td>-1.730027e+00</td>\n",
       "      <td>-9.724543e-01</td>\n",
       "      <td>-2.885415e+00</td>\n",
       "      <td>-1.732408e+00</td>\n",
       "      <td>-9.526596e-01</td>\n",
       "      <td>-2.705312e+00</td>\n",
       "      <td>-1.730599e+00</td>\n",
       "      <td>-8.377697e-01</td>\n",
       "      <td>-2.479189e+00</td>\n",
       "      <td>-1.731107e+00</td>\n",
       "      <td>-7.146187e-01</td>\n",
       "      <td>-2.695209e-01</td>\n",
       "      <td>-6.625666e-01</td>\n",
       "      <td>-7.968404e-01</td>\n",
       "      <td>-6.073939e-01</td>\n",
       "      <td>-2.264619e-01</td>\n",
       "      <td>-4.952153e-01</td>\n",
       "      <td>-4.221755e-01</td>\n",
       "      <td>-1.084725e+00</td>\n",
       "      <td>-4.670169e+00</td>\n",
       "      <td>-9.966000e-01</td>\n",
       "      <td>-7.556114e-01</td>\n",
       "      <td>-9.412959e-01</td>\n",
       "      <td>-9.336352e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.321584e-01</td>\n",
       "      <td>-8.659604e-01</td>\n",
       "      <td>-8.658190e-01</td>\n",
       "      <td>-6.819477e-01</td>\n",
       "      <td>-8.632094e-01</td>\n",
       "      <td>-9.724543e-01</td>\n",
       "      <td>-6.889230e-01</td>\n",
       "      <td>-8.647062e-01</td>\n",
       "      <td>-9.526596e-01</td>\n",
       "      <td>-6.935506e-01</td>\n",
       "      <td>-8.659706e-01</td>\n",
       "      <td>-8.377697e-01</td>\n",
       "      <td>-7.085818e-01</td>\n",
       "      <td>-8.667571e-01</td>\n",
       "      <td>-7.146187e-01</td>\n",
       "      <td>2.906835e-01</td>\n",
       "      <td>2.969626e-01</td>\n",
       "      <td>3.428450e-01</td>\n",
       "      <td>3.272016e-01</td>\n",
       "      <td>2.719195e-01</td>\n",
       "      <td>2.992355e-01</td>\n",
       "      <td>6.380529e-01</td>\n",
       "      <td>5.307856e-01</td>\n",
       "      <td>6.057191e-01</td>\n",
       "      <td>3.903798e-01</td>\n",
       "      <td>4.299310e-01</td>\n",
       "      <td>4.150297e-01</td>\n",
       "      <td>3.953860e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.386904e-04</td>\n",
       "      <td>-1.365973e-03</td>\n",
       "      <td>2.090639e-04</td>\n",
       "      <td>-1.125500e-03</td>\n",
       "      <td>-7.977719e-04</td>\n",
       "      <td>8.484855e-02</td>\n",
       "      <td>9.082319e-04</td>\n",
       "      <td>8.939700e-04</td>\n",
       "      <td>-9.526596e-01</td>\n",
       "      <td>1.093501e-03</td>\n",
       "      <td>3.138265e-04</td>\n",
       "      <td>-8.377697e-01</td>\n",
       "      <td>6.528141e-04</td>\n",
       "      <td>-1.998949e-04</td>\n",
       "      <td>-7.146187e-01</td>\n",
       "      <td>7.556306e-01</td>\n",
       "      <td>8.220505e-01</td>\n",
       "      <td>7.978708e-01</td>\n",
       "      <td>7.947510e-01</td>\n",
       "      <td>7.664560e-01</td>\n",
       "      <td>8.050897e-01</td>\n",
       "      <td>7.926677e-01</td>\n",
       "      <td>8.047199e-01</td>\n",
       "      <td>6.301876e-01</td>\n",
       "      <td>7.658091e-01</td>\n",
       "      <td>8.108293e-01</td>\n",
       "      <td>7.650499e-01</td>\n",
       "      <td>7.195430e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.318811e-01</td>\n",
       "      <td>8.655338e-01</td>\n",
       "      <td>8.662048e-01</td>\n",
       "      <td>6.806777e-01</td>\n",
       "      <td>8.628153e-01</td>\n",
       "      <td>1.142151e+00</td>\n",
       "      <td>6.888153e-01</td>\n",
       "      <td>8.641829e-01</td>\n",
       "      <td>1.158021e+00</td>\n",
       "      <td>6.939334e-01</td>\n",
       "      <td>8.661466e-01</td>\n",
       "      <td>1.296983e+00</td>\n",
       "      <td>7.082342e-01</td>\n",
       "      <td>8.659056e-01</td>\n",
       "      <td>1.500372e+00</td>\n",
       "      <td>1.432801e+00</td>\n",
       "      <td>1.489800e+00</td>\n",
       "      <td>1.378221e+00</td>\n",
       "      <td>1.418066e+00</td>\n",
       "      <td>1.464286e+00</td>\n",
       "      <td>1.471996e+00</td>\n",
       "      <td>9.855056e-01</td>\n",
       "      <td>1.154184e+00</td>\n",
       "      <td>8.183915e-01</td>\n",
       "      <td>1.333369e+00</td>\n",
       "      <td>1.315954e+00</td>\n",
       "      <td>1.293396e+00</td>\n",
       "      <td>1.316636e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.414175e+00</td>\n",
       "      <td>1.732883e+00</td>\n",
       "      <td>1.732090e+00</td>\n",
       "      <td>2.941910e+00</td>\n",
       "      <td>1.731837e+00</td>\n",
       "      <td>1.142151e+00</td>\n",
       "      <td>2.885307e+00</td>\n",
       "      <td>1.732988e+00</td>\n",
       "      <td>1.158021e+00</td>\n",
       "      <td>2.705695e+00</td>\n",
       "      <td>1.731326e+00</td>\n",
       "      <td>1.296983e+00</td>\n",
       "      <td>2.478841e+00</td>\n",
       "      <td>1.731909e+00</td>\n",
       "      <td>1.500372e+00</td>\n",
       "      <td>1.991209e+01</td>\n",
       "      <td>2.073755e+01</td>\n",
       "      <td>1.696594e+01</td>\n",
       "      <td>1.894311e+01</td>\n",
       "      <td>2.448835e+01</td>\n",
       "      <td>2.914014e+01</td>\n",
       "      <td>4.321080e+01</td>\n",
       "      <td>3.756345e+01</td>\n",
       "      <td>3.609834e+01</td>\n",
       "      <td>2.720432e+01</td>\n",
       "      <td>2.530246e+01</td>\n",
       "      <td>2.221914e+01</td>\n",
       "      <td>1.902509e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lepton_eta    lepton_phi  missing_energy_phi     jet_1_eta  \\\n",
       "count  2.600000e+06  2.600000e+06        2.600000e+06  2.600000e+06   \n",
       "mean  -4.470954e-18  1.700110e-17       -2.381958e-17 -9.263701e-18   \n",
       "std    1.000000e+00  1.000000e+00        1.000000e+00  1.000000e+00   \n",
       "min   -2.414452e+00 -1.731104e+00       -1.731925e+00 -2.942199e+00   \n",
       "25%   -7.321584e-01 -8.659604e-01       -8.658190e-01 -6.819477e-01   \n",
       "50%   -1.386904e-04 -1.365973e-03        2.090639e-04 -1.125500e-03   \n",
       "75%    7.318811e-01  8.655338e-01        8.662048e-01  6.806777e-01   \n",
       "max    2.414175e+00  1.732883e+00        1.732090e+00  2.941910e+00   \n",
       "\n",
       "          jet_1_phi    jet_1_btag     jet_2_eta     jet_2_phi    jet_2_btag  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean  -3.566378e-17  7.213785e-17 -1.918192e-17 -1.036573e-17  3.512711e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -1.730027e+00 -9.724543e-01 -2.885415e+00 -1.732408e+00 -9.526596e-01   \n",
       "25%   -8.632094e-01 -9.724543e-01 -6.889230e-01 -8.647062e-01 -9.526596e-01   \n",
       "50%   -7.977719e-04  8.484855e-02  9.082319e-04  8.939700e-04 -9.526596e-01   \n",
       "75%    8.628153e-01  1.142151e+00  6.888153e-01  8.641829e-01  1.158021e+00   \n",
       "max    1.731837e+00  1.142151e+00  2.885307e+00  1.732988e+00  1.158021e+00   \n",
       "\n",
       "          jet_3_eta     jet_3_phi    jet_3_btag     jet_4_eta     jet_4_phi  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   7.341819e-18  2.970888e-17 -1.521518e-17 -1.473556e-17  2.121790e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.705312e+00 -1.730599e+00 -8.377697e-01 -2.479189e+00 -1.731107e+00   \n",
       "25%   -6.935506e-01 -8.659706e-01 -8.377697e-01 -7.085818e-01 -8.667571e-01   \n",
       "50%    1.093501e-03  3.138265e-04 -8.377697e-01  6.528141e-04 -1.998949e-04   \n",
       "75%    6.939334e-01  8.661466e-01  1.296983e+00  7.082342e-01  8.659056e-01   \n",
       "max    2.705695e+00  1.731326e+00  1.296983e+00  2.478841e+00  1.731909e+00   \n",
       "\n",
       "         jet_4_btag     lepton_ph  missing_energy_magnitude      jet_1_pt  \\\n",
       "count  2.600000e+06  2.600000e+06              2.600000e+06  2.600000e+06   \n",
       "mean   1.082293e-16  1.000000e+00              1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00  1.000000e+00              1.000000e+00  1.000000e+00   \n",
       "min   -7.146187e-01 -2.695209e-01             -6.625666e-01 -7.968404e-01   \n",
       "25%   -7.146187e-01  2.906835e-01              2.969626e-01  3.428450e-01   \n",
       "50%   -7.146187e-01  7.556306e-01              8.220505e-01  7.978708e-01   \n",
       "75%    1.500372e+00  1.432801e+00              1.489800e+00  1.378221e+00   \n",
       "max    1.500372e+00  1.991209e+01              2.073755e+01  1.696594e+01   \n",
       "\n",
       "           jet_2_pt      jet_4_pt      jet_3_pt          m_jj         m_jjj  \\\n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06   \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -6.073939e-01 -2.264619e-01 -4.952153e-01 -4.221755e-01 -1.084725e+00   \n",
       "25%    3.272016e-01  2.719195e-01  2.992355e-01  6.380529e-01  5.307856e-01   \n",
       "50%    7.947510e-01  7.664560e-01  8.050897e-01  7.926677e-01  8.047199e-01   \n",
       "75%    1.418066e+00  1.464286e+00  1.471996e+00  9.855056e-01  1.154184e+00   \n",
       "max    1.894311e+01  2.448835e+01  2.914014e+01  4.321080e+01  3.756345e+01   \n",
       "\n",
       "               m_lv         m_jlv          m_bb         m_wbb        m_wwbb  \n",
       "count  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  2.600000e+06  \n",
       "mean   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "min   -4.670169e+00 -9.966000e-01 -7.556114e-01 -9.412959e-01 -9.336352e-01  \n",
       "25%    6.057191e-01  3.903798e-01  4.299310e-01  4.150297e-01  3.953860e-01  \n",
       "50%    6.301876e-01  7.658091e-01  8.108293e-01  7.650499e-01  7.195430e-01  \n",
       "75%    8.183915e-01  1.333369e+00  1.315954e+00  1.293396e+00  1.316636e+00  \n",
       "max    3.609834e+01  2.720432e+01  2.530246e+01  2.221914e+01  1.902509e+01  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# input\n",
    "model.add(tf.keras.Input(shape=(28,)))\n",
    "# hidden\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.1)))  \n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(1,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.001)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=1e-5)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = tf.compat.v1.ConfigProto(\n",
    "#        device_count = {'GPU': 0}\n",
    "#    )\n",
    "#sess = tf.compat.v1.Session(config=config)\n",
    "#sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.5681 - auc: 0.7722 - val_loss: 0.5362 - val_auc: 0.8036\n",
      "Epoch 2/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.5266 - auc: 0.8113 - val_loss: 0.5184 - val_auc: 0.8185\n",
      "Epoch 3/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.5134 - auc: 0.8221 - val_loss: 0.5091 - val_auc: 0.8259\n",
      "Epoch 4/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.5056 - auc: 0.8283 - val_loss: 0.5045 - val_auc: 0.8301\n",
      "Epoch 5/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.5002 - auc: 0.8325 - val_loss: 0.5001 - val_auc: 0.8332\n",
      "Epoch 6/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4961 - auc: 0.8356 - val_loss: 0.4965 - val_auc: 0.8360\n",
      "Epoch 7/250\n",
      "21000/21000 [==============================] - 56s 3ms/step - loss: 0.4927 - auc: 0.8381 - val_loss: 0.4953 - val_auc: 0.8363\n",
      "Epoch 8/250\n",
      "21000/21000 [==============================] - 59s 3ms/step - loss: 0.4899 - auc: 0.8402 - val_loss: 0.4943 - val_auc: 0.8387\n",
      "Epoch 9/250\n",
      "21000/21000 [==============================] - 58s 3ms/step - loss: 0.4874 - auc: 0.8421 - val_loss: 0.4910 - val_auc: 0.8401\n",
      "Epoch 10/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.4854 - auc: 0.8435 - val_loss: 0.4907 - val_auc: 0.8398\n",
      "Epoch 11/250\n",
      "21000/21000 [==============================] - 50s 2ms/step - loss: 0.4832 - auc: 0.8451 - val_loss: 0.4870 - val_auc: 0.8423\n",
      "Epoch 12/250\n",
      "21000/21000 [==============================] - 49s 2ms/step - loss: 0.4815 - auc: 0.8464 - val_loss: 0.4876 - val_auc: 0.8420\n",
      "Epoch 13/250\n",
      "21000/21000 [==============================] - 49s 2ms/step - loss: 0.4797 - auc: 0.8476 - val_loss: 0.4883 - val_auc: 0.8430\n",
      "Epoch 14/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4781 - auc: 0.8489 - val_loss: 0.4894 - val_auc: 0.8408\n",
      "Epoch 15/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4766 - auc: 0.8499 - val_loss: 0.4861 - val_auc: 0.8445\n",
      "Epoch 16/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4752 - auc: 0.8509 - val_loss: 0.4838 - val_auc: 0.8451\n",
      "Epoch 17/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4738 - auc: 0.8519 - val_loss: 0.4846 - val_auc: 0.8441\n",
      "Epoch 18/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4725 - auc: 0.8528 - val_loss: 0.4841 - val_auc: 0.8445\n",
      "Epoch 19/250\n",
      "21000/21000 [==============================] - 49s 2ms/step - loss: 0.4712 - auc: 0.8537 - val_loss: 0.4844 - val_auc: 0.8452\n",
      "Epoch 20/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4700 - auc: 0.8546 - val_loss: 0.4834 - val_auc: 0.8457\n",
      "Epoch 21/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4689 - auc: 0.8554 - val_loss: 0.4832 - val_auc: 0.8457\n",
      "Epoch 22/250\n",
      "21000/21000 [==============================] - 45s 2ms/step - loss: 0.4677 - auc: 0.8562 - val_loss: 0.4839 - val_auc: 0.8460\n",
      "Epoch 23/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4666 - auc: 0.8570 - val_loss: 0.4814 - val_auc: 0.8467\n",
      "Epoch 24/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4654 - auc: 0.8578 - val_loss: 0.4850 - val_auc: 0.8457\n",
      "Epoch 25/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4644 - auc: 0.8585 - val_loss: 0.4814 - val_auc: 0.8467\n",
      "Epoch 26/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4633 - auc: 0.8592 - val_loss: 0.4821 - val_auc: 0.8464\n",
      "Epoch 27/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4622 - auc: 0.8599 - val_loss: 0.4835 - val_auc: 0.8454\n",
      "Epoch 28/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4613 - auc: 0.8606 - val_loss: 0.4843 - val_auc: 0.8452\n",
      "Epoch 29/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4601 - auc: 0.8614 - val_loss: 0.4832 - val_auc: 0.8462\n",
      "Epoch 30/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4592 - auc: 0.8620 - val_loss: 0.4842 - val_auc: 0.8454\n",
      "Epoch 31/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4581 - auc: 0.8627 - val_loss: 0.4854 - val_auc: 0.8448\n",
      "Epoch 32/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4571 - auc: 0.8634 - val_loss: 0.4849 - val_auc: 0.8452\n",
      "Epoch 33/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4561 - auc: 0.8641 - val_loss: 0.4849 - val_auc: 0.8450\n",
      "Epoch 34/250\n",
      "21000/21000 [==============================] - 45s 2ms/step - loss: 0.4552 - auc: 0.8647 - val_loss: 0.4858 - val_auc: 0.8458\n",
      "Epoch 35/250\n",
      "21000/21000 [==============================] - 46s 2ms/step - loss: 0.4541 - auc: 0.8654 - val_loss: 0.4860 - val_auc: 0.8445\n",
      "Epoch 36/250\n",
      "21000/21000 [==============================] - 51s 2ms/step - loss: 0.4531 - auc: 0.8660 - val_loss: 0.4852 - val_auc: 0.8451\n",
      "Epoch 37/250\n",
      "21000/21000 [==============================] - 54s 3ms/step - loss: 0.4521 - auc: 0.8667 - val_loss: 0.4855 - val_auc: 0.8445\n",
      "Epoch 38/250\n",
      "21000/21000 [==============================] - 57s 3ms/step - loss: 0.4511 - auc: 0.8674 - val_loss: 0.4862 - val_auc: 0.8449\n",
      "Epoch 39/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.4501 - auc: 0.8680 - val_loss: 0.4863 - val_auc: 0.8443\n",
      "Epoch 40/250\n",
      "21000/21000 [==============================] - 51s 2ms/step - loss: 0.4490 - auc: 0.8688 - val_loss: 0.4871 - val_auc: 0.8431\n",
      "Epoch 41/250\n",
      "21000/21000 [==============================] - 49s 2ms/step - loss: 0.4479 - auc: 0.8695 - val_loss: 0.4900 - val_auc: 0.8438\n",
      "Epoch 42/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.4470 - auc: 0.8700 - val_loss: 0.4890 - val_auc: 0.8429\n",
      "Epoch 43/250\n",
      "21000/21000 [==============================] - 60s 3ms/step - loss: 0.4460 - auc: 0.8707 - val_loss: 0.4928 - val_auc: 0.8423\n",
      "Epoch 44/250\n",
      "21000/21000 [==============================] - 62s 3ms/step - loss: 0.4449 - auc: 0.8714 - val_loss: 0.4909 - val_auc: 0.8427\n",
      "Epoch 45/250\n",
      "21000/21000 [==============================] - 56s 3ms/step - loss: 0.4439 - auc: 0.8721 - val_loss: 0.4920 - val_auc: 0.8421\n",
      "Epoch 46/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.4429 - auc: 0.8727 - val_loss: 0.4902 - val_auc: 0.8420\n",
      "Epoch 47/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4418 - auc: 0.8734 - val_loss: 0.4949 - val_auc: 0.8408\n",
      "Epoch 48/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4408 - auc: 0.8740 - val_loss: 0.4933 - val_auc: 0.8421\n",
      "Epoch 49/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4397 - auc: 0.8747 - val_loss: 0.4944 - val_auc: 0.8410\n",
      "Epoch 50/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4387 - auc: 0.8753 - val_loss: 0.4957 - val_auc: 0.8406\n",
      "Epoch 51/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4376 - auc: 0.8760 - val_loss: 0.4950 - val_auc: 0.8405\n",
      "Epoch 52/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4367 - auc: 0.8766 - val_loss: 0.4971 - val_auc: 0.8396\n",
      "Epoch 53/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4356 - auc: 0.8773 - val_loss: 0.4999 - val_auc: 0.8392\n",
      "Epoch 54/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4345 - auc: 0.8780 - val_loss: 0.4982 - val_auc: 0.8388\n",
      "Epoch 55/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4336 - auc: 0.8786 - val_loss: 0.5012 - val_auc: 0.8371\n",
      "Epoch 56/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4325 - auc: 0.8792 - val_loss: 0.5006 - val_auc: 0.8382\n",
      "Epoch 57/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4315 - auc: 0.8798 - val_loss: 0.5013 - val_auc: 0.8378\n",
      "Epoch 58/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4304 - auc: 0.8805 - val_loss: 0.5064 - val_auc: 0.8364\n",
      "Epoch 59/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4293 - auc: 0.8812 - val_loss: 0.5038 - val_auc: 0.8371\n",
      "Epoch 60/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4283 - auc: 0.8818 - val_loss: 0.5075 - val_auc: 0.8342\n",
      "Epoch 61/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4272 - auc: 0.8825 - val_loss: 0.5056 - val_auc: 0.8358\n",
      "Epoch 62/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4262 - auc: 0.8831 - val_loss: 0.5085 - val_auc: 0.8338\n",
      "Epoch 63/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4252 - auc: 0.8836 - val_loss: 0.5093 - val_auc: 0.8354\n",
      "Epoch 64/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4242 - auc: 0.8843 - val_loss: 0.5124 - val_auc: 0.8335\n",
      "Epoch 65/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4232 - auc: 0.8849 - val_loss: 0.5103 - val_auc: 0.8344\n",
      "Epoch 66/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4222 - auc: 0.8855 - val_loss: 0.5115 - val_auc: 0.8346\n",
      "Epoch 67/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4210 - auc: 0.8861 - val_loss: 0.5113 - val_auc: 0.8334\n",
      "Epoch 68/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4202 - auc: 0.8866 - val_loss: 0.5168 - val_auc: 0.8325\n",
      "Epoch 69/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4191 - auc: 0.8872 - val_loss: 0.5146 - val_auc: 0.8320\n",
      "Epoch 70/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4182 - auc: 0.8878 - val_loss: 0.5183 - val_auc: 0.8297\n",
      "Epoch 71/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4172 - auc: 0.8884 - val_loss: 0.5170 - val_auc: 0.8315\n",
      "Epoch 72/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4162 - auc: 0.8890 - val_loss: 0.5189 - val_auc: 0.8307\n",
      "Epoch 73/250\n",
      "21000/21000 [==============================] - 49s 2ms/step - loss: 0.4152 - auc: 0.8895 - val_loss: 0.5210 - val_auc: 0.8311\n",
      "Epoch 74/250\n",
      "21000/21000 [==============================] - 47s 2ms/step - loss: 0.4144 - auc: 0.8900 - val_loss: 0.5227 - val_auc: 0.8303\n",
      "Epoch 75/250\n",
      "21000/21000 [==============================] - 48s 2ms/step - loss: 0.4133 - auc: 0.8907 - val_loss: 0.5241 - val_auc: 0.8293\n",
      "Epoch 76/250\n",
      "21000/21000 [==============================] - 45s 2ms/step - loss: 0.4125 - auc: 0.8911 - val_loss: 0.5237 - val_auc: 0.8292\n",
      "Epoch 77/250\n",
      "21000/21000 [==============================] - 44s 2ms/step - loss: 0.4115 - auc: 0.8917 - val_loss: 0.5259 - val_auc: 0.8281\n",
      "Epoch 78/250\n",
      "21000/21000 [==============================] - 44s 2ms/step - loss: 0.4106 - auc: 0.8922 - val_loss: 0.5268 - val_auc: 0.8290\n",
      "Epoch 79/250\n",
      "21000/21000 [==============================] - 44s 2ms/step - loss: 0.4097 - auc: 0.8927 - val_loss: 0.5258 - val_auc: 0.8279\n",
      "Epoch 80/250\n",
      "21000/21000 [==============================] - 45s 2ms/step - loss: 0.4088 - auc: 0.8932 - val_loss: 0.5280 - val_auc: 0.8260\n",
      "Epoch 81/250\n",
      "21000/21000 [==============================] - 45s 2ms/step - loss: 0.4079 - auc: 0.8937 - val_loss: 0.5297 - val_auc: 0.8276\n",
      "Epoch 82/250\n",
      "21000/21000 [==============================] - 85s 4ms/step - loss: 0.4071 - auc: 0.8942 - val_loss: 0.5315 - val_auc: 0.8277\n",
      "Epoch 83/250\n",
      "21000/21000 [==============================] - 97s 5ms/step - loss: 0.4062 - auc: 0.8947 - val_loss: 0.5303 - val_auc: 0.8259\n",
      "Epoch 84/250\n",
      "21000/21000 [==============================] - 102s 5ms/step - loss: 0.4054 - auc: 0.8951 - val_loss: 0.5334 - val_auc: 0.8264\n",
      "Epoch 85/250\n",
      "21000/21000 [==============================] - 107s 5ms/step - loss: 0.4046 - auc: 0.8956 - val_loss: 0.5339 - val_auc: 0.8257\n",
      "Epoch 86/250\n",
      "21000/21000 [==============================] - 106s 5ms/step - loss: 0.4038 - auc: 0.8961 - val_loss: 0.5338 - val_auc: 0.8251\n",
      "Epoch 87/250\n",
      "21000/21000 [==============================] - 128s 6ms/step - loss: 0.4029 - auc: 0.8966 - val_loss: 0.5379 - val_auc: 0.8245\n",
      "Epoch 88/250\n",
      "21000/21000 [==============================] - 134s 6ms/step - loss: 0.4019 - auc: 0.8970 - val_loss: 0.5371 - val_auc: 0.8229\n",
      "Epoch 89/250\n",
      "21000/21000 [==============================] - 134s 6ms/step - loss: 0.4013 - auc: 0.8974 - val_loss: 0.5386 - val_auc: 0.8250\n",
      "Epoch 90/250\n",
      "21000/21000 [==============================] - 132s 6ms/step - loss: 0.4006 - auc: 0.8978 - val_loss: 0.5405 - val_auc: 0.8248\n",
      "Epoch 91/250\n",
      "21000/21000 [==============================] - 144s 7ms/step - loss: 0.3998 - auc: 0.8983 - val_loss: 0.5417 - val_auc: 0.8241\n",
      "Epoch 92/250\n",
      "21000/21000 [==============================] - 141s 7ms/step - loss: 0.3991 - auc: 0.8986 - val_loss: 0.5411 - val_auc: 0.8219\n",
      "Epoch 93/250\n",
      "21000/21000 [==============================] - 119s 6ms/step - loss: 0.3983 - auc: 0.8991 - val_loss: 0.5433 - val_auc: 0.8229\n",
      "Epoch 94/250\n",
      "21000/21000 [==============================] - 82s 4ms/step - loss: 0.3975 - auc: 0.8995 - val_loss: 0.5447 - val_auc: 0.8206\n",
      "Epoch 95/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.3968 - auc: 0.8998 - val_loss: 0.5450 - val_auc: 0.8216\n",
      "Epoch 96/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.3961 - auc: 0.9002 - val_loss: 0.5478 - val_auc: 0.8218\n",
      "Epoch 97/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.3954 - auc: 0.9007 - val_loss: 0.5495 - val_auc: 0.8221\n",
      "Epoch 98/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.3948 - auc: 0.9010 - val_loss: 0.5492 - val_auc: 0.8221\n",
      "Epoch 99/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.3942 - auc: 0.9013 - val_loss: 0.5479 - val_auc: 0.8213\n",
      "Epoch 100/250\n",
      "21000/21000 [==============================] - 51s 2ms/step - loss: 0.3934 - auc: 0.9017 - val_loss: 0.5511 - val_auc: 0.8198\n",
      "Epoch 101/250\n",
      "21000/21000 [==============================] - 51s 2ms/step - loss: 0.3928 - auc: 0.9020 - val_loss: 0.5522 - val_auc: 0.8206\n",
      "Epoch 102/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.3922 - auc: 0.9024 - val_loss: 0.5530 - val_auc: 0.8186\n",
      "Epoch 103/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.3916 - auc: 0.9027 - val_loss: 0.5553 - val_auc: 0.8189\n",
      "Epoch 104/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.3909 - auc: 0.9031 - val_loss: 0.5539 - val_auc: 0.8200\n",
      "Epoch 105/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.3904 - auc: 0.9033 - val_loss: 0.5566 - val_auc: 0.8188\n",
      "Epoch 106/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.3897 - auc: 0.9037 - val_loss: 0.5567 - val_auc: 0.8199\n",
      "Epoch 107/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.3892 - auc: 0.9040 - val_loss: 0.5564 - val_auc: 0.8169\n",
      "Epoch 108/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.3885 - auc: 0.9043 - val_loss: 0.5588 - val_auc: 0.8181\n",
      "Epoch 109/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.3880 - auc: 0.9046 - val_loss: 0.5578 - val_auc: 0.8170\n",
      "Epoch 110/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.3875 - auc: 0.9049 - val_loss: 0.5605 - val_auc: 0.8177\n",
      "Epoch 111/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.3869 - auc: 0.9052 - val_loss: 0.5656 - val_auc: 0.8169\n",
      "Epoch 112/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.3865 - auc: 0.9054 - val_loss: 0.5616 - val_auc: 0.8176\n",
      "Epoch 113/250\n",
      "21000/21000 [==============================] - 53s 3ms/step - loss: 0.3859 - auc: 0.9057 - val_loss: 0.5621 - val_auc: 0.8162\n",
      "Epoch 114/250\n",
      "21000/21000 [==============================] - 52s 2ms/step - loss: 0.3855 - auc: 0.9059 - val_loss: 0.5626 - val_auc: 0.8165\n",
      "Epoch 115/250\n",
      "21000/21000 [==============================] - 45s 2ms/step - loss: 0.3849 - auc: 0.9062 - val_loss: 0.5651 - val_auc: 0.8171\n",
      "Epoch 116/250\n",
      "21000/21000 [==============================] - 43s 2ms/step - loss: 0.3845 - auc: 0.9064 - val_loss: 0.5648 - val_auc: 0.8167\n",
      "Epoch 117/250\n",
      "20987/21000 [============================>.] - ETA: 0s - loss: 0.3841 - auc: 0.9066"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m               return_dict=True)\n\u001b[0m\u001b[0;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1382\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \"\"\"\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_test_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m           \u001b[0mnumpy_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train, y_train, epochs=250, validation_data=(x_test,y_test), batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
