{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Week 12 Higgs Boson Case Study\n",
    "MSDS 7333 Quantifying the World  \n",
    "*Allison Roderick, Jenna Ford, and Will Arnost* \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<a href='#Section_1'> 1. Introduction </a>  \n",
    "<a href='#Section_2'> 2. Question </a>  \n",
    "<a href='#Section_3'> 3. Methods </a>  \n",
    "<a href='#Section_3_a'> &nbsp;&nbsp;&nbsp; a. Dataset </a>  \n",
    "<a href='#Section_3_b'> &nbsp;&nbsp;&nbsp; b. Neural Network Structure </a>  \n",
    "<a href='#Section_3_c'> &nbsp;&nbsp;&nbsp; c. Other Considerations </a>  \n",
    "<a href='#Section_4'> 4. Modeling </a>  \n",
    "<a href='#Section_5'> 5. Results </a>  \n",
    "<a href='#Section_6'> 6. Conclusion </a>  \n",
    "<a href='#Section_7'> 7. References </a>  \n",
    "<a href='#Section_8'> 8. Code </a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's case study involves replicating results produces in the paper \"Searching for Exotic Particles in High-Energy Physics with Deep Learning\" by Baldi, Sadowski, and Whiteson [1]. The 2014 paper looks to distinguish between particle collisions that produce exotic particle and those that do not. The authors investigate the use of deep neural networks to improve accuracy over other methods. \n",
    "\n",
    "We will attempt to replicate that paper's neural network architecture and performance. The packages used in the paper are outdated, so we will be using tensorflow to build our network. We hope to get as close to their AUC of 0.88 as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following paper: https://arxiv.org/pdf/1402.4735.pdf\n",
    "\n",
    "Build a replica Neural Network with the paper’s architecture using Tensorflow. If possible begin to train on the data located here: https://archive.ics.uci.edu/ml/datasets/HIGGS. How close can you get to the original results?\n",
    "To facilitate quicker training you may increase the batch size temporarily (this has a small impact on final result, but can speed you calculations significantly). You do not need to train a final result using the paper’s parameters, only the code for your model is required in your final submission.\n",
    "\n",
    "Include in your report:\n",
    "Based on the class notes and discussion suggest improvements to the procedure. What are standard practices now versus when this paper was written? What kind of improvements do they provide?\n",
    "How would you quantify if your result duplicated the paper’s?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section gives an overview of what we know about the data and how we prepared the dataset for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_3_a'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 11 million observations and 29 columns. The target column indicates if the collision produced exotic particles. The remaining 28 columns are numeric. There are no missing values in the data.\n",
    "\n",
    "To replicate the methods of the paper, we will use a sample of 2.6M records for model training and 100K records for validation. We do not know exactly which records they used, this will be one difference between our results and theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_3_b'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Neural Network Stucture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_3_c'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replicate the study, we want to create a network with the following structure:\n",
    "- 5 densly connected layers\n",
    " - 4 layers with 300 hidden units and tanh activation functions\n",
    " - 1 output layer with a linear activation function\n",
    "- learning rate of 0.05\n",
    "- weight decay coefficient of 1 × 10−5\n",
    " - The learning rate decayed by a factor of 1.0000002 every batch update until it reached a minimum of 10−6\n",
    "- We used a normal distribution with mean zero to initialize the weights. The standard deviation was set to 0.1 for the first layer, 0.001 for the output layer, and 0.05 for layers 2-4.\n",
    "- Mini-Batch sizes of 100\n",
    "- <!>A momentum term increased linearly over the first 200 epochs from 0.9 to 0.99, at which point it remained constant\n",
    "- <!>Training ended when the momentum had reached its maximum value and the minimum error on the validation set (500,000 examples) had not decreased by more than a factor of 0.00001 over 10 epochs.his early stopping prevented overfitting and resulted in each neural network being trained for 200-1000 epochs\n",
    "- <!> Autoencoder pretraining was performed by training a stack of single-hidden-layer autoencoder networks as in [9], then fine-tuning the full network using the class labels. Each autoencoder in the stack used tanh hidden units and linear outputs, and was trained with the same initialization scheme, learning algorithm, and stopping parameters as in the fine-tuning stage. When training with dropout, we increased the learning rate decay factor to 1.0000003, and only ended training when the momentum had reached its maximum value and the error on the validation set had not decreased for 40 epochs.\n",
    "- <!> Input features were standardized over the entire train/test set with mean zero and standard deviation one, except for those features with values strictly greater than zero – these we scaled so that the mean value was one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow is much newer than the Theano library used in the original paper. Methods for training neural networks have also changed since the paper was written in 2014. We are not able to replicate all aspects of the original network and explain those instances here.\n",
    "\n",
    "We mentioned earlier that from the 11M records in the dataset, we are not certain which ones were used by the paper's authors. If all columns in our sample have a distribution similar to the sample the authors used, we would expect the difference to be small. If they are not, the difference will be larger.\n",
    "\n",
    "<!> The paper indicates the output layer used a linear activation function. Tensorflow does not recognize this as a valid option for classification problems and throws an error <!error>. \n",
    "\n",
    "We did not use pretraining with an autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://arxiv.org/pdf/1402.4735.pdf Searching for Exotic Particles in High-Energy Physics with Deep Learning by Baldi, Sadowski, and Whiteson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip the File and Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-43511b12d109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_out\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML7331\\lib\\gzip.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"write\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ML7331\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv.gz'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with gzip.open('C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv.gz', 'rb') as f_in:\n",
    "    with open('C:\\\\Users\\\\allro\\\\JupyterNotebook\\\\QTW\\\\Data\\\\HIGGS.csv', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://archive.ics.uci.edu/ml/datasets/HIGGS#\n",
    "#df = pd.read_csv(\"./Data/HIGGS.csv\", header=None)\n",
    "df = pd.read_csv(\"../../HIGGS.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column names\n",
    "df.columns =['target', 'lepton_ph', 'lepton_eta', 'lepton_phi','missing_energy_magnitude','missing_energy_phi',\n",
    "             'jet_1_pt','jet_1_eta','jet_1_phi','jet_1_btag','jet_2_pt','jet_2_eta','jet_2_phi','jet_2_btag',\n",
    "             'jet_3_pt','jet_3_eta','jet_3_phi','jet_3_btag','jet_4_pt','jet_4_eta','jet_4_phi','jet_4_btag',\n",
    "             'm_jj','m_jjj','m_lv','m_jlv','m_bb','m_wbb','m_wwbb'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000000 entries, 0 to 10999999\n",
      "Data columns (total 29 columns):\n",
      "target                      float64\n",
      "lepton_ph                   float64\n",
      "lepton_eta                  float64\n",
      "lepton_phi                  float64\n",
      "missing_energy_magnitude    float64\n",
      "missing_energy_phi          float64\n",
      "jet_1_pt                    float64\n",
      "jet_1_eta                   float64\n",
      "jet_1_phi                   float64\n",
      "jet_1_btag                  float64\n",
      "jet_2_pt                    float64\n",
      "jet_2_eta                   float64\n",
      "jet_2_phi                   float64\n",
      "jet_2_btag                  float64\n",
      "jet_3_pt                    float64\n",
      "jet_3_eta                   float64\n",
      "jet_3_phi                   float64\n",
      "jet_3_btag                  float64\n",
      "jet_4_pt                    float64\n",
      "jet_4_eta                   float64\n",
      "jet_4_phi                   float64\n",
      "jet_4_btag                  float64\n",
      "m_jj                        float64\n",
      "m_jjj                       float64\n",
      "m_lv                        float64\n",
      "m_jlv                       float64\n",
      "m_bb                        float64\n",
      "m_wbb                       float64\n",
      "m_wwbb                      float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 2.4 GB\n"
     ]
    }
   ],
   "source": [
    "# Print out the data types\n",
    "df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  lepton_ph  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0     1.0   0.869293   -0.635082    0.225690                  0.327470   \n",
       "1     1.0   0.907542    0.329147    0.359412                  1.497970   \n",
       "2     1.0   0.798835    1.470639   -1.635975                  0.453773   \n",
       "3     0.0   1.344385   -0.876626    0.935913                  1.992050   \n",
       "4     1.0   1.105009    0.321356    1.522401                  0.882808   \n",
       "\n",
       "   missing_energy_phi  jet_1_pt  jet_1_eta  jet_1_phi  jet_1_btag  ...  \\\n",
       "0           -0.689993  0.754202  -0.248573  -1.092064    0.000000  ...   \n",
       "1           -0.313010  1.095531  -0.557525  -1.588230    2.173076  ...   \n",
       "2            0.425629  1.104875   1.282322   1.381664    0.000000  ...   \n",
       "3            0.882454  1.786066  -1.646778  -0.942383    0.000000  ...   \n",
       "4           -1.205349  0.681466  -1.070464  -0.921871    0.000000  ...   \n",
       "\n",
       "   jet_4_eta  jet_4_phi  jet_4_btag      m_jj     m_jjj      m_lv     m_jlv  \\\n",
       "0  -0.010455  -0.045767    3.101961  1.353760  0.979563  0.978076  0.920005   \n",
       "1  -1.138930  -0.000819    0.000000  0.302220  0.833048  0.985700  0.978098   \n",
       "2   1.128848   0.900461    0.000000  0.909753  1.108330  0.985692  0.951331   \n",
       "3  -0.678379  -1.360356    0.000000  0.946652  1.028704  0.998656  0.728281   \n",
       "4  -0.373566   0.113041    0.000000  0.755856  1.361057  0.986610  0.838085   \n",
       "\n",
       "       m_bb     m_wbb    m_wwbb  \n",
       "0  0.721657  0.988751  0.876678  \n",
       "1  0.779732  0.992356  0.798343  \n",
       "2  0.803252  0.865924  0.780118  \n",
       "3  0.869200  1.026736  0.957904  \n",
       "4  1.133295  0.872245  0.808487  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample the data, 2.6million records for train and 100k for test\n",
    "#train = df.sample(n=2700000, random_state=123)\n",
    "train = df.loc[1:2600000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[10900000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_btag</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_2_eta</th>\n",
       "      <th>jet_2_phi</th>\n",
       "      <th>jet_2_btag</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>jet_3_eta</th>\n",
       "      <th>jet_3_phi</th>\n",
       "      <th>jet_3_btag</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_btag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.527920</td>\n",
       "      <td>0.990813</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>1.001417</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.991479</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>-0.003438</td>\n",
       "      <td>1.000691</td>\n",
       "      <td>0.992980</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>-0.002011</td>\n",
       "      <td>0.998564</td>\n",
       "      <td>0.992009</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>1.001287</td>\n",
       "      <td>0.984663</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>-0.002528</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>1.030947</td>\n",
       "      <td>1.022480</td>\n",
       "      <td>1.049953</td>\n",
       "      <td>1.008384</td>\n",
       "      <td>0.972178</td>\n",
       "      <td>1.031611</td>\n",
       "      <td>0.958529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499222</td>\n",
       "      <td>0.565247</td>\n",
       "      <td>1.008622</td>\n",
       "      <td>1.003933</td>\n",
       "      <td>0.602900</td>\n",
       "      <td>1.004503</td>\n",
       "      <td>0.477187</td>\n",
       "      <td>1.008778</td>\n",
       "      <td>1.007950</td>\n",
       "      <td>1.027084</td>\n",
       "      <td>0.500264</td>\n",
       "      <td>1.008215</td>\n",
       "      <td>1.007988</td>\n",
       "      <td>1.048803</td>\n",
       "      <td>0.489483</td>\n",
       "      <td>1.009223</td>\n",
       "      <td>1.005359</td>\n",
       "      <td>1.194155</td>\n",
       "      <td>0.505068</td>\n",
       "      <td>1.008343</td>\n",
       "      <td>1.006153</td>\n",
       "      <td>1.400944</td>\n",
       "      <td>0.667706</td>\n",
       "      <td>0.375855</td>\n",
       "      <td>0.161874</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>0.527358</td>\n",
       "      <td>0.364740</td>\n",
       "      <td>0.312684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274697</td>\n",
       "      <td>-2.434002</td>\n",
       "      <td>-1.742508</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>-1.743927</td>\n",
       "      <td>0.147396</td>\n",
       "      <td>-2.968735</td>\n",
       "      <td>-1.741237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189107</td>\n",
       "      <td>-2.913090</td>\n",
       "      <td>-1.742372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263608</td>\n",
       "      <td>-2.728753</td>\n",
       "      <td>-1.742069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365354</td>\n",
       "      <td>-2.497265</td>\n",
       "      <td>-1.742691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117654</td>\n",
       "      <td>0.323551</td>\n",
       "      <td>0.154012</td>\n",
       "      <td>0.187605</td>\n",
       "      <td>0.066506</td>\n",
       "      <td>0.369259</td>\n",
       "      <td>0.398860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590204</td>\n",
       "      <td>-0.740270</td>\n",
       "      <td>-0.864718</td>\n",
       "      <td>0.578815</td>\n",
       "      <td>-0.871730</td>\n",
       "      <td>0.677619</td>\n",
       "      <td>-0.686255</td>\n",
       "      <td>-0.875857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.656839</td>\n",
       "      <td>-0.694715</td>\n",
       "      <td>-0.875173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650150</td>\n",
       "      <td>-0.698898</td>\n",
       "      <td>-0.870025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.615820</td>\n",
       "      <td>-0.716689</td>\n",
       "      <td>-0.875363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789978</td>\n",
       "      <td>0.845503</td>\n",
       "      <td>0.985733</td>\n",
       "      <td>0.768924</td>\n",
       "      <td>0.671727</td>\n",
       "      <td>0.819015</td>\n",
       "      <td>0.769170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>-0.003570</td>\n",
       "      <td>0.892142</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.894132</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>-0.007703</td>\n",
       "      <td>1.086538</td>\n",
       "      <td>0.890264</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.895844</td>\n",
       "      <td>-0.001648</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867503</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>-0.006646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894538</td>\n",
       "      <td>0.949578</td>\n",
       "      <td>0.989797</td>\n",
       "      <td>0.914493</td>\n",
       "      <td>0.872124</td>\n",
       "      <td>0.946225</td>\n",
       "      <td>0.870942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.237141</td>\n",
       "      <td>0.738214</td>\n",
       "      <td>0.866555</td>\n",
       "      <td>1.297944</td>\n",
       "      <td>0.864816</td>\n",
       "      <td>1.172114</td>\n",
       "      <td>0.686204</td>\n",
       "      <td>0.868313</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>1.202504</td>\n",
       "      <td>0.694592</td>\n",
       "      <td>0.872647</td>\n",
       "      <td>2.214872</td>\n",
       "      <td>1.223203</td>\n",
       "      <td>0.699244</td>\n",
       "      <td>0.870840</td>\n",
       "      <td>2.548224</td>\n",
       "      <td>1.218740</td>\n",
       "      <td>0.709938</td>\n",
       "      <td>0.867721</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.024222</td>\n",
       "      <td>1.081825</td>\n",
       "      <td>1.021190</td>\n",
       "      <td>1.138317</td>\n",
       "      <td>1.136823</td>\n",
       "      <td>1.139043</td>\n",
       "      <td>1.057087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.184022</td>\n",
       "      <td>2.434868</td>\n",
       "      <td>1.743236</td>\n",
       "      <td>8.273911</td>\n",
       "      <td>1.743257</td>\n",
       "      <td>8.577184</td>\n",
       "      <td>2.967694</td>\n",
       "      <td>1.741454</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>7.938468</td>\n",
       "      <td>2.913210</td>\n",
       "      <td>1.743175</td>\n",
       "      <td>2.214872</td>\n",
       "      <td>9.598233</td>\n",
       "      <td>2.730009</td>\n",
       "      <td>1.742884</td>\n",
       "      <td>2.548224</td>\n",
       "      <td>7.509841</td>\n",
       "      <td>2.498009</td>\n",
       "      <td>1.743372</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>23.905035</td>\n",
       "      <td>12.174695</td>\n",
       "      <td>4.156842</td>\n",
       "      <td>7.551758</td>\n",
       "      <td>9.266160</td>\n",
       "      <td>6.802347</td>\n",
       "      <td>5.974743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              target      lepton_ph     lepton_eta     lepton_phi  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.527920       0.990813       0.001182       0.001023   \n",
       "std         0.499222       0.565247       1.008622       1.003933   \n",
       "min         0.000000       0.274697      -2.434002      -1.742508   \n",
       "25%         0.000000       0.590204      -0.740270      -0.864718   \n",
       "50%         1.000000       0.852273       0.003842      -0.003570   \n",
       "75%         1.000000       1.237141       0.738214       0.866555   \n",
       "max         1.000000       7.184022       2.434868       1.743236   \n",
       "\n",
       "       missing_energy_magnitude  missing_energy_phi       jet_1_pt  \\\n",
       "count             100000.000000       100000.000000  100000.000000   \n",
       "mean                   1.001417           -0.000091       0.991479   \n",
       "std                    0.602900            1.004503       0.477187   \n",
       "min                    0.002749           -1.743927       0.147396   \n",
       "25%                    0.578815           -0.871730       0.677619   \n",
       "50%                    0.892142            0.004286       0.894132   \n",
       "75%                    1.297944            0.864816       1.172114   \n",
       "max                    8.273911            1.743257       8.577184   \n",
       "\n",
       "           jet_1_eta      jet_1_phi     jet_1_btag       jet_2_pt  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.000904      -0.003438       1.000691       0.992980   \n",
       "std         1.008778       1.007950       1.027084       0.500264   \n",
       "min        -2.968735      -1.741237       0.000000       0.189107   \n",
       "25%        -0.686255      -0.875857       0.000000       0.656839   \n",
       "50%         0.004926      -0.007703       1.086538       0.890264   \n",
       "75%         0.686204       0.868313       2.173076       1.202504   \n",
       "max         2.967694       1.741454       2.173076       7.938468   \n",
       "\n",
       "           jet_2_eta      jet_2_phi     jet_2_btag       jet_3_pt  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.000820      -0.002011       0.998564       0.992009   \n",
       "std         1.008215       1.007988       1.048803       0.489483   \n",
       "min        -2.913090      -1.742372       0.000000       0.263608   \n",
       "25%        -0.694715      -0.875173       0.000000       0.650150   \n",
       "50%         0.001032       0.003228       0.000000       0.895844   \n",
       "75%         0.694592       0.872647       2.214872       1.223203   \n",
       "max         2.913210       1.743175       2.214872       9.598233   \n",
       "\n",
       "           jet_3_eta      jet_3_phi     jet_3_btag       jet_4_pt  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.000257       0.002184       1.001287       0.984663   \n",
       "std         1.009223       1.005359       1.194155       0.505068   \n",
       "min        -2.728753      -1.742069       0.000000       0.365354   \n",
       "25%        -0.698898      -0.870025       0.000000       0.615820   \n",
       "50%        -0.001648       0.007117       0.000000       0.867503   \n",
       "75%         0.699244       0.870840       2.548224       1.218740   \n",
       "max         2.730009       1.742884       2.548224       7.509841   \n",
       "\n",
       "           jet_4_eta      jet_4_phi     jet_4_btag           m_jj  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       -0.001472      -0.002528       0.999623       1.030947   \n",
       "std         1.008343       1.006153       1.400944       0.667706   \n",
       "min        -2.497265      -1.742691       0.000000       0.117654   \n",
       "25%        -0.716689      -0.875363       0.000000       0.789978   \n",
       "50%         0.003703      -0.006646       0.000000       0.894538   \n",
       "75%         0.709938       0.867721       3.101961       1.024222   \n",
       "max         2.498009       1.743372       3.101961      23.905035   \n",
       "\n",
       "               m_jjj           m_lv          m_jlv           m_bb  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        1.022480       1.049953       1.008384       0.972178   \n",
       "std         0.375855       0.161874       0.395813       0.527358   \n",
       "min         0.323551       0.154012       0.187605       0.066506   \n",
       "25%         0.845503       0.985733       0.768924       0.671727   \n",
       "50%         0.949578       0.989797       0.914493       0.872124   \n",
       "75%         1.081825       1.021190       1.138317       1.136823   \n",
       "max        12.174695       4.156842       7.551758       9.266160   \n",
       "\n",
       "               m_wbb         m_wwbb  \n",
       "count  100000.000000  100000.000000  \n",
       "mean        1.031611       0.958529  \n",
       "std         0.364740       0.312684  \n",
       "min         0.369259       0.398860  \n",
       "25%         0.819015       0.769170  \n",
       "50%         0.946225       0.870942  \n",
       "75%         1.139043       1.057087  \n",
       "max         6.802347       5.974743  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split out target and explanatory variables\n",
    "y = train['target']\n",
    "pre_X = train.loc[:, df.columns != 'target']\n",
    "\n",
    "y1 = test['target']\n",
    "pre_X1 = test.loc[:, df.columns != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_ph</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_2_pt</th>\n",
       "      <th>jet_4_pt</th>\n",
       "      <th>jet_3_pt</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>1.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.266914</td>\n",
       "      <td>-0.656449</td>\n",
       "      <td>-0.768883</td>\n",
       "      <td>-0.606904</td>\n",
       "      <td>-0.226194</td>\n",
       "      <td>-0.488111</td>\n",
       "      <td>-0.367815</td>\n",
       "      <td>-0.859579</td>\n",
       "      <td>-4.534849</td>\n",
       "      <td>-1.073660</td>\n",
       "      <td>-0.717385</td>\n",
       "      <td>-0.815965</td>\n",
       "      <td>-0.789896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.291265</td>\n",
       "      <td>0.299047</td>\n",
       "      <td>0.342265</td>\n",
       "      <td>0.328070</td>\n",
       "      <td>0.269714</td>\n",
       "      <td>0.301588</td>\n",
       "      <td>0.639107</td>\n",
       "      <td>0.529133</td>\n",
       "      <td>0.603268</td>\n",
       "      <td>0.395017</td>\n",
       "      <td>0.430269</td>\n",
       "      <td>0.417127</td>\n",
       "      <td>0.394403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.754904</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>0.795997</td>\n",
       "      <td>0.794675</td>\n",
       "      <td>0.768031</td>\n",
       "      <td>0.803537</td>\n",
       "      <td>0.795704</td>\n",
       "      <td>0.806036</td>\n",
       "      <td>0.628376</td>\n",
       "      <td>0.762789</td>\n",
       "      <td>0.810272</td>\n",
       "      <td>0.765897</td>\n",
       "      <td>0.719884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.435791</td>\n",
       "      <td>1.491836</td>\n",
       "      <td>1.378543</td>\n",
       "      <td>1.418830</td>\n",
       "      <td>1.463459</td>\n",
       "      <td>1.472324</td>\n",
       "      <td>0.989928</td>\n",
       "      <td>1.157894</td>\n",
       "      <td>0.822313</td>\n",
       "      <td>1.328271</td>\n",
       "      <td>1.312210</td>\n",
       "      <td>1.294544</td>\n",
       "      <td>1.315203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.956689</td>\n",
       "      <td>13.062579</td>\n",
       "      <td>16.896800</td>\n",
       "      <td>14.883706</td>\n",
       "      <td>13.919465</td>\n",
       "      <td>18.582360</td>\n",
       "      <td>35.257918</td>\n",
       "      <td>30.671740</td>\n",
       "      <td>20.193411</td>\n",
       "      <td>17.531545</td>\n",
       "      <td>16.727507</td>\n",
       "      <td>16.821565</td>\n",
       "      <td>17.042534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lepton_ph  missing_energy_magnitude       jet_1_pt       jet_2_pt  \\\n",
       "count  100000.000000             100000.000000  100000.000000  100000.000000   \n",
       "mean        1.000000                  1.000000       1.000000       1.000000   \n",
       "std         1.000005                  1.000005       1.000005       1.000005   \n",
       "min        -0.266914                 -0.656449      -0.768883      -0.606904   \n",
       "25%         0.291265                  0.299047       0.342265       0.328070   \n",
       "50%         0.754904                  0.818750       0.795997       0.794675   \n",
       "75%         1.435791                  1.491836       1.378543       1.418830   \n",
       "max        11.956689                 13.062579      16.896800      14.883706   \n",
       "\n",
       "            jet_4_pt       jet_3_pt           m_jj          m_jjj  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        1.000000       1.000000       1.000000       1.000000   \n",
       "std         1.000005       1.000005       1.000005       1.000005   \n",
       "min        -0.226194      -0.488111      -0.367815      -0.859579   \n",
       "25%         0.269714       0.301588       0.639107       0.529133   \n",
       "50%         0.768031       0.803537       0.795704       0.806036   \n",
       "75%         1.463459       1.472324       0.989928       1.157894   \n",
       "max        13.919465      18.582360      35.257918      30.671740   \n",
       "\n",
       "                m_lv          m_jlv           m_bb          m_wbb  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        1.000000       1.000000       1.000000       1.000000   \n",
       "std         1.000005       1.000005       1.000005       1.000005   \n",
       "min        -4.534849      -1.073660      -0.717385      -0.815965   \n",
       "25%         0.603268       0.395017       0.430269       0.417127   \n",
       "50%         0.628376       0.762789       0.810272       0.765897   \n",
       "75%         0.822313       1.328271       1.312210       1.294544   \n",
       "max        20.193411      17.531545      16.727507      16.821565   \n",
       "\n",
       "              m_wwbb  \n",
       "count  100000.000000  \n",
       "mean        1.000000  \n",
       "std         1.000005  \n",
       "min        -0.789896  \n",
       "25%         0.394403  \n",
       "50%         0.719884  \n",
       "75%         1.315203  \n",
       "max        17.042534  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#NOT strictly greater than 0 columns:\n",
    "not_greater_than_0 = ['lepton_eta','lepton_phi','missing_energy_phi','jet_1_eta','jet_1_phi','jet_1_btag',\n",
    "                      'jet_2_eta','jet_2_phi','jet_2_btag','jet_3_eta','jet_3_phi','jet_3_btag','jet_4_eta',\n",
    "                      'jet_4_phi','jet_4_btag']\n",
    "\n",
    "# strictly greater than 0 columns:\n",
    "greater_than_0 = ['lepton_ph','missing_energy_magnitude','jet_1_pt','jet_2_pt','jet_4_pt','jet_3_pt','m_jj','m_jjj','m_lv',\n",
    "                  'm_jlv','m_bb','m_wbb','m_wwbb']\n",
    "\n",
    "\n",
    "# these columns scale where mean=0 and stdev=1\n",
    "to_scale1 = pre_X[not_greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train1 = scaler.fit_transform(to_scale1)\n",
    "scaled_train_df1 = pd.DataFrame(scaled_train1, columns=not_greater_than_0)\n",
    "\n",
    "# these columns scale where mean=1 and stdev=1\n",
    "to_scale2 = pre_X[greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train2 = scaler.fit_transform(to_scale2)\n",
    "scaled_train_df2 = pd.DataFrame(scaled_train2 + 1, columns=greater_than_0)\n",
    "scaled_train_df2.describe()\n",
    "\n",
    "\n",
    "# these columns scale where mean=0 and stdev=1\n",
    "to_scale11 = pre_X1[not_greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train11 = scaler.fit_transform(to_scale11)\n",
    "scaled_train_df11 = pd.DataFrame(scaled_train11, columns=not_greater_than_0)\n",
    "\n",
    "# these columns scale where mean=1 and stdev=1\n",
    "to_scale21 = pre_X1[greater_than_0]\n",
    "scaler = StandardScaler()\n",
    "scaled_train21 = scaler.fit_transform(to_scale21)\n",
    "scaled_train_df21 = pd.DataFrame(scaled_train21 + 1, columns=greater_than_0)\n",
    "scaled_train_df21.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# combine the separate scaled dataframes\n",
    "x_train = pd.concat([scaled_train_df1, scaled_train_df2], axis=1, sort=False)\n",
    "y_train = y\n",
    "\n",
    "x_test = pd.concat([scaled_train_df11, scaled_train_df21], axis=1, sort=False)\n",
    "y_test = y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all columns are numeric\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# input\n",
    "model.add(tf.keras.Input(shape=(28,)))\n",
    "# hidden\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.1)))  \n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(300,\n",
    "                       activation='tanh', \n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.05)))\n",
    "model.add(layers.Dense(1,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer=initializers.RandomNormal(stddev=0.001)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'SGD', 'learning_rate': 0.01, 'decay': 0.0, 'momentum': 0.9, 'nesterov': True}\n"
     ]
    }
   ],
   "source": [
    "#change learning rate and add stopping criteria\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# the learning rate decayed by a factor of 1.0000002 every batch update until it reached a minimum of 10−6\n",
    "def scheduler(epoch):\n",
    "    initial_lrate = 0.05\n",
    "    k = 1.0000002\n",
    "    lrate = initial_lrate * tf.math.exp(-k*epoch)\n",
    "    return max(lrate, 10e-6)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "#set early stopping criteria\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001 , patience=10)\n",
    "\n",
    "mom = tf.Variable(0.9)\n",
    "opt = tf.optimizers.SGD(momentum=mom, nesterov=True)\n",
    "print(opt.get_config())\n",
    "\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2600000 samples, validate on 100000 samples\n",
      "Epoch 1/250\n",
      "2600000/2600000 [==============================] - 190s 73us/sample - loss: 0.5395 - AUC: 0.8000 - val_loss: 0.5174 - val_AUC: 0.8213\n",
      "Epoch 2/250\n",
      "2600000/2600000 [==============================] - 191s 74us/sample - loss: 0.5005 - AUC: 0.8323 - val_loss: 0.5010 - val_AUC: 0.8341\n",
      "Epoch 3/250\n",
      "2600000/2600000 [==============================] - 185s 71us/sample - loss: 0.4890 - AUC: 0.8408 - val_loss: 0.4906 - val_AUC: 0.8401\n",
      "Epoch 4/250\n",
      "2600000/2600000 [==============================] - 207s 80us/sample - loss: 0.4835 - AUC: 0.8448 - val_loss: 0.4860 - val_AUC: 0.8433\n",
      "Epoch 5/250\n",
      "2600000/2600000 [==============================] - 182s 70us/sample - loss: 0.4809 - AUC: 0.8467 - val_loss: 0.4844 - val_AUC: 0.8443\n",
      "Epoch 6/250\n",
      "2600000/2600000 [==============================] - 184s 71us/sample - loss: 0.4798 - AUC: 0.8475 - val_loss: 0.4841 - val_AUC: 0.8445\n",
      "Epoch 7/250\n",
      "2600000/2600000 [==============================] - 190s 73us/sample - loss: 0.4793 - AUC: 0.8478 - val_loss: 0.4840 - val_AUC: 0.8447\n",
      "Epoch 8/250\n",
      "2600000/2600000 [==============================] - 199s 76us/sample - loss: 0.4791 - AUC: 0.8480 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 9/250\n",
      "2600000/2600000 [==============================] - 191s 73us/sample - loss: 0.4791 - AUC: 0.8480 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 10/250\n",
      "2600000/2600000 [==============================] - 194s 75us/sample - loss: 0.4790 - AUC: 0.8480 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 11/250\n",
      "2600000/2600000 [==============================] - 206s 79us/sample - loss: 0.4790 - AUC: 0.8480 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 12/250\n",
      "2600000/2600000 [==============================] - 186s 72us/sample - loss: 0.4790 - AUC: 0.8480 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 13/250\n",
      "2600000/2600000 [==============================] - 189s 73us/sample - loss: 0.4790 - AUC: 0.8480 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 14/250\n",
      "2600000/2600000 [==============================] - 194s 75us/sample - loss: 0.4790 - AUC: 0.8481 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 15/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4790 - AUC: 0.8481 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 16/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4790 - AUC: 0.8481 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 17/250\n",
      "2600000/2600000 [==============================] - 167s 64us/sample - loss: 0.4790 - AUC: 0.8481 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 18/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4790 - AUC: 0.8481 - val_loss: 0.4839 - val_AUC: 0.8447\n",
      "Epoch 19/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4790 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8447\n",
      "Epoch 20/250\n",
      "2600000/2600000 [==============================] - 170s 65us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 21/250\n",
      "2600000/2600000 [==============================] - 168s 65us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 22/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 23/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 24/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 25/250\n",
      "2600000/2600000 [==============================] - 167s 64us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 26/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 27/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 28/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 29/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 30/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 31/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4789 - AUC: 0.8481 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 32/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4789 - AUC: 0.8482 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 33/250\n",
      "2600000/2600000 [==============================] - 41s 16us/sample - loss: 0.4789 - AUC: 0.8482 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 34/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 35/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 36/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 37/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4838 - val_AUC: 0.8448\n",
      "Epoch 38/250\n",
      "2600000/2600000 [==============================] - 167s 64us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8448\n",
      "Epoch 39/250\n",
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8448\n",
      "Epoch 40/250\n",
      "2600000/2600000 [==============================] - 169s 65us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8448\n",
      "Epoch 41/250\n",
      "2600000/2600000 [==============================] - 182s 70us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8448\n",
      "Epoch 42/250\n",
      "2600000/2600000 [==============================] - 167s 64us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8448\n",
      "Epoch 43/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8448\n",
      "Epoch 44/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8448\n",
      "Epoch 45/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8448\n",
      "Epoch 46/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4788 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8448\n",
      "Epoch 47/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4787 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8449\n",
      "Epoch 48/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4787 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8449\n",
      "Epoch 49/250\n",
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4787 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8449\n",
      "Epoch 50/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4787 - AUC: 0.8482 - val_loss: 0.4837 - val_AUC: 0.8449\n",
      "Epoch 51/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4837 - val_AUC: 0.8449\n",
      "Epoch 52/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4837 - val_AUC: 0.8449\n",
      "Epoch 53/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4837 - val_AUC: 0.8449\n",
      "Epoch 54/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 55/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 56/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4837 - val_AUC: 0.8449\n",
      "Epoch 57/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 58/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 59/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 60/250\n",
      "2600000/2600000 [==============================] - 168s 65us/sample - loss: 0.4787 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 61/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 62/250\n",
      "2600000/2600000 [==============================] - 167s 64us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 63/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 64/250\n",
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 65/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 66/250\n",
      "2600000/2600000 [==============================] - 170s 65us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 67/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 68/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 69/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 70/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4786 - AUC: 0.8483 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 71/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4786 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 72/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4786 - AUC: 0.8484 - val_loss: 0.4836 - val_AUC: 0.8449\n",
      "Epoch 73/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4786 - AUC: 0.8484 - val_loss: 0.4836 - val_AUC: 0.8450\n",
      "Epoch 74/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4786 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 75/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 76/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 77/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 78/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 79/250\n",
      "2600000/2600000 [==============================] - 168s 65us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 80/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 81/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 82/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 83/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 84/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 85/250\n",
      "2600000/2600000 [==============================] - 168s 65us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 86/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 87/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 88/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4785 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 89/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4784 - AUC: 0.8484 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 90/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 91/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4835 - val_AUC: 0.8450\n",
      "Epoch 92/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8450\n",
      "Epoch 93/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8450\n",
      "Epoch 94/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8450\n",
      "Epoch 95/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8450\n",
      "Epoch 96/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8450\n",
      "Epoch 97/250\n",
      "2600000/2600000 [==============================] - 167s 64us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8450\n",
      "Epoch 98/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8450\n",
      "Epoch 99/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 100/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 101/250\n",
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 102/250\n",
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4784 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 103/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4783 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 104/250\n",
      "2600000/2600000 [==============================] - 168s 65us/sample - loss: 0.4783 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 105/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4783 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 106/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4783 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 107/250\n",
      "2600000/2600000 [==============================] - 168s 65us/sample - loss: 0.4783 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 108/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4783 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 109/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4783 - AUC: 0.8485 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 110/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4783 - AUC: 0.8486 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 111/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4783 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4783 - AUC: 0.8486 - val_loss: 0.4834 - val_AUC: 0.8451\n",
      "Epoch 113/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4783 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n",
      "Epoch 114/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4783 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n",
      "Epoch 115/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4783 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n",
      "Epoch 116/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4783 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n",
      "Epoch 117/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4783 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8452\n",
      "Epoch 118/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n",
      "Epoch 119/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n",
      "Epoch 120/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n",
      "Epoch 121/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n",
      "Epoch 122/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8451\n",
      "Epoch 123/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8452\n",
      "Epoch 124/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8452\n",
      "Epoch 125/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8452\n",
      "Epoch 126/250\n",
      "2600000/2600000 [==============================] - 168s 65us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8452\n",
      "Epoch 127/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4833 - val_AUC: 0.8452\n",
      "Epoch 128/250\n",
      "2600000/2600000 [==============================] - 176s 68us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 129/250\n",
      "2600000/2600000 [==============================] - 174s 67us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 130/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4782 - AUC: 0.8486 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 131/250\n",
      "2600000/2600000 [==============================] - 174s 67us/sample - loss: 0.4782 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 132/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 133/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 134/250\n",
      "2600000/2600000 [==============================] - 171s 66us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 135/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 136/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 137/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 138/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 139/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 140/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 141/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 142/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 143/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 144/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 145/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 146/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4781 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8452\n",
      "Epoch 147/250\n",
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4780 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8453\n",
      "Epoch 148/250\n",
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4780 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8453\n",
      "Epoch 149/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4780 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8453\n",
      "Epoch 150/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4780 - AUC: 0.8487 - val_loss: 0.4832 - val_AUC: 0.8453\n",
      "Epoch 151/250\n",
      "2600000/2600000 [==============================] - 180s 69us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 152/250\n",
      "2600000/2600000 [==============================] - 178s 68us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 153/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 154/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 155/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 156/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 157/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 158/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 159/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 160/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4780 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 161/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 162/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 163/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 164/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 165/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 166/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 167/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 168/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4830 - val_AUC: 0.8453\n",
      "Epoch 169/250\n",
      "2600000/2600000 [==============================] - 168s 65us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4831 - val_AUC: 0.8453\n",
      "Epoch 170/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4779 - AUC: 0.8488 - val_loss: 0.4830 - val_AUC: 0.8453\n",
      "Epoch 171/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4779 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8453\n",
      "Epoch 172/250\n",
      "2600000/2600000 [==============================] - 174s 67us/sample - loss: 0.4779 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8453\n",
      "Epoch 173/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4779 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8453\n",
      "Epoch 174/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4779 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8453\n",
      "Epoch 175/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4779 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8453\n",
      "Epoch 176/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 177/250\n",
      "2600000/2600000 [==============================] - 171s 66us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 178/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 179/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 180/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 181/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 182/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 183/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 184/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 185/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 186/250\n",
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 187/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 188/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 189/250\n",
      "2600000/2600000 [==============================] - 167s 64us/sample - loss: 0.4778 - AUC: 0.8489 - val_loss: 0.4830 - val_AUC: 0.8454\n",
      "Epoch 190/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4777 - AUC: 0.8489 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 191/250\n",
      "2600000/2600000 [==============================] - 169s 65us/sample - loss: 0.4777 - AUC: 0.8489 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 192/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 193/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 194/250\n",
      "2600000/2600000 [==============================] - 175s 67us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 195/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 196/250\n",
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 197/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 198/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 199/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 200/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 201/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 202/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8455\n",
      "Epoch 203/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8454\n",
      "Epoch 204/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4777 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8455\n",
      "Epoch 205/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4776 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8455\n",
      "Epoch 206/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4776 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8455\n",
      "Epoch 207/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4776 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8455\n",
      "Epoch 208/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4776 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8455\n",
      "Epoch 209/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4776 - AUC: 0.8490 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 210/250\n",
      "2600000/2600000 [==============================] - 165s 63us/sample - loss: 0.4776 - AUC: 0.8490 - val_loss: 0.4829 - val_AUC: 0.8455\n",
      "Epoch 211/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4776 - AUC: 0.8490 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 212/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4776 - AUC: 0.8490 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 213/250\n",
      "2600000/2600000 [==============================] - 171s 66us/sample - loss: 0.4776 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 214/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4776 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 215/250\n",
      "2600000/2600000 [==============================] - 176s 68us/sample - loss: 0.4776 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 216/250\n",
      "2600000/2600000 [==============================] - 185s 71us/sample - loss: 0.4776 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 217/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4776 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 218/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4776 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 219/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4776 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 220/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 221/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 222/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600000/2600000 [==============================] - 166s 64us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 223/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 224/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 225/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 226/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 227/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 228/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 229/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4828 - val_AUC: 0.8455\n",
      "Epoch 230/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 231/250\n",
      "2600000/2600000 [==============================] - 163s 63us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4827 - val_AUC: 0.8455\n",
      "Epoch 232/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4775 - AUC: 0.8491 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 233/250\n",
      "2600000/2600000 [==============================] - 164s 63us/sample - loss: 0.4775 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 234/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4775 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 235/250\n",
      "2600000/2600000 [==============================] - 169s 65us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 236/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 237/250\n",
      "2600000/2600000 [==============================] - 161s 62us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 238/250\n",
      "2600000/2600000 [==============================] - 168s 64us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 239/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 240/250\n",
      "2600000/2600000 [==============================] - 162s 62us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 241/250\n",
      "2600000/2600000 [==============================] - 171s 66us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 242/250\n",
      "2600000/2600000 [==============================] - 171s 66us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 243/250\n",
      "2600000/2600000 [==============================] - 172s 66us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 244/250\n",
      "2600000/2600000 [==============================] - 165s 64us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 245/250\n",
      "2600000/2600000 [==============================] - 226s 87us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 246/250\n",
      "2600000/2600000 [==============================] - 186s 71us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 247/250\n",
      "2600000/2600000 [==============================] - 328s 126us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 248/250\n",
      "2600000/2600000 [==============================] - 280s 108us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4827 - val_AUC: 0.8456\n",
      "Epoch 249/250\n",
      "2600000/2600000 [==============================] - 235s 90us/sample - loss: 0.4774 - AUC: 0.8492 - val_loss: 0.4826 - val_AUC: 0.8456\n",
      "Epoch 250/250\n",
      "2600000/2600000 [==============================] - 127s 49us/sample - loss: 0.4773 - AUC: 0.8492 - val_loss: 0.4826 - val_AUC: 0.8456\n"
     ]
    }
   ],
   "source": [
    "# train the model and evaluate against the test datset\n",
    "training_history = model.fit(x_train, y_train, epochs=250, validation_data=(x_test,y_test), batch_size=100,\n",
    "          callbacks=[lr_callback,es_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots for AUC and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = model.history.history['AUC']\n",
    "val_loss = model.history.history['val_AUC']\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Test')\n",
    "plt.legend(title='')\n",
    "plt.title('AUC Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df5xddX3n8df73vmVhIRIEvmV1AyUdo2IEUYEtVYrKrDbUBcVVKygmNouD2zVPjY8uusP2O6Crb8QHmtBQ2mtotWyjW4UAat1pUIGG4EkjaQRYUyUSSIEA8nMvfezf5xz75x750yYZObMncy8n4/HfdxzvufH/R4umff9nJ+KCMzMzFqV2t0BMzObnhwQZmaWywFhZma5HBBmZpbLAWFmZrkcEGZmlssBYWZmuRwQNitIekTSOW367OMlfU7STklPSfo3SR+RNK8d/TEbLweEWYEkHQP8CzAHODsi5gOvBRYCJx/G+jomt4dmY3NA2Kwn6d2StknaI2mdpBPSdkn6hKTHJT0p6QFJp6bTzpe0Oa0IfibpA2Os/n3AU8AlEfEIQEQ8FhHvjYgHJC2XFNk//JK+I+nydPhSSd9P+7EHuEbSE/V+pPMskfSMpOem4/9J0sZ0vnsknVbAfzabBRwQNqtJ+h3gfwFvBo4Hfgrclk5+HfBK4DdIfvFfBOxOp30O+IO0IjgV+PYYH3EO8A8RUZtAN18KbAeeC1wN/APwlsz0NwPfjYjHJZ0OrAX+AFgE/BWwTlL3BD7fZikHhM12bwPWRsQPI+IAcBVwtqTlwDAwH/gPgCJiS0TsTJcbBlZIWhARv4yIH46x/kXAzjGmjdeOiPh0RFQi4hngCzQHxFvTNoB3A38VEfdGRDUibgUOAGdNsA82CzkgbLY7gaRqACAifkVSJZwYEd8GbgBuBH4h6SZJC9JZLwTOB34q6buSzh5j/btJKpOJeKxl/NvAHEkvlfQ8YCVwezrtecD7091LT0h6AliWbqfZIXFA2Gy3g+SPKgDpmUWLgJ8BRMT1EXEG8AKSXU1/mrZviIgLSHb7/B/gy2Os/y7gDZLG+re2L32fm2k7rmWeplsup7urvkxSRbwV+HpEPJVOfgz484hYmHnNjYgvjvH5ZmNyQNhs0impJ/PqINk1c5mklel++v8J3BsRj0h6SforvZPkD/l+oCqpS9LbJB0dEcPAXqA6xmd+HFgA3Jr+2kfSiZI+Lum0iBgkCaNLJJUlvZPxnd30BZJjIm9jZPcSwM3Ae9J+S9I8Sf9R0vxD/G9l5oCwWWU98Ezm9eGIuBv478BXSY4VnAxcnM6/gOQP7i9JdkPtBv4ynfZ24BFJe4H3AJfkfWBE7AFeRnLM4l5JTwF3A08C29LZ3k1SmewmqVTuebYNiYh7SULrBOAbmfb+dH03pP3eBlz6bOszyyM/MMjMzPK4gjAzs1wOCDMzy+WAMDOzXA4IMzPLNWNu/LV48eJYvnx5u7thZnZEuf/++3dFxJK8aTMmIJYvX05/f3+7u2FmdkSR9NOxpnkXk5mZ5XJAmJlZLgeEmZnlmjHHIMzMDsXw8DADAwPs37+/3V2ZEj09PSxdupTOzs5xL1NoQEg6F/gUUAY+GxHXtky/FPgL0jtnAjdExGcz0xcAW4DbI+KKIvtqZrPLwMAA8+fPZ/ny5Uhqd3cKFRHs3r2bgYEBent7x71cYbuYJJVJ7qN/HrACeIukFTmzfikiVqavz7ZMuwb4blF9NLPZa//+/SxatGjGhwOAJBYtWnTI1VKRxyDOBLZFxPaIGCJ5jOMF411Y0hnAscC3Cuqfmc1ysyEc6g5nW4sMiBNpfhLWQNrW6sL0YfBfkbQMIH24ysdIH84yFkmrJfVL6h8cHDysTu47UOHj39rKvz76y8Na3sxspioyIPLiqvXe4l8DlkfEaSRP3ro1bf8jYH1EtD5qsXllETdFRF9E9C1Zknsh4LPaP1zl+m9v48GfPXlYy5uZHY7du3ezcuVKVq5cyXHHHceJJ57YGB8aGhrXOi677DK2bt1aWB+LPEg9QPIs3LqlJI93bIiI3ZnRm4Hr0uGzgd+S9EfAUUCXpF9FxJrJ7mQpLbuqNT8Xw8ymzqJFi9i4cSMAH/7whznqqKP4wAc+0DRPRBARlEr5v+VvueWWQvtYZAWxAThFUq+kLpKndK3LziAp+zD3VSRnLBERb4uIX4uI5cAHgL8pIhwASqUkIJwPZjYdbNu2jVNPPZX3vOc9nH766ezcuZPVq1fT19fHC17wAq6++urGvK94xSvYuHEjlUqFhQsXsmbNGl70ohdx9tln8/jjj0+4L4VVEBFRkXQFcAfJaa5rI2KTpKuB/ohYB1wpaRVQAfbQhkcjpvlAzQlhNmt95Gub2Lxj76Suc8UJC/jQ777gsJbdvHkzt9xyC5/5zGcAuPbaaznmmGOoVCq8+tWv5o1vfCMrVjSfFPrkk0/y27/921x77bW8733vY+3ataxZM7Hf1YVeBxER60meA5xt+2Bm+CrgqmdZx18Df11A9wAoNyoIB4SZTQ8nn3wyL3nJSxrjX/ziF/nc5z5HpVJhx44dbN68eVRAzJkzh/POOw+AM844g+9973sT7sesv5K6cQzCAWE2ax3uL/2izJs3rzH88MMP86lPfYr77ruPhQsXcskll+Rez9DV1dUYLpfLVCqVCfdj1t+LqR4Qzgczm4727t3L/PnzWbBgATt37uSOO+6Yss92BZEeg/BZTGY2HZ1++umsWLGCU089lZNOOomXv/zlU/bZihny07mvry8O54FBEUHvVet572tO4U9e+xsF9MzMpqMtW7bw/Oc/v93dmFJ52yzp/ojoy5t/1u9iUmMX08wISjOzyTLrAwKSM5l8kNrMrJkDAihLvlDOzKyFAwKQfKGcmVkrBwTJLiZfKGdm1swBQXItRLXW7l6YmU0vDgiSayFcQZjZVJqM230DrF27lp///OeF9HHWXygHyR1dHRBmNpXGc7vv8Vi7di2nn346xx133GR30QEB9bOYHBBmNj3ceuut3HjjjQwNDfGyl72MG264gVqtxmWXXcbGjRuJCFavXs2xxx7Lxo0bueiii5gzZw733Xdf0z2ZJsoBQXKxnI9BmM1i31gDP39wctd53AvhvGsPebGHHnqI22+/nXvuuYeOjg5Wr17Nbbfdxsknn8yuXbt48MGkn0888QQLFy7k05/+NDfccAMrV66c3P7jgACgXPKV1GY2Pdx1111s2LCBvr7k7hfPPPMMy5Yt4/Wvfz1bt27lve99L+effz6ve93rCu+LA4L6WUwOCLNZ6zB+6RclInjnO9/JNddcM2raAw88wDe+8Q2uv/56vvrVr3LTTTcV2hefxUQSEM4HM5sOzjnnHL785S+za9cuIDnb6dFHH2VwcJCI4E1vehMf+chH+OEPfwjA/PnzeeqppwrpiysIoFTyaa5mNj288IUv5EMf+hDnnHMOtVqNzs5OPvOZz1Aul3nXu95FRCCJ6667DoDLLruMyy+/vJCD1LP+dt8Ar/qLf+JFyxbyqYtfPMm9MrPpyrf7Tvh238/CxyDMzEZzQJBcKDdDCikzs0njgCC51YYrCLPZZ6bsYh+Pw9nWQgNC0rmStkraJmlNzvRLJQ1K2pi+Lk/bnyfp/rRtk6T3FNnPkq+kNpt1enp62L1796wIiYhg9+7d9PT0HNJyhZ3FJKkM3Ai8FhgANkhaFxGbW2b9UkRc0dK2E3hZRByQdBTwULrsjiL66oAwm32WLl3KwMAAg4OD7e7KlOjp6WHp0qWHtEyRp7meCWyLiO0Akm4DLgBaA2KUiMjeyrCbgiudcskHqc1mm87OTnp7e9vdjWmtyD+8JwKPZcYH0rZWF0p6QNJXJC2rN0paJumBdB3XFVU9QP1230Wt3czsyFRkQCinrfXP8NeA5RFxGnAXcGtjxojH0vZfB94h6dhRHyCtltQvqX8iZaJv921mNlqRATEALMuMLwWaqoCI2B0RB9LRm4EzWleSVg6bgN/KmXZTRPRFRN+SJUsOu6O+3beZ2WhFBsQG4BRJvZK6gIuBddkZJB2fGV0FbEnbl0qakw4/B3g5sLWojvpCOTOz0Qo7SB0RFUlXAHcAZWBtRGySdDXQHxHrgCslrQIqwB7g0nTx5wMfkxQku6r+MiIm+WbtI5J7MRW1djOzI1OhN+uLiPXA+pa2D2aGrwKuylnuTuC0IvuWVZKo+IlBZmZNfCU1yWmuPgZhZtbMAUH6yFHng5lZEwcEUNbsuieLmdl4OCDwWUxmZnkcENQvlGt3L8zMphcHBOmtNpwQZmZNHBD4LCYzszwOCOpnMTkgzMyyHBAk92JyPpiZNXNA4EeOmpnlcUDg232bmeVxQJA+ctQVhJlZEwcE9edBtLsXZmbTiwOC5HbfPovJzKyZAwLvYjIzy+OAIA0IVxBmZk0cECRXUvs0VzOzZg4IkgrCBYSZWTMHBOmFck4IM7MmDgh8sz4zszwOCJKb9dVq7e6Fmdn04oAAyiVcQZiZtXBAkD5y1AFhZtak0ICQdK6krZK2SVqTM/1SSYOSNqavy9P2lZL+RdImSQ9IuqjIftbPYgqHhJlZQ0dRK5ZUBm4EXgsMABskrYuIzS2zfikirmhpexr4/Yh4WNIJwP2S7oiIJ4roa0kCoBZQVhGfYGZ25CmygjgT2BYR2yNiCLgNuGA8C0bEjyPi4XR4B/A4sKSojpbT/wo+DmFmNqLIgDgReCwzPpC2tbow3Y30FUnLWidKOhPoAv49Z9pqSf2S+gcHBw+7o0orCF9NbWY2osiAyNtZ0/oX+GvA8og4DbgLuLVpBdLxwN8Cl0XEqBNRI+KmiOiLiL4lSw6/wCiXlK7vsFdhZjbjFBkQA0C2IlgK7MjOEBG7I+JAOnozcEZ9mqQFwP8F/ltE/KDAfpLmg89kMjPLKDIgNgCnSOqV1AVcDKzLzpBWCHWrgC1pexdwO/A3EfH3BfYRyB6kdkCYmdUVdhZTRFQkXQHcAZSBtRGxSdLVQH9ErAOulLQKqAB7gEvTxd8MvBJYJKnedmlEbCyir42A8DEIM7OGwgICICLWA+tb2j6YGb4KuCpnuc8Dny+yb1n1YxDOBzOzEb6SmswxCCeEmVmDAyKCztozdFLxMQgzswwHxL5dXHznWVxU/icHhJlZhgOi3AlAJxXvYjIzy3BAlLuAJCBcQJiZjXBANAKi6grCzCzDAVEqE4gu+SC1mVmWA0KiVur0WUxmZi0cEJAJiHb3xMxs+nBAAJEGhI9BmJmNcECQVBBd3sVkZtbEAQFEqSvZxTTqiRNmZrOXAwKIciedPovJzKyJA4KRg9R+YJCZ2QgHBMlB6i6qhAPCzKzBAcHIMYiqj0GYmTU4IEiPQfgsJjOzJg4IgFJ6kNrXQZiZNTgggCh3pddBtLsnZmbThwMCoOyzmMzMWjkgyFwo54AwM2twQECjgvAxCDOzEQ4IgHIXnfIDg8zMssYVEJJOltSdDr9K0pWSFo5juXMlbZW0TdKanOmXShqUtDF9XZ6Z9k1JT0j6+qFs0OGIcqcPUpuZtRhvBfFVoCrp14HPAb3AFw62gKQycCNwHrACeIukFTmzfikiVqavz2ba/wJ4+zj7NzFlH4MwM2s13oCoRUQFeAPwyYj4E+D4Z1nmTGBbRGyPiCHgNuCC8XYsIu4Gnhrv/BPigDAzG2W8ATEs6S3AO4D6Lp/OZ1nmROCxzPhA2tbqQkkPSPqKpGXj7A8AklZL6pfUPzg4eCiLNiv7gUFmZq3GGxCXAWcDfx4RP5HUC3z+WZZRTlvrX+CvAcsj4jTgLuDWcfYnWVnETRHRFxF9S5YsOZRFm6jcRbcqhAPCzKyhYzwzRcRm4EoASc8B5kfEtc+y2ACQrQiWAjta1rs7M3ozcN14+jPpyl0A1KrDbfl4M7PpaLxnMX1H0gJJxwA/Am6R9PFnWWwDcIqkXkldwMXAupb1Zo9jrAK2jL/rk0cdSUBQHWrHx5uZTUvj3cV0dETsBf4zcEtEnAGcc7AF0oPaVwB3kPzh/3JEbJJ0taRV6WxXStok6UckFcql9eUlfQ/4e+A1kgYkvf5QNuyQpBUENQeEmVnduHYxAR3pr/03A3823pVHxHpgfUvbBzPDVwFXjbHsb433cyZKaUDIu5jMzBrGW0FcTVIJ/HtEbJB0EvBwcd2aWupITsiKiisIM7O68R6k/nuS3T318e3AhUV1aqrVj0GUfAzCzKxhvAepl0q6XdLjkn4h6auSlhbdualS38UUDggzs4bx7mK6heQMpBNILnb7Wto2I6ijO3mv+RiEmVndeANiSUTcEhGV9PXXwOFfmTbN1I9B+CC1mdmI8QbELkmXSCqnr0uA3c+61BFCPs3VzGyU8QbEO0lOcf05sBN4I8ntN2aEUn0XkysIM7OGcQVERDwaEasiYklEPDcifo/korkZobGLyccgzMwaJvJEufdNWi/arHGQ2mcxmZk1TCQg8u7WekQqp9dBuIIwMxsxkYCYMffGlgPCzGyUg15JLekp8oNAwJxCetQGvheTmdloBw2IiJg/VR1pqzQgSj7N1cysYSK7mGaOsncxmZm1ckAAlJPTXEsOCDOzBgcEuIIwM8vhgABXEGZmORwQAKUyFUoOCDOzDAdEapgOSuGAMDOrc0CkKnS4gjAzy3BApIbpoOwKwsyswQGRcgVhZtbMAZEaVidlB4SZWUOhASHpXElbJW2TtCZn+qWSBiVtTF+XZ6a9Q9LD6esdRfYTYIhOyrUDRX+MmdkR46D3YpoISWXgRuC1wACwQdK6iNjcMuuXIuKKlmWPAT4E9JHcLPD+dNlfFtXfIXXR4YAwM2sosoI4E9gWEdsjYgi4DbhgnMu+HrgzIvakoXAncG5B/QRgWD10VPcX+RFmZkeUIgPiROCxzPhA2tbqQkkPSPqKpGWHsqyk1ZL6JfUPDg5OqLPDpW5XEGZmGUUGRN4T51qfLfE1YHlEnAbcBdx6CMsSETdFRF9E9C1ZsmRCna2UeuisuYIwM6srMiAGgGWZ8aXAjuwMEbE7Iuo/228GzhjvspOtUu6mI1xBmJnVFRkQG4BTJPVK6gIuBtZlZ5B0fGZ0FbAlHb4DeJ2k50h6DvC6tK0w1XIP3d7FZGbWUNhZTBFRkXQFyR/2MrA2IjZJuhroj4h1wJWSVgEVYA9wabrsHknXkIQMwNURsaeovgJUSz10hp8oZ2ZWV1hAAETEemB9S9sHM8NXAVeNsexaYG2R/cuqdcyhB1cQZmZ1vpI6FR09dDMEtVq7u2JmNi04IFLRMScZqPhMJjMzcEA0REdPMuCAMDMDHBAN6poLQGX/r9rcEzOz6cEBUZdWEEP7n25zR8zMpgcHRKqUVhBDriDMzAAHRIO6koPUw/ufaXNPzMymBwdEqpxWEMOuIMzMAAdEQz0gqkOuIMzMwAHR0NFdP4tpX5t7YmY2PTggUuU0IKrDriDMzMAB0dDRMw+A2gFXEGZm4IBo6OxJKojasK+kNjMDB0RDV1pBxJAvlDMzAwdEQ3dXN5UoET4GYWYGOCAaero62E8XckCYmQEOiIbuzhLP0AUVB4SZGTggGno6yhxwBWFm1uCASHWWxTPRjao+i8nMDBwQDZIYUhdlPzDIzAxwQDQ5oG5KriDMzAAHRJNhdVN2QJiZAQ6IJkOlbjpqB9rdDTOzaaHQgJB0rqStkrZJWnOQ+d4oKST1peNdkm6R9KCkH0l6VZH9rKuUeuiouYIwM4MCA0JSGbgROA9YAbxF0oqc+eYDVwL3ZprfDRARLwReC3xMUuHVztPl+cyr7i36Y8zMjghF/tE9E9gWEdsjYgi4DbggZ75rgI8C2Z/uK4C7ASLiceAJoK/AvgLwdHkhR9Wegupw0R9lZjbtFRkQJwKPZcYH0rYGSS8GlkXE11uW/RFwgaQOSb3AGcCy1g+QtFpSv6T+wcHBCXf46c7npAN7JrwuM7MjXZEBoZy2aExMdhl9Anh/znxrSQKlH/gkcA9QGbWyiJsioi8i+pYsWTLhDj/ddUw6sGvC6zIzO9J1FLjuAZp/9S8FdmTG5wOnAt+RBHAcsE7SqojoB/6kPqOke4CHC+wrAAe60gpi38SrETOzI12RFcQG4BRJvZK6gIuBdfWJEfFkRCyOiOURsRz4AbAqIvolzZU0D0DSa4FKRGwusK8AVHsWJQP7XEGYmRVWQURERdIVwB1AGVgbEZskXQ30R8S6gyz+XOAOSTXgZ8Dbi+pnVnl+spsq9g3m7h8zM5tNitzFRESsB9a3tH1wjHlflRl+BPjNIvuWp3vBImohqk8N0jnVH25mNs34SuqMo+fOYQ/zGdrrYxBmZg6IjIVzO9kT86k+9Xi7u2Jm1nYOiIyFczrZHUf7NFczMxwQTY6e28lu5lN2QJiZOSCyFs7tYk8soOOAr6Q2M3NAZCS7mBbQPbzX92Mys1nPAZExt6vMoNLbbTzxaHs7Y2bWZg6IDEls607vSP7oD9rbGTOzNnNAtPjl3F5+VVoAP72n3V0xM2srB0SLo+d2s6XrVPjp99vdFTOztnJAtFg4t5N/1Qr45U9g745nX8DMbIZyQLQ4ek4X/1w9NRn58u/Dz34IEQdfyMxsBir0Zn1HooVzO/nm/uPholvhH6+Am18N85bA4t+Ezh4od0O5Ezq6odyVvEplUDl5zw7X3xGUSum8ncny5a7kvdSRv4xKI+MqJfOVO9L5W1719WWXaVqvfweY2aFzQLRYOKeTfUNVhn5zFV1//ErYuh4e+T7s2Z48irQ6lLwqB5JrJaoHoFaFqCXvtQpENRlmGlUeeaGRDaVGuGhk3noINsKoPDqARo13jARaYzz9vMbwoS5bzh8/rGU7Mtvesqx8k3ezLAdEi+fM6wJgz74hjjv6GHjxJcnrcNRqQGTCYzgNlaGR91olDZhq5r3WMp55r1WSZevLNdY53DJvJWc9leYwa0xL54vITB+GylAagPXPqjb3uXW9tUq6bHa8OjJvfTxqk/eFTSaNEUSt4VLqSCrJ7vkjwVIP2Gxl1wisejBn5lU5rSA708qwM/nvUznQUqHW5+lsqRBbqs28aRJJ9Zr5rOw2NG1vJlxbt7Xpx4WDdDZxQLR43qK5ADyyex/HHd0zsZU1du2k/0CZ4PpminoQNQXIOMOlHnxN4zkheljLtgbdQZatHIADe5NtqW9P5cDo9WTfI/uDIRv06bBK0NGThvMBplUFmjUqSA9WHR5iJXiwKnBS1n8oVeVhbNsMC1MHRIvexfMA2D64j7NOWtTm3sxQUvKruez//Q6qWkkrxKFkOLeqrI1UgKOqwgDqYVyvMg8SwoccuIcZrtn5hoda5m0N8DGWa92G6epQwomA/U+m1elRyXZWhmDOwmRdUUv/7XQ3HwPt6EqOkb7mv0969/0vtMUJR8+hu6PET3b9qt1dsdmuHqKdc9rdk+nvsKvKvF2wB6k2DzlgW3Yh5wVnfRig5+gkFIaeSiqQcjfsfyKZpnLy+dX0+OfQPqjuSYZLxTwD0wHRolQSvYvn8ZNd+9rdFTMbr1IJSl3t7sWM4/Mfc/Qunsd2B4SZzXIOiBy9i+fx6O6nGa5O07NtzMymgAMiR+/ieVRqwcAvn2l3V8zM2qbQgJB0rqStkrZJWnOQ+d4oKST1peOdkm6V9KCkLZKuKrKfrU5+7lEA/NvOvVP5sWZm00phASGpDNwInAesAN4iaUXOfPOBK4F7M81vAroj4oXAGcAfSFpeVF9bnXrC0cktNzb9fKo+0sxs2imygjgT2BYR2yNiCLgNuCBnvmuAjwL7M20BzJPUAcwBhoAp+znf1VHivFOP587Nv+CZoWl8jrWZWYGKDIgTgccy4wNpW4OkFwPLIuLrLct+BdgH7AQeBf4yIva0foCk1ZL6JfUPDg5Oaud/90XH8/RQlTu3/GJS12tmdqQoMiDyrjFv3DtAUgn4BPD+nPnOBKrACUAv8H5JJ41aWcRNEdEXEX1LliyZnF6nXtq7iN7F87jm65vZ+aQPVpvZ7FPkhXIDwLLM+FIg+wSe+cCpwHeU3K/kOGCdpFXAW4FvRsQw8Lik7wN9wPYC+9ukXBJ/9fYzeMON32fVDd/nkpc+j5W/tpD5PR30dJTp7izR3VGis1yioyQ6SiU6yqKjLDpLJUqlI/seLGZmRQbEBuAUSb3Az4CLSf7wAxARTwKL6+OSvgN8ICL6Jb0G+B1JnwfmAmcBnyywr7l+49j5/N27z+Kj3/w3PnHXjw9p2ZKgo1yisyQ66iFSToKkszzS1llOgyUTMp2jpiXLlEsj0zrK6Xoay4hyaXRbR6atnIZXdlr281pDLjutXBI6wm88ZmaHprCAiIiKpCuAO4AysDYiNkm6GuiPiHUHWfxG4BbgIZJdVbdExANF9fVgVi5byBfefRZ79g2x7fFfsW+owoHhKvuHawxVagzXalSqwXC1RrUWVGrJcKUajWmVao3hWlDNttVqDKfTGsvUajwznEyrr7NSi6b1N7XValP6sLt6yHU2giQTgJmQyw/AkZDMC7l6AGbX2VlfZ8v85VLz+hrBlhPGB53HwWd2UIoZ8jjNvr6+6O/vb3c3plw2NLIBNBIkaRDVRoKlHlhN06oxMn/63hpylZYAzIZcPQAbbaMCMKjWmteZ/exswE613BAZI3TKpSTAyjnVX1KhjVRy2YqvXr0lbdlqT5TLzevMztMxarmceUa1jYx3OADtWUi6PyL68qb5Zn1HuHJJlEvldndj0kREU4XUFE6tQdcSekmAZcKulrPsqHUk81Rr0RKkI/PUQ7hai5HwrAVPD1UayyXT0pBrqfjqy1XT11SrB0VriOQGYqa6GrsKy7blB2hzJZeM1z97ZPnscpn+jRnOaViWm+dxABbHAWHTipT8EekswxxmTvDV1dLgagRKtTlYslVfNnQaFVZLpVetjczbHIbN82TbmubJhmZL2/7hGpVatVFNtoZzUzjWq9g2BWASHq0VVqk5WLLhmN1t2bLc6F2RpZzlWivN/F2YHa2BmFehZo79jWovqa0nvDggzKZQqSS60n/wMzEA6xVgU/XUEkDZUBs1T6Nqy5mnpdprmicN20rLOqu15t2f2RgVJOQAAAWQSURBVGN6Q5Ua+4aqBwnQvOpz6gOwJA4SQMnwihMWcMNbT5/0z3ZAmNmkyVaAPZ0zMwDrx8rG3oU5+rje+OcZOdbXqMxa58mp3n7tmLmFbK8DwsxsnKR0V9DMy75cvt23mZnlckCYmVkuB4SZmeVyQJiZWS4HhJmZ5XJAmJlZLgeEmZnlckCYmVmuGXM3V0mDwE8nsIrFwK5J6s6Rwts8O3ibZ4fD3ebnRUTuIzlnTEBMlKT+sW55O1N5m2cHb/PsUMQ2exeTmZnlckCYmVkuB8SIm9rdgTbwNs8O3ubZYdK32ccgzMwslysIMzPL5YAwM7Ncsz4gJJ0raaukbZLWtLs/RZH0iKQHJW2U1J+2HSPpTkkPp+/PaXc/J0rSWkmPS3oo05a7nUpcn373D0ia/Gc2ToExtvnDkn6Wft8bJZ2fmXZVus1bJb2+Pb0+fJKWSfonSVskbZL03rR9pn/PY213cd91RMzaF1AG/h04CegCfgSsaHe/CtrWR4DFLW0fBdakw2uA69rdz0nYzlcCpwMPPdt2AucD3wAEnAXc2+7+T+I2fxj4QM68K9L/z7uB3vT//3K7t+EQt/d44PR0eD7w43S7Zvr3PNZ2F/Zdz/YK4kxgW0Rsj4gh4Dbggjb3aSpdANyaDt8K/F4b+zIpIuKfgT0tzWNt5wXA30TiB8BCScdPTU8nzxjbPJYLgNsi4kBE/ATYRvLv4IgRETsj4ofp8FPAFuBEZv73PNZ2j2XC3/VsD4gTgccy4wMc/D/4kSyAb0m6X9LqtO3YiNgJyf98wHPb1rtijbWdM/37vyLdpbI2s/twRm2zpOXAi4F7mUXfc8t2Q0Hf9WwPCOW0zdTzfl8eEacD5wH/RdIr292haWAmf///GzgZWAnsBD6Wts+YbZZ0FPBV4I8jYu/BZs1pOyK3GXK3u7DverYHxACwLDO+FNjRpr4UKiJ2pO+PA7eTlJq/qJfa6fvj7ethocbazhn7/UfELyKiGhE14GZGdi3MiG2W1EnyR/LvIuIf0uYZ/z3nbXeR3/VsD4gNwCmSeiV1ARcD69rcp0knaZ6k+fVh4HXAQyTb+o50tncA/9ieHhZurO1cB/x+epbLWcCT9V0UR7qWfexvIPm+IdnmiyV1S+oFTgHum+r+TYQkAZ8DtkTExzOTZvT3PNZ2F/pdt/vIfLtfJGc4/JjkCP+ftbs/BW3jSSRnM/wI2FTfTmARcDfwcPp+TLv7Ognb+kWSMnuY5BfUu8baTpIS/Mb0u38Q6Gt3/ydxm/823aYH0j8Ux2fm/7N0m7cC57W7/4exva8g2VXyALAxfZ0/C77nsba7sO/at9owM7Ncs30Xk5mZjcEBYWZmuRwQZmaWywFhZma5HBBmZpbLAWF2CCRVM3fN3DiZdwCWtDx7R1azdutodwfMjjDPRMTKdnfCbCq4gjCbBOnzNq6TdF/6+vW0/XmS7k5vpHa3pF9L24+VdLukH6Wvl6WrKku6Ob3f/7ckzWnbRtms54AwOzRzWnYxXZSZtjcizgRuAD6Ztt1Acqvp04C/A65P268HvhsRLyJ5lsOmtP0U4MaIeAHwBHBhwdtjNiZfSW12CCT9KiKOyml/BPidiNie3lDt5xGxSNIuklsfDKftOyNisaRBYGlEHMisYzlwZ0Scko7/V6AzIv5H8VtmNporCLPJE2MMjzVPngOZ4So+Tmht5IAwmzwXZd7/JR2+h+QuwQBvA/5fOnw38IcAksqSFkxVJ83Gy79OzA7NHEkbM+PfjIj6qa7dku4l+eH1lrTtSmCtpD8FBoHL0vb3AjdJehdJpfCHJHdkNZs2fAzCbBKkxyD6ImJXu/tiNlm8i8nMzHK5gjAzs1yuIMzMLJcDwszMcjkgzMwslwPCzMxyOSDMzCzX/wfWJ4np6qb4LAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Test')\n",
    "plt.legend(title='')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
