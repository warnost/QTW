AP
diffs = offlineSummary[ , c("posX", "posY")] -
AP[ offlineSummary$mac, ]
offlineSummary$dist = sqrt(diffs[ , 1]^2 + diffs[ , 2]^2)
oldPar = par(mar = c(3.1, 3.1, 1, 1))
library(lattice)
xyplot(signal ~ dist | factor(mac) + factor(angle),
data = offlineSummary, pch = 19, cex = 0.3,
xlab ="distance")
par(oldPar)
head(offlineSummary)
fit <- lm(cbind(posX,posY) ~ signal + angle, data=offlineSummary)
summary(fit)
library(knitr)
library(rmdformats)
library(formatR)
library(naniar)
library(tidyverse)
library(magrittr)
library(caret)
library(pls)
## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
cache=FALSE,
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
########## Data Cleanup ##########
processLine = function(x){
tokens = strsplit(x, "[;=,]")[[1]]
if (length(tokens) == 10)
return(NULL)
# keep the last 4 columns related to measurements
tmp = matrix(tokens[ - (1:10) ], ncol= 4, byrow = TRUE)
# don't include columns 1, 3, 4, 9 because these are variable names
cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow(tmp), 6, byrow = TRUE), tmp)
}
roundOrientation = function(angles) {
refs = seq(0, by = 45, length  = 9)
q = sapply(angles, function(o) which.min(abs(o - refs)))
c(refs[1:8], 0)[q]
}
readData = function(filename = '../Data/offline.final.trace.txt',
subMacs = c("00:0f:a3:39:e1:c0", "00:0f:a3:39:dd:cd", "00:14:bf:b1:97:8a",
"00:14:bf:3b:c7:c6", "00:14:bf:b1:97:90", "00:14:bf:b1:97:8d",
"00:14:bf:b1:97:81"))  {
txt = readLines(filename)
#remove comments
lines = txt[ substr(txt, 1, 1) != "#" ]
#apply function to read and parse data
tmp = lapply(lines, processLine)
offline = as.data.frame(do.call("rbind", tmp),
stringsAsFactors= FALSE)
names(offline) = c("time", "scanMac",
"posX", "posY", "posZ", "orientation",
"mac", "signal", "channel", "type")
# keep only signals from access points
offline = offline[ offline$type == "3", ]
# drop scanMac, posZ, channel, and type - no info in them
dropVars = c("scanMac", "posZ", "channel", "type")
offline = offline[ , !( names(offline) %in% dropVars ) ]
# drop more unwanted access points
offline = offline[ offline$mac %in% subMacs, ]
# convert numeric values
numVars = c("time", "posX", "posY", "orientation", "signal")
offline[ numVars ] = lapply(offline[ numVars ], as.numeric)
# convert time to POSIX
offline$rawTime = offline$time
offline$time = offline$time/1000
class(offline$time) = c("POSIXt", "POSIXct")
# round orientations to nearest 45
offline$angle = roundOrientation(offline$orientation)
return(offline)
}
offline = readData(filename = '../Data/offline.final.trace.txt')
subMacs = names(sort(table(offline$mac), decreasing = TRUE))
online = readData(filename = '../Data/online.final.trace.txt')
# investigating frequency of measurements by mac address
vals = data.frame(table(mac=offline['mac']))
#vals[order(-vals$Freq),] #used table 2 instead
# format signal stength as an integer
offline$signal %<>% as.integer
online$signal %<>% as.integer
# remove channel and scanMac as they have no bearing on the analysis
# transpose long to wide so each of the 7 macs has a column
offline_pivot<-offline %>% pivot_wider(names_from = mac,values_from = signal, values_fn = list(signal=mean))
online_pivot<- online %>% pivot_wider(names_from = mac,values_from = signal, values_fn = list(signal=mean))
miss_plot <- vis_miss(offline_pivot,warn_large_data = F)
# impute NA
#offline_pivot[is.na(offline_pivot)] <- -100
#online_pivot[is.na(online_pivot)] <- -100
# drop records with missing values
non_miss <- complete.cases(offline_pivot)
offline_pivot <- offline_pivot[non_miss,]
non_miss <- complete.cases(online_pivot)
online_pivot <- online_pivot[non_miss,]
# create categorical variable representing the x-y position
offline_pivot$posXY = paste(offline_pivot$posX, offline_pivot$posY, sep = "-")
online_pivot$posXY = paste(online_pivot$posX, online_pivot$posY, sep = "-")
online_pivot$posXY_round = paste(round(online_pivot$posX,digits=0), round(online_pivot$posY,digits=0), sep = "-")
# create 2 datasets for the 2 different models, offline_pivot has posXY and offline_pivot_x_y has both posX and posY
offline_pivot_x_y = offline_pivot
# remove variables that are not needed for modeling
offline_pivot = select(offline_pivot,-c(time, posX, posY, orientation, rawTime))
offline_pivot_x_y = select(offline_pivot_x_y,-c(time, orientation, rawTime, posXY))
# create 2 datasets for the 2 different models, offline_pivot has posXY and offline_pivot_x_y has both posX and posY
online_pivot_x_y = online_pivot
# remove variables that are not needed for modeling
online_pivot = select(online_pivot,-c(time, posX, posY, orientation, rawTime, posXY))
online_pivot = online_pivot %>% rename(posXY = posXY_round)
online_pivot_x_y = select(online_pivot_x_y,-c(time, orientation, rawTime, posXY_round))
########## Model Code ##########
# all 7 subMacs
set.seed(123)
trControl <- trainControl(method  = "cv", number  = 5)
fit <- train(posXY ~ .,
method     = "knn",
tuneGrid   = expand.grid(k = seq(3,11,2)),
trControl  = trControl,
data       = offline_pivot,
preProcess = c("center","scale")
)
predict = predict(fit, newdata=online_pivot, type="raw")
accuracy <- as.data.frame(cbind(predict=as.factor(predict),actual=as.factor(online_pivot$posXY)))
accuracy$correct <- accuracy[,1] == accuracy[,2]
acc = sum(accuracy$correct)/length(accuracy$correct)
predict_split = str_split(predict, "-", simplify=TRUE)
actual_split = str_split(online_pivot$posXY, "-", simplify=TRUE)
mean_distance_error = mean(sqrt((as.numeric(predict_split[,1]) - as.numeric(actual_split[,1]))^2+(as.numeric(predict_split[,2]) - as.numeric(actual_split[,2]))^2))
# 6 subMacs, including 00:0f:a3:39:dd:cd
offline_pivot_6 = select(offline_pivot,-c(`00:0f:a3:39:e1:c0`))
online_pivot_6 = select(online_pivot,-c(`00:0f:a3:39:e1:c0`))
set.seed(123)
trControl <- trainControl(method  = "cv", number  = 5)
fit <- train(posXY ~ .,
method     = "knn",
tuneGrid   = expand.grid(k = seq(3,11,2)),
trControl  = trControl,
data       = offline_pivot_6,
preProcess = c("center","scale")
)
predict = predict(fit, newdata=online_pivot_6, type="raw")
accuracy <- as.data.frame(cbind(predict=as.factor(predict),actual=as.factor(online_pivot_6$posXY)))
accuracy$correct <- accuracy[,1] == accuracy[,2]
acc = sum(accuracy$correct)/length(accuracy$correct)
predict_split = str_split(predict, "-", simplify=TRUE)
actual_split = str_split(online_pivot_6$posXY, "-", simplify=TRUE)
mean_distance_error = mean(sqrt((as.numeric(predict_split[,1]) - as.numeric(actual_split[,1]))^2+(as.numeric(predict_split[,2]) - as.numeric(actual_split[,2]))^2))
# 6 subMacs, including 00:0f:a3:39:e1:c0
offline_pivot_6 = select(offline_pivot,-c(`00:0f:a3:39:dd:cd`))
online_pivot_6 = select(online_pivot,-c(`00:0f:a3:39:dd:cd`))
set.seed(123)
trControl <- trainControl(method  = "cv", number  = 5)
fit <- train(posXY ~ .,
method     = "knn",
tuneGrid   = expand.grid(k = seq(3,11,2)),
trControl  = trControl,
data       = offline_pivot_6,
preProcess = c("center","scale")
)
predict = predict(fit, newdata=online_pivot_6, type="raw")
accuracy <- as.data.frame(cbind(predict=as.factor(predict),actual=as.factor(online_pivot_6$posXY)))
accuracy$correct <- accuracy[,1] == accuracy[,2]
acc = sum(accuracy$correct)/length(accuracy$correct)
predict_split = str_split(predict, "-", simplify=TRUE)
actual_split = str_split(online_pivot_6$posXY, "-", simplify=TRUE)
mean_distance_error = mean(sqrt((as.numeric(predict_split[,1]) - as.numeric(actual_split[,1]))^2+(as.numeric(predict_split[,2]) - as.numeric(actual_split[,2]))^2))
# multi-target regression
# all 7 subMacs
X <- cbind(offline_pivot_x_y$`00:14:bf:b1:97:8a`,offline_pivot_x_y$`00:14:bf:b1:97:90`,offline_pivot_x_y$`00:0f:a3:39:e1:c0`,offline_pivot_x_y$`00:14:bf:b1:97:8d`,
offline_pivot_x_y$`00:14:bf:b1:97:81`,offline_pivot_x_y$`00:14:bf:3b:c7:c6`,offline_pivot_x_y$`00:0f:a3:39:dd:cd`,offline_pivot_x_y$angle)
fit <- plsr(cbind(posX,posY) ~ X, ncomp=8, data=offline_pivot_x_y, scale=TRUE, validation="CV", segments=5)
mean_distance_error <- mean(sqrt(fit$residuals[1]^2+fit$residuals[2]^2))
newX <- cbind(online_pivot_x_y$`00:14:bf:b1:97:8a`,online_pivot_x_y$`00:14:bf:b1:97:90`,online_pivot_x_y$`00:0f:a3:39:e1:c0`,online_pivot_x_y$`00:14:bf:b1:97:8d`,
online_pivot_x_y$`00:14:bf:b1:97:81`,online_pivot_x_y$`00:14:bf:3b:c7:c6`,online_pivot_x_y$`00:0f:a3:39:dd:cd`,online_pivot_x_y$angle)
predict <- predict(fit, ncomp=8, newdata=newX)
predict_df <- as.data.frame(predict)
names(predict_df) = c("posX","posY")
mean_distance_error <- mean(sqrt((online_pivot_x_y$posX - predict_df$posX)^2+(online_pivot_x_y$posY - predict_df$posY)^2))
# 6 subMacs, including 00:0f:a3:39:dd:cd
X <- cbind(offline_pivot_x_y$`00:14:bf:b1:97:8a`,offline_pivot_x_y$`00:14:bf:b1:97:90`,offline_pivot_x_y$`00:14:bf:b1:97:8d`,
offline_pivot_x_y$`00:14:bf:b1:97:81`,offline_pivot_x_y$`00:14:bf:3b:c7:c6`,offline_pivot_x_y$`00:0f:a3:39:dd:cd`,offline_pivot_x_y$angle)
fit <- plsr(cbind(posX,posY) ~ X, ncomp=7, data=offline_pivot_x_y, scale=TRUE, validation="CV", segments=5)
mean_distance_error <- mean(sqrt(fit$residuals[1]^2+fit$residuals[2]^2))
newX <- cbind(online_pivot_x_y$`00:14:bf:b1:97:8a`,online_pivot_x_y$`00:14:bf:b1:97:90`,online_pivot_x_y$`00:14:bf:b1:97:8d`,
online_pivot_x_y$`00:14:bf:b1:97:81`,online_pivot_x_y$`00:14:bf:3b:c7:c6`,online_pivot_x_y$`00:0f:a3:39:dd:cd`,online_pivot_x_y$angle)
predict <- predict(fit, ncomp=7, newdata=newX)
predict_df <- as.data.frame(predict)
names(predict_df) = c("posX","posY")
mean_distance_error <- mean(sqrt((online_pivot_x_y$posX - predict_df$posX)^2+(online_pivot_x_y$posY - predict_df$posY)^2))
# 6 subMacs, including 00:0f:a3:39:e1:c0
X <- cbind(offline_pivot_x_y$`00:14:bf:b1:97:8a`,offline_pivot_x_y$`00:14:bf:b1:97:90`,offline_pivot_x_y$`00:0f:a3:39:e1:c0`,offline_pivot_x_y$`00:14:bf:b1:97:8d`,
offline_pivot_x_y$`00:14:bf:b1:97:81`,offline_pivot_x_y$`00:14:bf:3b:c7:c6`,offline_pivot_x_y$angle)
fit <- plsr(cbind(posX,posY) ~ X, ncomp=7, data=offline_pivot_x_y, scale=TRUE, validation="CV", segments=5)
mean_distance_error <- mean(sqrt(fit$residuals[1]^2+fit$residuals[2]^2))
newX <- cbind(online_pivot_x_y$`00:14:bf:b1:97:8a`,online_pivot_x_y$`00:14:bf:b1:97:90`,online_pivot_x_y$`00:0f:a3:39:e1:c0`,online_pivot_x_y$`00:14:bf:b1:97:8d`,
online_pivot_x_y$`00:14:bf:b1:97:81`,online_pivot_x_y$`00:14:bf:3b:c7:c6`,online_pivot_x_y$angle)
predict <- predict(fit, ncomp=7, newdata=newX)
predict_df <- as.data.frame(predict)
names(predict_df) = c("posX","posY")
mean_distance_error <- mean(sqrt((online_pivot_x_y$posX - predict_df$posX)^2+(online_pivot_x_y$posY - predict_df$posY)^2))
# this is the first non-comment line in the text file
txt = readLines('../Data/offline.final.trace.txt')
txt[4]
caption <- "Figure 1: This plot shows where missing data is located in the data. We can see that some observations are missing signal strength for some access points"
caption <- paste0(strwrap(caption, 80), sep="", collapse="\n")
p + labs(title="Figure 1: Missing Signal Strength Data",
caption= caption) +
theme(plot.caption = element_text(hjust = 0, face= "italic"),
legend.title = element_blank()) +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))
caption <- "Figure 1: This plot shows where missing data is located in the data. We can see that some observations are missing signal strength for some access points"
caption <- paste0(strwrap(caption, 80), sep="", collapse="\n")
miss_plot + labs(title="Figure 1: Missing Signal Strength Data",
caption= caption) +
theme(plot.caption = element_text(hjust = 0, face= "italic"),
legend.title = element_blank()) +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))
library(readr)
parkin <- read_csv("C:/Users/William/Downloads/parkinsons_updrs.data")
View(parkin)
library(caret)
library(glmnet)
?caret
?train
train.control <- trainControl(method = "cv", number = 10)
tune.grid <- c(alpha=seq(0,1,0.01), lambda=c(c(1,2,5) %o% 10^(-1:-7)))
tune.grid <- expand.grid(alpha=seq(0,1,0.01), lambda=c(c(1,2,5) %o% 10^(-1:-7)))
fit <- train(total_UPDRS ~ ., data=parkin, tuneGrid = tune.grid, trControl = train.control)
fit <- train(total_UPDRS ~ ., data=parkin, method = "glmnet", tuneGrid = tune.grid, trControl = train.control)
fit
fit <- train(total_UPDRS ~ ., data=select(parkin, -c(motor_UPDRS)), method = "glmnet", tuneGrid = tune.grid, trControl = train.control)
fit
fit <- train(motor_UPDRS~ ., data=select(parkin, -c(total_UPDRS)), method = "glmnet", tuneGrid = tune.grid, trControl = train.control)
fit
?glmnet
fit <- train(motor_UPDRS~ ., data=select(parkin, -c(total_UPDRS)), method = "glmnet", tuneGrid = tune.grid, trControl = train.control, family = "gaussian")
fit <- train(motor_UPDRS~ ., data=select(parkin, -c(total_UPDRS)), method = "glmnet", tuneGrid = tune.grid, trControl = train.control, family = "poisson")
setwd("C:/Users/William/OneDrive/MSDS_7331_QTW/cherry")
setwd("C:/Users/William/OneDrive/MSDS_7331_QTW/cherry")
library(XML)
ubase = "http://www.cherryblossom.org/"
menURLs =
c("results/1999/cb99m.html", "results/2000/Cb003m.htm", "results/2001/oof_m.html",
"results/2002/oofm.htm", "results/2003/CB03-M.HTM",
"results/2004/men.htm", "results/2005/CB05-M.htm",
"results/2006/men.htm", "results/2007/men.htm",
"results/2008/men.htm", "results/2009/09cucb-M.htm",
"results/2010/2010cucb10m-m.htm",
"results/2011/2011cucb10m-m.htm",
"results/2012/2012cucb10m-m.htm")
urls = paste(ubase, menURLs, sep = "")
urls[1:3]
extractResTable =
# takes a list of websites from the cherry blossom race
# a list of years corresponding to the year the result is for
# and the gender of the participant
# Retrieve data from web site,
# find the preformatted text,
# and write lines or return as a character vector.
# returns a list of strings corrsponding to lines in the web url
function(url = "http://www.cherryblossom.org/results/2009/09cucb-F.htm",
year = 1999, sex = "male", file = NULL)
{
doc = htmlParse(url)
if (year == 2000) {
# Get preformatted text from 4th font element
# The top file is ill formed so the <pre> search doesn't work.
ff = getNodeSet(doc, "//font")
txt = xmlValue(ff[[4]])
els = strsplit(txt, "\r\n")[[1]]
}
else if (year == 2009 & sex == "male") {
# Get preformatted text from <div class="Section1"> element
# Each line of results is in a <pre> element
div1 = getNodeSet(doc, "//div[@class='Section1']")
pres = getNodeSet(div1[[1]], "//pre")
els = sapply(pres, xmlValue)
els = gsub("Â", " ", els)
}
else if (year == 1999) {
# Get preformatted text from <pre> elements
pres = getNodeSet(doc, "//pre")
txt = xmlValue(pres[[1]])
els = strsplit(txt, "\n")[[1]]
}
else {
# Get preformatted text from <pre> elements
pres = getNodeSet(doc, "//pre")
txt = xmlValue(pres[[1]])
els = strsplit(txt, "\r\n")[[1]]
}
if (is.null(file)) return(els)
# Write the lines as a text file.
writeLines(els, con = file)
}
years = 1999:2012
menTables = mapply(extractResTable, url = urls, year = years)
names(menTables) = years
sapply(menTables, length)
#womenTables = mapply(extractResTable, url = urls,
#                       year = years, sex = rep("female", 14))
#names(womenTables) = years
#sapply(womenTables, length)
save(menTables, file = "CBMenTextTables.rda")
extractResTable(url = urls[11], year = 2009, sex = 'male')
womenURLs =
c("results/1999/cb99f.html", "results/2000/Cb003f.htm", "results/2001/oof_f.html",
"results/2002/ooff.htm", "results/2003/CB03-F.HTM",
"results/2004/women.htm", "results/2005/CB05-F.htm",
"results/2006/women.htm", "results/2007/women.htm",
"results/2008/women.htm", "results/2009/09cucb-F.htm",
"results/2010/2010cucb10m-F.htm",
"results/2011/2011cucb10m-F.htm",
"results/2012/2012cucb10m-F.htm")
years = 1999:2012
urls = paste(ubase, womenURLs, sep = "")
urls[1:3]
womenTables = mapply(extractResTable, url = urls, year = years, sex='female')
names(womenTables) = years
sapply(womenTables, length)
tail(menTables$'2009')
years = 1999:2012
urls = paste(ubase, womenURLs, sep = "")
urls[1:3]
womenTables = mapply(extractResTable, url = urls, year = years, sex='female')
names(womenTables) = years
sapply(womenTables, length)
save(womenTables, file = "CBWomenTextTables.rda")
setwd("C:/Users/William/OneDrive/MSDS_7331_QTW/cherry")
mensTables <- load(file = "CBMenTextTables.rda")
setwd("C:/Users/William/OneDrive/MSDS_7331_QTW/cherry")
mensTables <- load(file = "CBMenTextTables.rda")
setwd("C:/Users/William/OneDrive/MSDS_7331_QTW/cherry")
mensTables <- load(file = "CBMenTextTables.rda")
womensTables <- load(file = "CBWomenTextTables.rda")
mensTables[-1]
mensTables[14]
mensTables[[14]]
mensTables[[13]]
mensTables[[1]]
mensTables[[2]]
mensTables[2012]
mensTables[[2012]]
mensTables[[1]]
setwd("C:/Users/William/OneDrive/MSDS_7331_QTW/cherry")
mensTables <- load(file = "CBMenTextTables.rda")
womensTables <- load(file = "CBWomenTextTables.rda")
setwd("C:/Users/William/OneDrive/MSDS_7331_QTW/cherry")
load(file = "CBMenTextTables.rda")
load(file = "CBWomenTextTables.rda")
menTables$2012
menTables$"2012"
head(menTables$"2012")
class(menTables)
dim(menTables)
str(menTables)
## I would reccommend setting this to your working directory
setwd("C:/Users/William/OneDrive/MSDS_7331_QTW/cherry")
## Load the mens tables
load(file = "CBMenTextTables.rda")
print("List menTables read in")
## Load the womens tables
load(file = "CBWomenTextTables.rda")
print("List womenTables read in")
els = menTables$"2012"
els[1:10]
els2011 = menTables$"2011"
els2011[1:10]
eqIndex = grep("^===", els)
eqIndex
eqIndex = grep("^===", els)
eqIndex
first3 = substr(els, 1, 3)
which(first3 == "===")
eqIndex = grep("^===", els)
eqIndex
first3 = substr(els, 1, 3)
which(first3 == "===")
spacerRow = els[eqIndex]
headerRow = els[eqIndex - 1]
body = els[ -(1:eqIndex) ]
headerRow = tolower(headerRow)
ageStart = regexpr("ag", headerRow)
ageStart
age = substr(body, start = ageStart, stop = ageStart + 1)
head(age)
ageStart = regexpr("ag", headerRow)
#ageStart
#age = substr(body, start = ageStart, stop = ageStart + 1)
#head(age)
ageStart = regexpr("ag", headerRow)
ageStart
#age = substr(body, start = ageStart, stop = ageStart + 1)
#head(age)
ageStart
ageStart*1
ageStart = regexpr("ag", headerRow)
ageStart
age = substr(body, start = ageStart, stop = ageStart + 1)
#head(age)
ageStart = regexpr("ag", headerRow)
ageStart
age = substr(body, start = ageStart, stop = ageStart + 1)
head(age)
?regexpr
ageStart = regexpr("ag", headerRow)
ageStart
age = substr(body, start = ageStart, stop = ageStart + 1)
head(age)
summary(as.numeric(age))
blankLocs = gregexpr(" ", spacerRow)
blankLocs
blankLocs = gregexpr(" ", spacerRow)
blankLocs
searchLocs = c(0, blankLocs[[1]])
blankLocs = gregexpr(" ", spacerRow)
blankLocs
searchLocs = c(0, blankLocs[[1]])
Values = mapply(substr, list(body),
start = searchLocs[ -length(searchLocs)] + 1,
stop = searchLocs[ -1 ] - 1)
setwd("C:/Users/William/OneDrive/MSDS_7331_QTW/cherry")
library(XML)
ubase = "http://www.cherryblossom.org/"
menURLs =
c("results/1999/cb99m.html", "results/2000/Cb003m.htm", "results/2001/oof_m.html",
"results/2002/oofm.htm", "results/2003/CB03-M.HTM",
"results/2004/men.htm", "results/2005/CB05-M.htm",
"results/2006/men.htm", "results/2007/men.htm",
"results/2008/men.htm", "results/2009/09cucb-M.htm",
"results/2010/2010cucb10m-m.htm",
"results/2011/2011cucb10m-m.htm",
"results/2012/2012cucb10m-m.htm")
urls = paste(ubase, menURLs, sep = "")
urls[1:3]
extractResTable =
# takes a list of websites from the cherry blossom race
# a list of years corresponding to the year the result is for
# and the gender of the participant
# Retrieve data from web site,
# find the preformatted text,
# and write lines or return as a character vector.
# returns a list of strings corrsponding to lines in the web url
function(url = "http://www.cherryblossom.org/results/2009/09cucb-F.htm",
year = 1999, sex = "male", file = NULL)
{
doc = htmlParse(url)
if (year == 2000) {
# Get preformatted text from 4th font element
# The top file is ill formed so the <pre> search doesn't work.
ff = getNodeSet(doc, "//font")
txt = xmlValue(ff[[4]])
els = strsplit(txt, "\r\n")[[1]]
}
else if (year == 2009 & sex == "male") {
# Get preformatted text from <div class="Section1"> element
# Each line of results is in a <pre> element
div1 = getNodeSet(doc, "//div[@class='Section1']")
pres = getNodeSet(div1[[1]], "//pre")
els = sapply(pres, xmlValue)
els = gsub("Â", " ", els)
}
else if (year == 1999) {
# Get preformatted text from <pre> elements
pres = getNodeSet(doc, "//pre")
txt = xmlValue(pres[[1]])
els = strsplit(txt, "\n")[[1]]
}
else {
# Get preformatted text from <pre> elements
pres = getNodeSet(doc, "//pre")
txt = xmlValue(pres[[1]])
els = strsplit(txt, "\r\n")[[1]]
}
if (is.null(file)) return(els)
# Write the lines as a text file.
writeLines(els, con = file)
}
years = 1999:2012
menTables = mapply(extractResTable, url = urls, year = years)
names(menTables) = years
sapply(menTables, length)
#womenTables = mapply(extractResTable, url = urls,
#                       year = years, sex = rep("female", 14))
#names(womenTables) = years
#sapply(womenTables, length)
save(menTables, file = "CBMenTextTables.rda")
extractResTable(url = urls[11], year = 2009, sex = 'male')
womenURLs =
c("results/1999/cb99f.html", "results/2000/Cb003f.htm", "results/2001/oof_f.html",
"results/2002/ooff.htm", "results/2003/CB03-F.HTM",
"results/2004/women.htm", "results/2005/CB05-F.htm",
"results/2006/women.htm", "results/2007/women.htm",
"results/2008/women.htm", "results/2009/09cucb-F.htm",
"results/2010/2010cucb10m-F.htm",
"results/2011/2011cucb10m-F.htm",
"results/2012/2012cucb10m-F.htm")
years = 1999:2012
urls = paste(ubase, womenURLs, sep = "")
urls[1:3]
womenTables = mapply(extractResTable, url = urls, year = years, sex='female')
names(womenTables) = years
sapply(womenTables, length)
save(womenTables, file = "CBWomenTextTables.rda")
tail(menTables$'2009')
