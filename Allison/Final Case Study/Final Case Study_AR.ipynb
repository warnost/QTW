{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Final Case Study\n",
    "MSDS 7333 Quantifying the World  \n",
    "*Allison Roderick, Jenna Ford, and Will Arnost* \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<a href='#Section_1'> 1. Introduction </a>  \n",
    "<a href='#Section_2'> 2. Question </a>  \n",
    "<a href='#Section_3'> 3. Methods </a>  \n",
    "<a href='#Section_3_a'> &nbsp;&nbsp;&nbsp; a. Dataset </a>  \n",
    "<a href='#Section_3_b'> &nbsp;&nbsp;&nbsp; b. Neural Network Structure </a>  \n",
    "<a href='#Section_3_c'> &nbsp;&nbsp;&nbsp; c. Other Considerations </a>  \n",
    "<a href='#Section_4'> 4. Modeling </a>  \n",
    "<a href='#Section_5'> 5. Results </a>  \n",
    "<a href='#Section_6'> 6. Conclusion </a>  \n",
    "<a href='#Section_7'> 7. References </a>  \n",
    "<a href='#Section_8'> 8. Code </a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's case study involves replicating results produced in the paper \"Searching for Exotic Particles in High-Energy Physics with Deep Learning\" by Baldi, Sadowski, and Whiteson [1]. The 2014 paper looks to distinguish between particle collisions that produce exotic particles and those that do not. The authors investigate the use of deep neural networks to improve accuracy over other methods. \n",
    "\n",
    "We will attempt to replicate that paper's neural network architecture and performance. The packages used in the paper are outdated, so we will be using Tensorflow to build our network. We hope to get as close to their AUC of 0.885 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section gives an overview of what we know about the data and how we prepared the dataset for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_3_a'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_3_b'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Neural Network Stucture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_3_c'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Section_8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../final_project.csv\")\n",
    "df = pd.read_csv(\"./Data/final_project.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are missing values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160000 entries, 0 to 159999\n",
      "Data columns (total 51 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   x0      159974 non-null  float64\n",
      " 1   x1      159975 non-null  float64\n",
      " 2   x2      159962 non-null  float64\n",
      " 3   x3      159963 non-null  float64\n",
      " 4   x4      159974 non-null  float64\n",
      " 5   x5      159963 non-null  float64\n",
      " 6   x6      159974 non-null  float64\n",
      " 7   x7      159973 non-null  float64\n",
      " 8   x8      159979 non-null  float64\n",
      " 9   x9      159970 non-null  float64\n",
      " 10  x10     159957 non-null  float64\n",
      " 11  x11     159970 non-null  float64\n",
      " 12  x12     159964 non-null  float64\n",
      " 13  x13     159969 non-null  float64\n",
      " 14  x14     159966 non-null  float64\n",
      " 15  x15     159965 non-null  float64\n",
      " 16  x16     159974 non-null  float64\n",
      " 17  x17     159973 non-null  float64\n",
      " 18  x18     159960 non-null  float64\n",
      " 19  x19     159965 non-null  float64\n",
      " 20  x20     159962 non-null  float64\n",
      " 21  x21     159971 non-null  float64\n",
      " 22  x22     159973 non-null  float64\n",
      " 23  x23     159953 non-null  float64\n",
      " 24  x24     159972 non-null  object \n",
      " 25  x25     159978 non-null  float64\n",
      " 26  x26     159964 non-null  float64\n",
      " 27  x27     159970 non-null  float64\n",
      " 28  x28     159965 non-null  float64\n",
      " 29  x29     159970 non-null  object \n",
      " 30  x30     159970 non-null  object \n",
      " 31  x31     159961 non-null  float64\n",
      " 32  x32     159969 non-null  object \n",
      " 33  x33     159959 non-null  float64\n",
      " 34  x34     159959 non-null  float64\n",
      " 35  x35     159970 non-null  float64\n",
      " 36  x36     159973 non-null  float64\n",
      " 37  x37     159977 non-null  object \n",
      " 38  x38     159969 non-null  float64\n",
      " 39  x39     159977 non-null  float64\n",
      " 40  x40     159964 non-null  float64\n",
      " 41  x41     159960 non-null  float64\n",
      " 42  x42     159974 non-null  float64\n",
      " 43  x43     159963 non-null  float64\n",
      " 44  x44     159960 non-null  float64\n",
      " 45  x45     159971 non-null  float64\n",
      " 46  x46     159969 non-null  float64\n",
      " 47  x47     159963 non-null  float64\n",
      " 48  x48     159968 non-null  float64\n",
      " 49  x49     159968 non-null  float64\n",
      " 50  y       160000 non-null  int64  \n",
      "dtypes: float64(45), int64(1), object(5)\n",
      "memory usage: 62.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159974.000000</td>\n",
       "      <td>159975.000000</td>\n",
       "      <td>159962.000000</td>\n",
       "      <td>159963.000000</td>\n",
       "      <td>159974.000000</td>\n",
       "      <td>159963.000000</td>\n",
       "      <td>159974.000000</td>\n",
       "      <td>159973.000000</td>\n",
       "      <td>159979.000000</td>\n",
       "      <td>159970.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>159960.000000</td>\n",
       "      <td>159974.000000</td>\n",
       "      <td>159963.000000</td>\n",
       "      <td>159960.000000</td>\n",
       "      <td>159971.000000</td>\n",
       "      <td>159969.000000</td>\n",
       "      <td>159963.000000</td>\n",
       "      <td>159968.000000</td>\n",
       "      <td>159968.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.001028</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>-1.150145</td>\n",
       "      <td>-0.024637</td>\n",
       "      <td>-0.000549</td>\n",
       "      <td>0.013582</td>\n",
       "      <td>-1.670670</td>\n",
       "      <td>-7.692795</td>\n",
       "      <td>-0.030540</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>...</td>\n",
       "      <td>6.701076</td>\n",
       "      <td>-1.833820</td>\n",
       "      <td>-0.002091</td>\n",
       "      <td>-0.006250</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-12.755395</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>-0.674224</td>\n",
       "      <td>0.401231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.371137</td>\n",
       "      <td>6.340632</td>\n",
       "      <td>13.273480</td>\n",
       "      <td>8.065032</td>\n",
       "      <td>6.382293</td>\n",
       "      <td>7.670076</td>\n",
       "      <td>19.298665</td>\n",
       "      <td>30.542264</td>\n",
       "      <td>8.901185</td>\n",
       "      <td>6.355040</td>\n",
       "      <td>...</td>\n",
       "      <td>18.680196</td>\n",
       "      <td>5.110705</td>\n",
       "      <td>1.534952</td>\n",
       "      <td>4.164595</td>\n",
       "      <td>0.396621</td>\n",
       "      <td>36.608641</td>\n",
       "      <td>4.788157</td>\n",
       "      <td>1.935501</td>\n",
       "      <td>15.036738</td>\n",
       "      <td>0.490149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.592635</td>\n",
       "      <td>-26.278302</td>\n",
       "      <td>-59.394048</td>\n",
       "      <td>-35.476594</td>\n",
       "      <td>-28.467536</td>\n",
       "      <td>-33.822988</td>\n",
       "      <td>-86.354483</td>\n",
       "      <td>-181.506976</td>\n",
       "      <td>-37.691045</td>\n",
       "      <td>-27.980659</td>\n",
       "      <td>...</td>\n",
       "      <td>-82.167224</td>\n",
       "      <td>-27.933750</td>\n",
       "      <td>-6.876234</td>\n",
       "      <td>-17.983487</td>\n",
       "      <td>-1.753221</td>\n",
       "      <td>-201.826828</td>\n",
       "      <td>-21.086333</td>\n",
       "      <td>-8.490155</td>\n",
       "      <td>-65.791191</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.251641</td>\n",
       "      <td>-4.260973</td>\n",
       "      <td>-10.166536</td>\n",
       "      <td>-5.454438</td>\n",
       "      <td>-4.313118</td>\n",
       "      <td>-5.148130</td>\n",
       "      <td>-14.780146</td>\n",
       "      <td>-27.324771</td>\n",
       "      <td>-6.031058</td>\n",
       "      <td>-4.260619</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.804080</td>\n",
       "      <td>-5.162869</td>\n",
       "      <td>-1.039677</td>\n",
       "      <td>-2.812055</td>\n",
       "      <td>-0.266518</td>\n",
       "      <td>-36.428329</td>\n",
       "      <td>-3.216016</td>\n",
       "      <td>-1.320800</td>\n",
       "      <td>-10.931753</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.002047</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>-1.340932</td>\n",
       "      <td>-0.031408</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>-1.948594</td>\n",
       "      <td>-6.956789</td>\n",
       "      <td>-0.016840</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>...</td>\n",
       "      <td>6.840110</td>\n",
       "      <td>-1.923754</td>\n",
       "      <td>-0.004385</td>\n",
       "      <td>-0.010484</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>-12.982497</td>\n",
       "      <td>0.035865</td>\n",
       "      <td>-0.011993</td>\n",
       "      <td>-0.574410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.248532</td>\n",
       "      <td>4.284220</td>\n",
       "      <td>7.871676</td>\n",
       "      <td>5.445179</td>\n",
       "      <td>4.306660</td>\n",
       "      <td>5.190749</td>\n",
       "      <td>11.446931</td>\n",
       "      <td>12.217071</td>\n",
       "      <td>5.972349</td>\n",
       "      <td>4.305734</td>\n",
       "      <td>...</td>\n",
       "      <td>19.266367</td>\n",
       "      <td>1.453507</td>\n",
       "      <td>1.033275</td>\n",
       "      <td>2.783274</td>\n",
       "      <td>0.269049</td>\n",
       "      <td>11.445443</td>\n",
       "      <td>3.268028</td>\n",
       "      <td>1.317703</td>\n",
       "      <td>9.651072</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.600849</td>\n",
       "      <td>27.988178</td>\n",
       "      <td>63.545653</td>\n",
       "      <td>38.906025</td>\n",
       "      <td>26.247812</td>\n",
       "      <td>35.550110</td>\n",
       "      <td>92.390605</td>\n",
       "      <td>149.150634</td>\n",
       "      <td>39.049831</td>\n",
       "      <td>27.377842</td>\n",
       "      <td>...</td>\n",
       "      <td>100.050432</td>\n",
       "      <td>22.668041</td>\n",
       "      <td>6.680922</td>\n",
       "      <td>19.069759</td>\n",
       "      <td>1.669205</td>\n",
       "      <td>150.859415</td>\n",
       "      <td>20.836854</td>\n",
       "      <td>8.226552</td>\n",
       "      <td>66.877604</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x0             x1             x2             x3  \\\n",
       "count  159974.000000  159975.000000  159962.000000  159963.000000   \n",
       "mean       -0.001028       0.001358      -1.150145      -0.024637   \n",
       "std         0.371137       6.340632      13.273480       8.065032   \n",
       "min        -1.592635     -26.278302     -59.394048     -35.476594   \n",
       "25%        -0.251641      -4.260973     -10.166536      -5.454438   \n",
       "50%        -0.002047       0.004813      -1.340932      -0.031408   \n",
       "75%         0.248532       4.284220       7.871676       5.445179   \n",
       "max         1.600849      27.988178      63.545653      38.906025   \n",
       "\n",
       "                  x4             x5             x6             x7  \\\n",
       "count  159974.000000  159963.000000  159974.000000  159973.000000   \n",
       "mean       -0.000549       0.013582      -1.670670      -7.692795   \n",
       "std         6.382293       7.670076      19.298665      30.542264   \n",
       "min       -28.467536     -33.822988     -86.354483    -181.506976   \n",
       "25%        -4.313118      -5.148130     -14.780146     -27.324771   \n",
       "50%         0.000857       0.014118      -1.948594      -6.956789   \n",
       "75%         4.306660       5.190749      11.446931      12.217071   \n",
       "max        26.247812      35.550110      92.390605     149.150634   \n",
       "\n",
       "                  x8             x9  ...            x41            x42  \\\n",
       "count  159979.000000  159970.000000  ...  159960.000000  159974.000000   \n",
       "mean       -0.030540       0.005462  ...       6.701076      -1.833820   \n",
       "std         8.901185       6.355040  ...      18.680196       5.110705   \n",
       "min       -37.691045     -27.980659  ...     -82.167224     -27.933750   \n",
       "25%        -6.031058      -4.260619  ...      -5.804080      -5.162869   \n",
       "50%        -0.016840       0.006045  ...       6.840110      -1.923754   \n",
       "75%         5.972349       4.305734  ...      19.266367       1.453507   \n",
       "max        39.049831      27.377842  ...     100.050432      22.668041   \n",
       "\n",
       "                 x43            x44            x45            x46  \\\n",
       "count  159963.000000  159960.000000  159971.000000  159969.000000   \n",
       "mean       -0.002091      -0.006250       0.000885     -12.755395   \n",
       "std         1.534952       4.164595       0.396621      36.608641   \n",
       "min        -6.876234     -17.983487      -1.753221    -201.826828   \n",
       "25%        -1.039677      -2.812055      -0.266518     -36.428329   \n",
       "50%        -0.004385      -0.010484       0.001645     -12.982497   \n",
       "75%         1.033275       2.783274       0.269049      11.445443   \n",
       "max         6.680922      19.069759       1.669205     150.859415   \n",
       "\n",
       "                 x47            x48            x49              y  \n",
       "count  159963.000000  159968.000000  159968.000000  160000.000000  \n",
       "mean        0.028622      -0.000224      -0.674224       0.401231  \n",
       "std         4.788157       1.935501      15.036738       0.490149  \n",
       "min       -21.086333      -8.490155     -65.791191       0.000000  \n",
       "25%        -3.216016      -1.320800     -10.931753       0.000000  \n",
       "50%         0.035865      -0.011993      -0.574410       0.000000  \n",
       "75%         3.268028       1.317703       9.651072       1.000000  \n",
       "max        20.836854       8.226552      66.877604       1.000000  \n",
       "\n",
       "[8 rows x 46 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling\n",
    "profile = df.profile_report(title=\"Final Jeopardy\",pool_size=4)\n",
    "profile.to_file(\"final_jeopardy.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Numeric columns that imported as string\n",
    "df['x32'] = df['x32'].replace('[\\%,]', '', regex=True).astype(float)/100\n",
    "df['x37'] = df['x37'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "df['x29'] = df['x29'].replace('Dev', 'Dec', regex=True)\n",
    "df['x29'] = df['x29'].replace('July', 'Jul', regex=True)\n",
    "df['x29'] = df['x29'].replace('January', 'Jan', regex=True)\n",
    "df['x29'] = df['x29'].replace('sept.', 'Sep', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.166563</td>\n",
       "      <td>-3.961588</td>\n",
       "      <td>4.621113</td>\n",
       "      <td>2.481908</td>\n",
       "      <td>-1.800135</td>\n",
       "      <td>0.804684</td>\n",
       "      <td>6.718751</td>\n",
       "      <td>-14.789997</td>\n",
       "      <td>-1.040673</td>\n",
       "      <td>-4.204950</td>\n",
       "      <td>6.187465</td>\n",
       "      <td>13.251523</td>\n",
       "      <td>25.665413</td>\n",
       "      <td>-5.017267</td>\n",
       "      <td>10.503714</td>\n",
       "      <td>-2.517678</td>\n",
       "      <td>2.117910</td>\n",
       "      <td>5.865923</td>\n",
       "      <td>-6.666158</td>\n",
       "      <td>1.791497</td>\n",
       "      <td>-1.909114</td>\n",
       "      <td>-1.737940</td>\n",
       "      <td>-2.516715</td>\n",
       "      <td>3.553013</td>\n",
       "      <td>euorpe</td>\n",
       "      <td>-0.801340</td>\n",
       "      <td>1.142950</td>\n",
       "      <td>1.005131</td>\n",
       "      <td>-18.473784</td>\n",
       "      <td>Jul</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>-3.851669</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.940031</td>\n",
       "      <td>-5.492063</td>\n",
       "      <td>0.627121</td>\n",
       "      <td>-0.873824</td>\n",
       "      <td>1313.96</td>\n",
       "      <td>-1.353729</td>\n",
       "      <td>-5.186148</td>\n",
       "      <td>-10.612200</td>\n",
       "      <td>-1.497117</td>\n",
       "      <td>5.414063</td>\n",
       "      <td>-2.325655</td>\n",
       "      <td>1.674827</td>\n",
       "      <td>-0.264332</td>\n",
       "      <td>60.781427</td>\n",
       "      <td>-7.689696</td>\n",
       "      <td>0.151589</td>\n",
       "      <td>-8.040166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.149894</td>\n",
       "      <td>-0.585676</td>\n",
       "      <td>27.839856</td>\n",
       "      <td>4.152333</td>\n",
       "      <td>6.426802</td>\n",
       "      <td>-2.426943</td>\n",
       "      <td>40.477058</td>\n",
       "      <td>-6.725709</td>\n",
       "      <td>0.896421</td>\n",
       "      <td>0.330165</td>\n",
       "      <td>-11.708859</td>\n",
       "      <td>-2.352809</td>\n",
       "      <td>-25.014934</td>\n",
       "      <td>9.799608</td>\n",
       "      <td>-10.960705</td>\n",
       "      <td>1.504000</td>\n",
       "      <td>-2.397836</td>\n",
       "      <td>-9.301839</td>\n",
       "      <td>-1.999413</td>\n",
       "      <td>5.045258</td>\n",
       "      <td>-5.809984</td>\n",
       "      <td>10.814319</td>\n",
       "      <td>-0.478112</td>\n",
       "      <td>10.590601</td>\n",
       "      <td>asia</td>\n",
       "      <td>0.818792</td>\n",
       "      <td>-0.642987</td>\n",
       "      <td>0.751086</td>\n",
       "      <td>3.749377</td>\n",
       "      <td>Aug</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>1.391594</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>2.211462</td>\n",
       "      <td>-4.460591</td>\n",
       "      <td>1.035461</td>\n",
       "      <td>0.228270</td>\n",
       "      <td>1962.78</td>\n",
       "      <td>32.816804</td>\n",
       "      <td>-5.150012</td>\n",
       "      <td>2.147427</td>\n",
       "      <td>36.292790</td>\n",
       "      <td>4.490915</td>\n",
       "      <td>0.762561</td>\n",
       "      <td>6.526662</td>\n",
       "      <td>1.007927</td>\n",
       "      <td>15.805696</td>\n",
       "      <td>-4.896678</td>\n",
       "      <td>-0.320283</td>\n",
       "      <td>16.719974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.321707</td>\n",
       "      <td>-1.429819</td>\n",
       "      <td>12.251561</td>\n",
       "      <td>6.586874</td>\n",
       "      <td>-5.304647</td>\n",
       "      <td>-11.311090</td>\n",
       "      <td>17.812850</td>\n",
       "      <td>11.060572</td>\n",
       "      <td>5.325880</td>\n",
       "      <td>-2.632984</td>\n",
       "      <td>1.572647</td>\n",
       "      <td>-4.170771</td>\n",
       "      <td>12.078602</td>\n",
       "      <td>-5.158498</td>\n",
       "      <td>7.302780</td>\n",
       "      <td>-2.192431</td>\n",
       "      <td>-4.065428</td>\n",
       "      <td>-7.675055</td>\n",
       "      <td>4.041629</td>\n",
       "      <td>-6.633628</td>\n",
       "      <td>1.700321</td>\n",
       "      <td>-2.419221</td>\n",
       "      <td>2.467521</td>\n",
       "      <td>-5.270615</td>\n",
       "      <td>asia</td>\n",
       "      <td>-0.718315</td>\n",
       "      <td>-0.566757</td>\n",
       "      <td>4.171088</td>\n",
       "      <td>11.522448</td>\n",
       "      <td>Jul</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-3.262082</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.419607</td>\n",
       "      <td>-3.804056</td>\n",
       "      <td>-0.763357</td>\n",
       "      <td>-1.612561</td>\n",
       "      <td>430.47</td>\n",
       "      <td>-0.333199</td>\n",
       "      <td>8.728585</td>\n",
       "      <td>-0.863137</td>\n",
       "      <td>-0.368491</td>\n",
       "      <td>9.088864</td>\n",
       "      <td>-0.689886</td>\n",
       "      <td>-2.731118</td>\n",
       "      <td>0.754200</td>\n",
       "      <td>30.856417</td>\n",
       "      <td>-7.428573</td>\n",
       "      <td>-2.090804</td>\n",
       "      <td>-7.869421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.245594</td>\n",
       "      <td>5.076677</td>\n",
       "      <td>-24.149632</td>\n",
       "      <td>3.637307</td>\n",
       "      <td>6.505811</td>\n",
       "      <td>2.290224</td>\n",
       "      <td>-35.111751</td>\n",
       "      <td>-18.913592</td>\n",
       "      <td>-0.337041</td>\n",
       "      <td>-5.568076</td>\n",
       "      <td>-2.000255</td>\n",
       "      <td>-19.286668</td>\n",
       "      <td>10.995330</td>\n",
       "      <td>-5.914378</td>\n",
       "      <td>2.511400</td>\n",
       "      <td>1.292362</td>\n",
       "      <td>-2.496882</td>\n",
       "      <td>-15.722954</td>\n",
       "      <td>-2.735382</td>\n",
       "      <td>1.117536</td>\n",
       "      <td>1.923670</td>\n",
       "      <td>-14.179167</td>\n",
       "      <td>1.470625</td>\n",
       "      <td>-11.484431</td>\n",
       "      <td>asia</td>\n",
       "      <td>-0.052430</td>\n",
       "      <td>-0.558582</td>\n",
       "      <td>9.215569</td>\n",
       "      <td>30.595226</td>\n",
       "      <td>Jul</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>-2.285241</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-3.442715</td>\n",
       "      <td>4.420160</td>\n",
       "      <td>1.164532</td>\n",
       "      <td>3.033455</td>\n",
       "      <td>-2366.29</td>\n",
       "      <td>14.188669</td>\n",
       "      <td>-6.385060</td>\n",
       "      <td>12.084421</td>\n",
       "      <td>15.691546</td>\n",
       "      <td>-7.467775</td>\n",
       "      <td>2.940789</td>\n",
       "      <td>-6.424112</td>\n",
       "      <td>0.419776</td>\n",
       "      <td>-72.424569</td>\n",
       "      <td>5.361375</td>\n",
       "      <td>1.806070</td>\n",
       "      <td>-7.670847</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.273366</td>\n",
       "      <td>0.306326</td>\n",
       "      <td>-11.352593</td>\n",
       "      <td>1.676758</td>\n",
       "      <td>2.928441</td>\n",
       "      <td>-0.616824</td>\n",
       "      <td>-16.505817</td>\n",
       "      <td>27.532281</td>\n",
       "      <td>1.199715</td>\n",
       "      <td>-4.309105</td>\n",
       "      <td>6.667530</td>\n",
       "      <td>1.965913</td>\n",
       "      <td>-28.106348</td>\n",
       "      <td>-1.258950</td>\n",
       "      <td>5.759941</td>\n",
       "      <td>0.472584</td>\n",
       "      <td>-1.150097</td>\n",
       "      <td>-14.118709</td>\n",
       "      <td>4.527964</td>\n",
       "      <td>-1.284372</td>\n",
       "      <td>-9.026317</td>\n",
       "      <td>-7.039818</td>\n",
       "      <td>-1.978748</td>\n",
       "      <td>-15.998166</td>\n",
       "      <td>asia</td>\n",
       "      <td>-0.223449</td>\n",
       "      <td>0.350781</td>\n",
       "      <td>1.811182</td>\n",
       "      <td>-4.094084</td>\n",
       "      <td>Jul</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>0.921047</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.431640</td>\n",
       "      <td>12.165494</td>\n",
       "      <td>-0.167726</td>\n",
       "      <td>-0.341604</td>\n",
       "      <td>-620.66</td>\n",
       "      <td>-12.578926</td>\n",
       "      <td>1.133798</td>\n",
       "      <td>30.004727</td>\n",
       "      <td>-13.911297</td>\n",
       "      <td>-5.229937</td>\n",
       "      <td>1.783928</td>\n",
       "      <td>3.957801</td>\n",
       "      <td>-0.096988</td>\n",
       "      <td>-14.085435</td>\n",
       "      <td>-0.208351</td>\n",
       "      <td>-0.894942</td>\n",
       "      <td>15.724742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1         x2        x3        x4         x5         x6  \\\n",
       "0 -0.166563 -3.961588   4.621113  2.481908 -1.800135   0.804684   6.718751   \n",
       "1 -0.149894 -0.585676  27.839856  4.152333  6.426802  -2.426943  40.477058   \n",
       "2 -0.321707 -1.429819  12.251561  6.586874 -5.304647 -11.311090  17.812850   \n",
       "3 -0.245594  5.076677 -24.149632  3.637307  6.505811   2.290224 -35.111751   \n",
       "4 -0.273366  0.306326 -11.352593  1.676758  2.928441  -0.616824 -16.505817   \n",
       "\n",
       "          x7        x8        x9        x10        x11        x12       x13  \\\n",
       "0 -14.789997 -1.040673 -4.204950   6.187465  13.251523  25.665413 -5.017267   \n",
       "1  -6.725709  0.896421  0.330165 -11.708859  -2.352809 -25.014934  9.799608   \n",
       "2  11.060572  5.325880 -2.632984   1.572647  -4.170771  12.078602 -5.158498   \n",
       "3 -18.913592 -0.337041 -5.568076  -2.000255 -19.286668  10.995330 -5.914378   \n",
       "4  27.532281  1.199715 -4.309105   6.667530   1.965913 -28.106348 -1.258950   \n",
       "\n",
       "         x14       x15       x16        x17       x18       x19       x20  \\\n",
       "0  10.503714 -2.517678  2.117910   5.865923 -6.666158  1.791497 -1.909114   \n",
       "1 -10.960705  1.504000 -2.397836  -9.301839 -1.999413  5.045258 -5.809984   \n",
       "2   7.302780 -2.192431 -4.065428  -7.675055  4.041629 -6.633628  1.700321   \n",
       "3   2.511400  1.292362 -2.496882 -15.722954 -2.735382  1.117536  1.923670   \n",
       "4   5.759941  0.472584 -1.150097 -14.118709  4.527964 -1.284372 -9.026317   \n",
       "\n",
       "         x21       x22        x23     x24       x25       x26       x27  \\\n",
       "0  -1.737940 -2.516715   3.553013  euorpe -0.801340  1.142950  1.005131   \n",
       "1  10.814319 -0.478112  10.590601    asia  0.818792 -0.642987  0.751086   \n",
       "2  -2.419221  2.467521  -5.270615    asia -0.718315 -0.566757  4.171088   \n",
       "3 -14.179167  1.470625 -11.484431    asia -0.052430 -0.558582  9.215569   \n",
       "4  -7.039818 -1.978748 -15.998166    asia -0.223449  0.350781  1.811182   \n",
       "\n",
       "         x28  x29        x30       x31     x32       x33        x34       x35  \\\n",
       "0 -18.473784  Jul    tuesday -3.851669  0.0000 -1.940031  -5.492063  0.627121   \n",
       "1   3.749377  Aug  wednesday  1.391594 -0.0002  2.211462  -4.460591  1.035461   \n",
       "2  11.522448  Jul  wednesday -3.262082 -0.0001  0.419607  -3.804056 -0.763357   \n",
       "3  30.595226  Jul  wednesday -2.285241  0.0001 -3.442715   4.420160  1.164532   \n",
       "4  -4.094084  Jul    tuesday  0.921047  0.0001 -0.431640  12.165494 -0.167726   \n",
       "\n",
       "        x36      x37        x38       x39        x40        x41       x42  \\\n",
       "0 -0.873824  1313.96  -1.353729 -5.186148 -10.612200  -1.497117  5.414063   \n",
       "1  0.228270  1962.78  32.816804 -5.150012   2.147427  36.292790  4.490915   \n",
       "2 -1.612561   430.47  -0.333199  8.728585  -0.863137  -0.368491  9.088864   \n",
       "3  3.033455 -2366.29  14.188669 -6.385060  12.084421  15.691546 -7.467775   \n",
       "4 -0.341604  -620.66 -12.578926  1.133798  30.004727 -13.911297 -5.229937   \n",
       "\n",
       "        x43       x44       x45        x46       x47       x48        x49  y  \n",
       "0 -2.325655  1.674827 -0.264332  60.781427 -7.689696  0.151589  -8.040166  0  \n",
       "1  0.762561  6.526662  1.007927  15.805696 -4.896678 -0.320283  16.719974  0  \n",
       "2 -0.689886 -2.731118  0.754200  30.856417 -7.428573 -2.090804  -7.869421  0  \n",
       "3  2.940789 -6.424112  0.419776 -72.424569  5.361375  1.806070  -7.670847  0  \n",
       "4  1.783928  3.957801 -0.096988 -14.085435 -0.208351 -0.894942  15.724742  1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", 500, \"display.max_columns\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160000 entries, 0 to 159999\n",
      "Data columns (total 51 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   x0      159974 non-null  float64\n",
      " 1   x1      159975 non-null  float64\n",
      " 2   x2      159962 non-null  float64\n",
      " 3   x3      159963 non-null  float64\n",
      " 4   x4      159974 non-null  float64\n",
      " 5   x5      159963 non-null  float64\n",
      " 6   x6      159974 non-null  float64\n",
      " 7   x7      159973 non-null  float64\n",
      " 8   x8      159979 non-null  float64\n",
      " 9   x9      159970 non-null  float64\n",
      " 10  x10     159957 non-null  float64\n",
      " 11  x11     159970 non-null  float64\n",
      " 12  x12     159964 non-null  float64\n",
      " 13  x13     159969 non-null  float64\n",
      " 14  x14     159966 non-null  float64\n",
      " 15  x15     159965 non-null  float64\n",
      " 16  x16     159974 non-null  float64\n",
      " 17  x17     159973 non-null  float64\n",
      " 18  x18     159960 non-null  float64\n",
      " 19  x19     159965 non-null  float64\n",
      " 20  x20     159962 non-null  float64\n",
      " 21  x21     159971 non-null  float64\n",
      " 22  x22     159973 non-null  float64\n",
      " 23  x23     159953 non-null  float64\n",
      " 24  x24     159972 non-null  object \n",
      " 25  x25     159978 non-null  float64\n",
      " 26  x26     159964 non-null  float64\n",
      " 27  x27     159970 non-null  float64\n",
      " 28  x28     159965 non-null  float64\n",
      " 29  x29     159970 non-null  object \n",
      " 30  x30     159970 non-null  object \n",
      " 31  x31     159961 non-null  float64\n",
      " 32  x32     159969 non-null  float64\n",
      " 33  x33     159959 non-null  float64\n",
      " 34  x34     159959 non-null  float64\n",
      " 35  x35     159970 non-null  float64\n",
      " 36  x36     159973 non-null  float64\n",
      " 37  x37     159977 non-null  float64\n",
      " 38  x38     159969 non-null  float64\n",
      " 39  x39     159977 non-null  float64\n",
      " 40  x40     159964 non-null  float64\n",
      " 41  x41     159960 non-null  float64\n",
      " 42  x42     159974 non-null  float64\n",
      " 43  x43     159963 non-null  float64\n",
      " 44  x44     159960 non-null  float64\n",
      " 45  x45     159971 non-null  float64\n",
      " 46  x46     159969 non-null  float64\n",
      " 47  x47     159963 non-null  float64\n",
      " 48  x48     159968 non-null  float64\n",
      " 49  x49     159968 non-null  float64\n",
      " 50  y       160000 non-null  int64  \n",
      "dtypes: float64(47), int64(1), object(3)\n",
      "memory usage: 62.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing numeric\n",
    "numb = df.select_dtypes(include='number').columns\n",
    "df[numb] = df[numb].fillna(df[numb].median().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x24 (28,)\n",
      "x29 (30,)\n",
      "x30 (30,)\n"
     ]
    }
   ],
   "source": [
    "# show columns with missing values still (only the categorical columns left)\n",
    "for i in df.columns:\n",
    "    if df.loc[df[i].isna(),i].shape[0]>0:\n",
    "        print(i, df.loc[df[i].isna(),i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing categorical columns with the word 'Missing'\n",
    "cats = df.select_dtypes(include='object').columns\n",
    "\n",
    "df[cats] = df[cats].transform(lambda x: x.fillna('Missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show columns with missing values still (none)\n",
    "for i in df.columns:\n",
    "    if df.loc[df[i].isna(),i].shape[0]>0:\n",
    "        print(i, df.loc[df[i].isna(),i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160000 entries, 0 to 159999\n",
      "Data columns (total 49 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   x0      160000 non-null  float64\n",
      " 1   x1      160000 non-null  float64\n",
      " 2   x2      160000 non-null  float64\n",
      " 3   x3      160000 non-null  float64\n",
      " 4   x4      160000 non-null  float64\n",
      " 5   x5      160000 non-null  float64\n",
      " 6   x7      160000 non-null  float64\n",
      " 7   x8      160000 non-null  float64\n",
      " 8   x9      160000 non-null  float64\n",
      " 9   x10     160000 non-null  float64\n",
      " 10  x11     160000 non-null  float64\n",
      " 11  x12     160000 non-null  float64\n",
      " 12  x13     160000 non-null  float64\n",
      " 13  x14     160000 non-null  float64\n",
      " 14  x15     160000 non-null  float64\n",
      " 15  x16     160000 non-null  float64\n",
      " 16  x17     160000 non-null  float64\n",
      " 17  x18     160000 non-null  float64\n",
      " 18  x19     160000 non-null  float64\n",
      " 19  x20     160000 non-null  float64\n",
      " 20  x21     160000 non-null  float64\n",
      " 21  x22     160000 non-null  float64\n",
      " 22  x23     160000 non-null  float64\n",
      " 23  x24     160000 non-null  object \n",
      " 24  x25     160000 non-null  float64\n",
      " 25  x26     160000 non-null  float64\n",
      " 26  x27     160000 non-null  float64\n",
      " 27  x28     160000 non-null  float64\n",
      " 28  x29     160000 non-null  object \n",
      " 29  x30     160000 non-null  object \n",
      " 30  x31     160000 non-null  float64\n",
      " 31  x32     160000 non-null  float64\n",
      " 32  x33     160000 non-null  float64\n",
      " 33  x34     160000 non-null  float64\n",
      " 34  x35     160000 non-null  float64\n",
      " 35  x36     160000 non-null  float64\n",
      " 36  x37     160000 non-null  float64\n",
      " 37  x38     160000 non-null  float64\n",
      " 38  x39     160000 non-null  float64\n",
      " 39  x40     160000 non-null  float64\n",
      " 40  x42     160000 non-null  float64\n",
      " 41  x43     160000 non-null  float64\n",
      " 42  x44     160000 non-null  float64\n",
      " 43  x45     160000 non-null  float64\n",
      " 44  x46     160000 non-null  float64\n",
      " 45  x47     160000 non-null  float64\n",
      " 46  x48     160000 non-null  float64\n",
      " 47  x49     160000 non-null  float64\n",
      " 48  y       160000 non-null  int64  \n",
      "dtypes: float64(45), int64(1), object(3)\n",
      "memory usage: 59.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df=df.drop(columns=['x6','x41'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is:  (160000, 48)\n",
      "The shape of y is:  (160000,)\n"
     ]
    }
   ],
   "source": [
    "X = df.copy().drop(columns=[\"y\"])\n",
    "print(\"The shape of X is: \", X.shape)\n",
    "\n",
    "y = df.loc[:,\"y\"].copy()\n",
    "print(\"The shape of y is: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and One-Hot Encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def transform_data(data):\n",
    "    #OH encode\n",
    "    label_encode = data.select_dtypes(include='object').columns\n",
    "    normalize = data.select_dtypes(include='number').columns\n",
    "\n",
    "    data_OHE = pd.get_dummies(data, columns=label_encode)\n",
    "    \n",
    "    #Standardize the variables\n",
    "    scaler = StandardScaler()\n",
    "    data_OHE[normalize] = scaler.fit_transform(data_OHE[normalize])\n",
    " \n",
    "    return data_OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>x24_Missing</th>\n",
       "      <th>x24_america</th>\n",
       "      <th>x24_asia</th>\n",
       "      <th>x24_euorpe</th>\n",
       "      <th>x29_Apr</th>\n",
       "      <th>x29_Aug</th>\n",
       "      <th>x29_Dec</th>\n",
       "      <th>x29_Feb</th>\n",
       "      <th>x29_Jan</th>\n",
       "      <th>x29_Jul</th>\n",
       "      <th>x29_Jun</th>\n",
       "      <th>x29_Mar</th>\n",
       "      <th>x29_May</th>\n",
       "      <th>x29_Missing</th>\n",
       "      <th>x29_Nov</th>\n",
       "      <th>x29_Oct</th>\n",
       "      <th>x29_Sep</th>\n",
       "      <th>x30_Missing</th>\n",
       "      <th>x30_friday</th>\n",
       "      <th>x30_monday</th>\n",
       "      <th>x30_thurday</th>\n",
       "      <th>x30_tuesday</th>\n",
       "      <th>x30_wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.446058</td>\n",
       "      <td>-0.625059</td>\n",
       "      <td>0.434853</td>\n",
       "      <td>0.310829</td>\n",
       "      <td>-0.281989</td>\n",
       "      <td>0.103154</td>\n",
       "      <td>-0.232398</td>\n",
       "      <td>-0.113491</td>\n",
       "      <td>-0.662595</td>\n",
       "      <td>0.785889</td>\n",
       "      <td>1.507768</td>\n",
       "      <td>1.830589</td>\n",
       "      <td>-0.561261</td>\n",
       "      <td>1.507268</td>\n",
       "      <td>-0.769972</td>\n",
       "      <td>0.423723</td>\n",
       "      <td>0.773434</td>\n",
       "      <td>-1.471070</td>\n",
       "      <td>0.232661</td>\n",
       "      <td>-0.380367</td>\n",
       "      <td>-0.181618</td>\n",
       "      <td>-0.466617</td>\n",
       "      <td>0.189905</td>\n",
       "      <td>-0.633551</td>\n",
       "      <td>1.356818</td>\n",
       "      <td>0.149007</td>\n",
       "      <td>-1.281721</td>\n",
       "      <td>-1.389773</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>-1.106396</td>\n",
       "      <td>-0.685330</td>\n",
       "      <td>0.263178</td>\n",
       "      <td>-0.552575</td>\n",
       "      <td>1.313755</td>\n",
       "      <td>-0.438894</td>\n",
       "      <td>-1.010999</td>\n",
       "      <td>-0.486785</td>\n",
       "      <td>1.418299</td>\n",
       "      <td>-1.513949</td>\n",
       "      <td>0.403711</td>\n",
       "      <td>-0.668753</td>\n",
       "      <td>2.008931</td>\n",
       "      <td>-1.612152</td>\n",
       "      <td>0.078445</td>\n",
       "      <td>-0.489915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.401142</td>\n",
       "      <td>-0.092590</td>\n",
       "      <td>2.184324</td>\n",
       "      <td>0.517973</td>\n",
       "      <td>1.007145</td>\n",
       "      <td>-0.318226</td>\n",
       "      <td>0.031663</td>\n",
       "      <td>0.104146</td>\n",
       "      <td>0.051099</td>\n",
       "      <td>-1.488004</td>\n",
       "      <td>-0.271764</td>\n",
       "      <td>-1.605533</td>\n",
       "      <td>1.093712</td>\n",
       "      <td>-1.575223</td>\n",
       "      <td>0.459369</td>\n",
       "      <td>-0.482390</td>\n",
       "      <td>-1.230581</td>\n",
       "      <td>-0.443183</td>\n",
       "      <td>0.661100</td>\n",
       "      <td>-1.052294</td>\n",
       "      <td>1.152486</td>\n",
       "      <td>-0.089903</td>\n",
       "      <td>0.662008</td>\n",
       "      <td>0.648640</td>\n",
       "      <td>-0.761327</td>\n",
       "      <td>0.111501</td>\n",
       "      <td>0.257502</td>\n",
       "      <td>0.505044</td>\n",
       "      <td>-1.922310</td>\n",
       "      <td>1.269235</td>\n",
       "      <td>-0.556607</td>\n",
       "      <td>0.434798</td>\n",
       "      <td>0.139242</td>\n",
       "      <td>1.962659</td>\n",
       "      <td>1.584238</td>\n",
       "      <td>-1.003960</td>\n",
       "      <td>0.261950</td>\n",
       "      <td>1.237653</td>\n",
       "      <td>0.498220</td>\n",
       "      <td>1.568880</td>\n",
       "      <td>2.539292</td>\n",
       "      <td>0.780253</td>\n",
       "      <td>-1.028765</td>\n",
       "      <td>-0.165378</td>\n",
       "      <td>1.156898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.864117</td>\n",
       "      <td>-0.225734</td>\n",
       "      <td>1.009787</td>\n",
       "      <td>0.819873</td>\n",
       "      <td>-0.831135</td>\n",
       "      <td>-1.476650</td>\n",
       "      <td>0.614063</td>\n",
       "      <td>0.601806</td>\n",
       "      <td>-0.415214</td>\n",
       "      <td>0.199534</td>\n",
       "      <td>-0.479086</td>\n",
       "      <td>0.909405</td>\n",
       "      <td>-0.577035</td>\n",
       "      <td>1.047584</td>\n",
       "      <td>-0.670551</td>\n",
       "      <td>-0.817003</td>\n",
       "      <td>-1.015645</td>\n",
       "      <td>0.887405</td>\n",
       "      <td>-0.876716</td>\n",
       "      <td>0.241360</td>\n",
       "      <td>-0.254028</td>\n",
       "      <td>0.454422</td>\n",
       "      <td>-0.402011</td>\n",
       "      <td>-0.567844</td>\n",
       "      <td>-0.670917</td>\n",
       "      <td>0.616418</td>\n",
       "      <td>0.795881</td>\n",
       "      <td>-1.176707</td>\n",
       "      <td>-0.960632</td>\n",
       "      <td>0.243872</td>\n",
       "      <td>-0.474674</td>\n",
       "      <td>-0.321222</td>\n",
       "      <td>-1.016301</td>\n",
       "      <td>0.430150</td>\n",
       "      <td>-0.378472</td>\n",
       "      <td>1.699345</td>\n",
       "      <td>0.085290</td>\n",
       "      <td>2.137400</td>\n",
       "      <td>-0.448141</td>\n",
       "      <td>-0.654377</td>\n",
       "      <td>1.899509</td>\n",
       "      <td>1.191419</td>\n",
       "      <td>-1.557610</td>\n",
       "      <td>-1.080233</td>\n",
       "      <td>-0.478558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.659021</td>\n",
       "      <td>0.800509</td>\n",
       "      <td>-1.732948</td>\n",
       "      <td>0.454106</td>\n",
       "      <td>1.019525</td>\n",
       "      <td>0.296857</td>\n",
       "      <td>-0.367422</td>\n",
       "      <td>-0.034436</td>\n",
       "      <td>-0.877112</td>\n",
       "      <td>-0.254436</td>\n",
       "      <td>-2.202916</td>\n",
       "      <td>0.835959</td>\n",
       "      <td>-0.661464</td>\n",
       "      <td>0.359497</td>\n",
       "      <td>0.394676</td>\n",
       "      <td>-0.502264</td>\n",
       "      <td>-2.078959</td>\n",
       "      <td>-0.605286</td>\n",
       "      <td>0.143918</td>\n",
       "      <td>0.279832</td>\n",
       "      <td>-1.503922</td>\n",
       "      <td>0.270205</td>\n",
       "      <td>-0.818853</td>\n",
       "      <td>-0.040855</td>\n",
       "      <td>-0.661222</td>\n",
       "      <td>1.361168</td>\n",
       "      <td>2.116902</td>\n",
       "      <td>-0.823695</td>\n",
       "      <td>0.962724</td>\n",
       "      <td>-1.966285</td>\n",
       "      <td>0.551668</td>\n",
       "      <td>0.489044</td>\n",
       "      <td>1.900138</td>\n",
       "      <td>-2.366972</td>\n",
       "      <td>0.481324</td>\n",
       "      <td>-1.244525</td>\n",
       "      <td>0.845053</td>\n",
       "      <td>-1.102473</td>\n",
       "      <td>1.917474</td>\n",
       "      <td>-1.541250</td>\n",
       "      <td>1.056248</td>\n",
       "      <td>-1.630082</td>\n",
       "      <td>1.113870</td>\n",
       "      <td>0.933341</td>\n",
       "      <td>-0.465351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.733857</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>-0.768724</td>\n",
       "      <td>0.210985</td>\n",
       "      <td>0.458963</td>\n",
       "      <td>-0.082200</td>\n",
       "      <td>1.153419</td>\n",
       "      <td>0.138222</td>\n",
       "      <td>-0.678986</td>\n",
       "      <td>0.846885</td>\n",
       "      <td>0.220747</td>\n",
       "      <td>-1.815131</td>\n",
       "      <td>-0.141475</td>\n",
       "      <td>0.826018</td>\n",
       "      <td>0.144087</td>\n",
       "      <td>-0.232023</td>\n",
       "      <td>-1.867001</td>\n",
       "      <td>0.994524</td>\n",
       "      <td>-0.172353</td>\n",
       "      <td>-1.606310</td>\n",
       "      <td>-0.745123</td>\n",
       "      <td>-0.367206</td>\n",
       "      <td>-1.121648</td>\n",
       "      <td>-0.176202</td>\n",
       "      <td>0.417295</td>\n",
       "      <td>0.268009</td>\n",
       "      <td>-0.285753</td>\n",
       "      <td>0.334997</td>\n",
       "      <td>0.962724</td>\n",
       "      <td>-0.243241</td>\n",
       "      <td>1.518249</td>\n",
       "      <td>-0.070886</td>\n",
       "      <td>-0.218485</td>\n",
       "      <td>-0.621116</td>\n",
       "      <td>-1.103504</td>\n",
       "      <td>0.220015</td>\n",
       "      <td>1.896616</td>\n",
       "      <td>-0.664564</td>\n",
       "      <td>1.163705</td>\n",
       "      <td>0.951968</td>\n",
       "      <td>-0.246791</td>\n",
       "      <td>-0.036334</td>\n",
       "      <td>-0.049498</td>\n",
       "      <td>-0.462313</td>\n",
       "      <td>1.090704</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159995</th>\n",
       "      <td>-1.309589</td>\n",
       "      <td>-0.673746</td>\n",
       "      <td>0.118113</td>\n",
       "      <td>-0.244019</td>\n",
       "      <td>0.275489</td>\n",
       "      <td>-0.154046</td>\n",
       "      <td>1.597177</td>\n",
       "      <td>-0.212644</td>\n",
       "      <td>-0.374491</td>\n",
       "      <td>-0.097816</td>\n",
       "      <td>-1.435799</td>\n",
       "      <td>-1.079819</td>\n",
       "      <td>0.171087</td>\n",
       "      <td>-1.348465</td>\n",
       "      <td>0.581124</td>\n",
       "      <td>0.663369</td>\n",
       "      <td>-0.701101</td>\n",
       "      <td>-0.505468</td>\n",
       "      <td>-0.014671</td>\n",
       "      <td>-0.320516</td>\n",
       "      <td>-0.697041</td>\n",
       "      <td>1.055926</td>\n",
       "      <td>0.367743</td>\n",
       "      <td>0.125562</td>\n",
       "      <td>0.571108</td>\n",
       "      <td>0.054634</td>\n",
       "      <td>1.948343</td>\n",
       "      <td>0.938940</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>1.324623</td>\n",
       "      <td>-0.780613</td>\n",
       "      <td>-0.133254</td>\n",
       "      <td>-1.483602</td>\n",
       "      <td>-0.892451</td>\n",
       "      <td>-0.861474</td>\n",
       "      <td>0.481424</td>\n",
       "      <td>-0.159377</td>\n",
       "      <td>1.943269</td>\n",
       "      <td>-0.568841</td>\n",
       "      <td>-0.338020</td>\n",
       "      <td>-0.919989</td>\n",
       "      <td>0.767514</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>-1.979672</td>\n",
       "      <td>1.933069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159996</th>\n",
       "      <td>2.227142</td>\n",
       "      <td>0.757558</td>\n",
       "      <td>1.756477</td>\n",
       "      <td>1.413174</td>\n",
       "      <td>0.268962</td>\n",
       "      <td>0.909770</td>\n",
       "      <td>-0.150145</td>\n",
       "      <td>-0.430371</td>\n",
       "      <td>1.055904</td>\n",
       "      <td>-0.823173</td>\n",
       "      <td>-0.986539</td>\n",
       "      <td>-1.528971</td>\n",
       "      <td>0.371886</td>\n",
       "      <td>-1.167562</td>\n",
       "      <td>1.231081</td>\n",
       "      <td>-0.008203</td>\n",
       "      <td>0.826907</td>\n",
       "      <td>1.585016</td>\n",
       "      <td>0.040006</td>\n",
       "      <td>0.234384</td>\n",
       "      <td>-1.708560</td>\n",
       "      <td>-0.906572</td>\n",
       "      <td>0.743386</td>\n",
       "      <td>-0.975301</td>\n",
       "      <td>-0.851959</td>\n",
       "      <td>-0.348621</td>\n",
       "      <td>-1.247645</td>\n",
       "      <td>2.527407</td>\n",
       "      <td>-0.960632</td>\n",
       "      <td>0.702462</td>\n",
       "      <td>-0.932502</td>\n",
       "      <td>0.385921</td>\n",
       "      <td>-1.090294</td>\n",
       "      <td>1.588480</td>\n",
       "      <td>0.326692</td>\n",
       "      <td>-1.446234</td>\n",
       "      <td>-0.361241</td>\n",
       "      <td>0.523510</td>\n",
       "      <td>-0.443119</td>\n",
       "      <td>-1.210699</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>0.369774</td>\n",
       "      <td>1.328151</td>\n",
       "      <td>-0.710233</td>\n",
       "      <td>-0.063168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159997</th>\n",
       "      <td>-2.159656</td>\n",
       "      <td>0.845621</td>\n",
       "      <td>0.632436</td>\n",
       "      <td>-0.926509</td>\n",
       "      <td>0.359744</td>\n",
       "      <td>-0.361141</td>\n",
       "      <td>1.644046</td>\n",
       "      <td>0.163082</td>\n",
       "      <td>1.067517</td>\n",
       "      <td>-0.017163</td>\n",
       "      <td>-0.494201</td>\n",
       "      <td>-0.417264</td>\n",
       "      <td>-0.188937</td>\n",
       "      <td>0.228828</td>\n",
       "      <td>-0.064052</td>\n",
       "      <td>0.760486</td>\n",
       "      <td>0.535330</td>\n",
       "      <td>0.289341</td>\n",
       "      <td>-0.377945</td>\n",
       "      <td>-1.720478</td>\n",
       "      <td>-1.237651</td>\n",
       "      <td>0.093582</td>\n",
       "      <td>-0.627511</td>\n",
       "      <td>-0.751442</td>\n",
       "      <td>-0.431736</td>\n",
       "      <td>-0.918515</td>\n",
       "      <td>0.747972</td>\n",
       "      <td>0.294570</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.056458</td>\n",
       "      <td>0.665110</td>\n",
       "      <td>-0.291879</td>\n",
       "      <td>0.098819</td>\n",
       "      <td>0.687173</td>\n",
       "      <td>-0.377298</td>\n",
       "      <td>-1.633326</td>\n",
       "      <td>1.466160</td>\n",
       "      <td>0.330655</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>1.740438</td>\n",
       "      <td>0.540774</td>\n",
       "      <td>0.016396</td>\n",
       "      <td>0.676048</td>\n",
       "      <td>0.636174</td>\n",
       "      <td>0.266686</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159998</th>\n",
       "      <td>0.916897</td>\n",
       "      <td>1.200062</td>\n",
       "      <td>0.491160</td>\n",
       "      <td>-0.347323</td>\n",
       "      <td>0.634097</td>\n",
       "      <td>1.996178</td>\n",
       "      <td>-0.083183</td>\n",
       "      <td>0.296669</td>\n",
       "      <td>0.668143</td>\n",
       "      <td>-0.951236</td>\n",
       "      <td>1.383922</td>\n",
       "      <td>1.514428</td>\n",
       "      <td>-0.519404</td>\n",
       "      <td>-1.142107</td>\n",
       "      <td>1.277822</td>\n",
       "      <td>-1.187514</td>\n",
       "      <td>-0.352399</td>\n",
       "      <td>-1.167210</td>\n",
       "      <td>2.005025</td>\n",
       "      <td>0.784011</td>\n",
       "      <td>-0.070308</td>\n",
       "      <td>1.852185</td>\n",
       "      <td>1.390488</td>\n",
       "      <td>0.148532</td>\n",
       "      <td>1.207218</td>\n",
       "      <td>-1.388047</td>\n",
       "      <td>0.859245</td>\n",
       "      <td>1.306812</td>\n",
       "      <td>-1.922310</td>\n",
       "      <td>1.406382</td>\n",
       "      <td>-0.451382</td>\n",
       "      <td>-0.479975</td>\n",
       "      <td>-0.758560</td>\n",
       "      <td>0.438891</td>\n",
       "      <td>-0.375215</td>\n",
       "      <td>-0.115790</td>\n",
       "      <td>-1.762733</td>\n",
       "      <td>0.241218</td>\n",
       "      <td>-2.242042</td>\n",
       "      <td>0.133567</td>\n",
       "      <td>0.519041</td>\n",
       "      <td>0.487295</td>\n",
       "      <td>0.349411</td>\n",
       "      <td>-0.144367</td>\n",
       "      <td>-0.087277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159999</th>\n",
       "      <td>-0.796862</td>\n",
       "      <td>-0.065319</td>\n",
       "      <td>-0.735482</td>\n",
       "      <td>-0.695553</td>\n",
       "      <td>-0.631113</td>\n",
       "      <td>2.076608</td>\n",
       "      <td>-1.267061</td>\n",
       "      <td>2.063913</td>\n",
       "      <td>-0.720909</td>\n",
       "      <td>-0.378441</td>\n",
       "      <td>-0.960182</td>\n",
       "      <td>-0.085912</td>\n",
       "      <td>-0.982999</td>\n",
       "      <td>-1.126801</td>\n",
       "      <td>2.009851</td>\n",
       "      <td>-0.229162</td>\n",
       "      <td>-1.918983</td>\n",
       "      <td>-0.614679</td>\n",
       "      <td>-0.414046</td>\n",
       "      <td>1.498184</td>\n",
       "      <td>-1.178271</td>\n",
       "      <td>0.406908</td>\n",
       "      <td>1.541078</td>\n",
       "      <td>1.580666</td>\n",
       "      <td>-0.285199</td>\n",
       "      <td>0.662837</td>\n",
       "      <td>0.330628</td>\n",
       "      <td>-0.581580</td>\n",
       "      <td>1.924402</td>\n",
       "      <td>1.863432</td>\n",
       "      <td>0.532386</td>\n",
       "      <td>1.019898</td>\n",
       "      <td>0.696543</td>\n",
       "      <td>-1.229875</td>\n",
       "      <td>1.131572</td>\n",
       "      <td>-0.333197</td>\n",
       "      <td>-2.492302</td>\n",
       "      <td>0.631322</td>\n",
       "      <td>0.583571</td>\n",
       "      <td>-0.442438</td>\n",
       "      <td>-1.071316</td>\n",
       "      <td>-0.126320</td>\n",
       "      <td>1.229819</td>\n",
       "      <td>-1.429864</td>\n",
       "      <td>1.078920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160000 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x0        x1        x2        x3        x4        x5        x7  \\\n",
       "0      -0.446058 -0.625059  0.434853  0.310829 -0.281989  0.103154 -0.232398   \n",
       "1      -0.401142 -0.092590  2.184324  0.517973  1.007145 -0.318226  0.031663   \n",
       "2      -0.864117 -0.225734  1.009787  0.819873 -0.831135 -1.476650  0.614063   \n",
       "3      -0.659021  0.800509 -1.732948  0.454106  1.019525  0.296857 -0.367422   \n",
       "4      -0.733857  0.048101 -0.768724  0.210985  0.458963 -0.082200  1.153419   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "159995 -1.309589 -0.673746  0.118113 -0.244019  0.275489 -0.154046  1.597177   \n",
       "159996  2.227142  0.757558  1.756477  1.413174  0.268962  0.909770 -0.150145   \n",
       "159997 -2.159656  0.845621  0.632436 -0.926509  0.359744 -0.361141  1.644046   \n",
       "159998  0.916897  1.200062  0.491160 -0.347323  0.634097  1.996178 -0.083183   \n",
       "159999 -0.796862 -0.065319 -0.735482 -0.695553 -0.631113  2.076608 -1.267061   \n",
       "\n",
       "              x8        x9       x10       x11       x12       x13       x14  \\\n",
       "0      -0.113491 -0.662595  0.785889  1.507768  1.830589 -0.561261  1.507268   \n",
       "1       0.104146  0.051099 -1.488004 -0.271764 -1.605533  1.093712 -1.575223   \n",
       "2       0.601806 -0.415214  0.199534 -0.479086  0.909405 -0.577035  1.047584   \n",
       "3      -0.034436 -0.877112 -0.254436 -2.202916  0.835959 -0.661464  0.359497   \n",
       "4       0.138222 -0.678986  0.846885  0.220747 -1.815131 -0.141475  0.826018   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "159995 -0.212644 -0.374491 -0.097816 -1.435799 -1.079819  0.171087 -1.348465   \n",
       "159996 -0.430371  1.055904 -0.823173 -0.986539 -1.528971  0.371886 -1.167562   \n",
       "159997  0.163082  1.067517 -0.017163 -0.494201 -0.417264 -0.188937  0.228828   \n",
       "159998  0.296669  0.668143 -0.951236  1.383922  1.514428 -0.519404 -1.142107   \n",
       "159999  2.063913 -0.720909 -0.378441 -0.960182 -0.085912 -0.982999 -1.126801   \n",
       "\n",
       "             x15       x16       x17       x18       x19       x20       x21  \\\n",
       "0      -0.769972  0.423723  0.773434 -1.471070  0.232661 -0.380367 -0.181618   \n",
       "1       0.459369 -0.482390 -1.230581 -0.443183  0.661100 -1.052294  1.152486   \n",
       "2      -0.670551 -0.817003 -1.015645  0.887405 -0.876716  0.241360 -0.254028   \n",
       "3       0.394676 -0.502264 -2.078959 -0.605286  0.143918  0.279832 -1.503922   \n",
       "4       0.144087 -0.232023 -1.867001  0.994524 -0.172353 -1.606310 -0.745123   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "159995  0.581124  0.663369 -0.701101 -0.505468 -0.014671 -0.320516 -0.697041   \n",
       "159996  1.231081 -0.008203  0.826907  1.585016  0.040006  0.234384 -1.708560   \n",
       "159997 -0.064052  0.760486  0.535330  0.289341 -0.377945 -1.720478 -1.237651   \n",
       "159998  1.277822 -1.187514 -0.352399 -1.167210  2.005025  0.784011 -0.070308   \n",
       "159999  2.009851 -0.229162 -1.918983 -0.614679 -0.414046  1.498184 -1.178271   \n",
       "\n",
       "             x22       x23       x25       x26       x27       x28       x31  \\\n",
       "0      -0.466617  0.189905 -0.633551  1.356818  0.149007 -1.281721 -1.389773   \n",
       "1      -0.089903  0.662008  0.648640 -0.761327  0.111501  0.257502  0.505044   \n",
       "2       0.454422 -0.402011 -0.567844 -0.670917  0.616418  0.795881 -1.176707   \n",
       "3       0.270205 -0.818853 -0.040855 -0.661222  1.361168  2.116902 -0.823695   \n",
       "4      -0.367206 -1.121648 -0.176202  0.417295  0.268009 -0.285753  0.334997   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "159995  1.055926  0.367743  0.125562  0.571108  0.054634  1.948343  0.938940   \n",
       "159996 -0.906572  0.743386 -0.975301 -0.851959 -0.348621 -1.247645  2.527407   \n",
       "159997  0.093582 -0.627511 -0.751442 -0.431736 -0.918515  0.747972  0.294570   \n",
       "159998  1.852185  1.390488  0.148532  1.207218 -1.388047  0.859245  1.306812   \n",
       "159999  0.406908  1.541078  1.580666 -0.285199  0.662837  0.330628 -0.581580   \n",
       "\n",
       "             x32       x33       x34       x35       x36       x37       x38  \\\n",
       "0       0.001046 -1.106396 -0.685330  0.263178 -0.552575  1.313755 -0.438894   \n",
       "1      -1.922310  1.269235 -0.556607  0.434798  0.139242  1.962659  1.584238   \n",
       "2      -0.960632  0.243872 -0.474674 -0.321222 -1.016301  0.430150 -0.378472   \n",
       "3       0.962724 -1.966285  0.551668  0.489044  1.900138 -2.366972  0.481324   \n",
       "4       0.962724 -0.243241  1.518249 -0.070886 -0.218485 -0.621116 -1.103504   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "159995  0.001046  1.324623 -0.780613 -0.133254 -1.483602 -0.892451 -0.861474   \n",
       "159996 -0.960632  0.702462 -0.932502  0.385921 -1.090294  1.588480  0.326692   \n",
       "159997  0.001046  0.056458  0.665110 -0.291879  0.098819  0.687173 -0.377298   \n",
       "159998 -1.922310  1.406382 -0.451382 -0.479975 -0.758560  0.438891 -0.375215   \n",
       "159999  1.924402  1.863432  0.532386  1.019898  0.696543 -1.229875  1.131572   \n",
       "\n",
       "             x39       x40       x42       x43       x44       x45       x46  \\\n",
       "0      -1.010999 -0.486785  1.418299 -1.513949  0.403711 -0.668753  2.008931   \n",
       "1      -1.003960  0.261950  1.237653  0.498220  1.568880  2.539292  0.780253   \n",
       "2       1.699345  0.085290  2.137400 -0.448141 -0.654377  1.899509  1.191419   \n",
       "3      -1.244525  0.845053 -1.102473  1.917474 -1.541250  1.056248 -1.630082   \n",
       "4       0.220015  1.896616 -0.664564  1.163705  0.951968 -0.246791 -0.036334   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "159995  0.481424 -0.159377  1.943269 -0.568841 -0.338020 -0.919989  0.767514   \n",
       "159996 -1.446234 -0.361241  0.523510 -0.443119 -1.210699 -0.047363  0.369774   \n",
       "159997 -1.633326  1.466160  0.330655  0.482411  1.740438  0.540774  0.016396   \n",
       "159998 -0.115790 -1.762733  0.241218 -2.242042  0.133567  0.519041  0.487295   \n",
       "159999 -0.333197 -2.492302  0.631322  0.583571 -0.442438 -1.071316 -0.126320   \n",
       "\n",
       "             x47       x48       x49  x24_Missing  x24_america  x24_asia  \\\n",
       "0      -1.612152  0.078445 -0.489915            0            0         0   \n",
       "1      -1.028765 -0.165378  1.156898            0            0         1   \n",
       "2      -1.557610 -1.080233 -0.478558            0            0         1   \n",
       "3       1.113870  0.933341 -0.465351            0            0         1   \n",
       "4      -0.049498 -0.462313  1.090704            0            0         1   \n",
       "...          ...       ...       ...          ...          ...       ...   \n",
       "159995  0.905588 -1.979672  1.933069            0            0         1   \n",
       "159996  1.328151 -0.710233 -0.063168            0            0         1   \n",
       "159997  0.676048  0.636174  0.266686            0            0         1   \n",
       "159998  0.349411 -0.144367 -0.087277            0            0         1   \n",
       "159999  1.229819 -1.429864  1.078920            0            0         1   \n",
       "\n",
       "        x24_euorpe  x29_Apr  x29_Aug  x29_Dec  x29_Feb  x29_Jan  x29_Jul  \\\n",
       "0                1        0        0        0        0        0        1   \n",
       "1                0        0        1        0        0        0        0   \n",
       "2                0        0        0        0        0        0        1   \n",
       "3                0        0        0        0        0        0        1   \n",
       "4                0        0        0        0        0        0        1   \n",
       "...            ...      ...      ...      ...      ...      ...      ...   \n",
       "159995           0        0        1        0        0        0        0   \n",
       "159996           0        0        0        0        0        0        0   \n",
       "159997           0        0        0        0        0        0        0   \n",
       "159998           0        0        0        0        0        0        0   \n",
       "159999           0        0        1        0        0        0        0   \n",
       "\n",
       "        x29_Jun  x29_Mar  x29_May  x29_Missing  x29_Nov  x29_Oct  x29_Sep  \\\n",
       "0             0        0        0            0        0        0        0   \n",
       "1             0        0        0            0        0        0        0   \n",
       "2             0        0        0            0        0        0        0   \n",
       "3             0        0        0            0        0        0        0   \n",
       "4             0        0        0            0        0        0        0   \n",
       "...         ...      ...      ...          ...      ...      ...      ...   \n",
       "159995        0        0        0            0        0        0        0   \n",
       "159996        0        0        1            0        0        0        0   \n",
       "159997        1        0        0            0        0        0        0   \n",
       "159998        0        0        1            0        0        0        0   \n",
       "159999        0        0        0            0        0        0        0   \n",
       "\n",
       "        x30_Missing  x30_friday  x30_monday  x30_thurday  x30_tuesday  \\\n",
       "0                 0           0           0            0            1   \n",
       "1                 0           0           0            0            0   \n",
       "2                 0           0           0            0            0   \n",
       "3                 0           0           0            0            0   \n",
       "4                 0           0           0            0            1   \n",
       "...             ...         ...         ...          ...          ...   \n",
       "159995            0           0           0            0            0   \n",
       "159996            0           0           0            0            0   \n",
       "159997            0           0           0            0            0   \n",
       "159998            0           0           0            0            0   \n",
       "159999            0           0           0            0            1   \n",
       "\n",
       "        x30_wednesday  \n",
       "0                   0  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   0  \n",
       "...               ...  \n",
       "159995              1  \n",
       "159996              1  \n",
       "159997              1  \n",
       "159998              1  \n",
       "159999              0  \n",
       "\n",
       "[160000 rows x 68 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X2 = transform_data(X)\n",
    "display(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>x24_Missing</th>\n",
       "      <th>x24_america</th>\n",
       "      <th>x24_asia</th>\n",
       "      <th>x24_euorpe</th>\n",
       "      <th>x29_Apr</th>\n",
       "      <th>x29_Aug</th>\n",
       "      <th>x29_Dec</th>\n",
       "      <th>x29_Feb</th>\n",
       "      <th>x29_Jan</th>\n",
       "      <th>x29_Jul</th>\n",
       "      <th>x29_Jun</th>\n",
       "      <th>x29_Mar</th>\n",
       "      <th>x29_May</th>\n",
       "      <th>x29_Missing</th>\n",
       "      <th>x29_Nov</th>\n",
       "      <th>x29_Oct</th>\n",
       "      <th>x29_Sep</th>\n",
       "      <th>x30_Missing</th>\n",
       "      <th>x30_friday</th>\n",
       "      <th>x30_monday</th>\n",
       "      <th>x30_thurday</th>\n",
       "      <th>x30_tuesday</th>\n",
       "      <th>x30_wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>1.600000e+05</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "      <td>160000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.707346e-18</td>\n",
       "      <td>8.082424e-18</td>\n",
       "      <td>-3.330669e-17</td>\n",
       "      <td>-2.255973e-17</td>\n",
       "      <td>-1.150191e-17</td>\n",
       "      <td>-7.105427e-19</td>\n",
       "      <td>-2.340350e-17</td>\n",
       "      <td>2.646772e-17</td>\n",
       "      <td>-1.172396e-17</td>\n",
       "      <td>-2.233769e-17</td>\n",
       "      <td>4.618528e-18</td>\n",
       "      <td>1.224576e-17</td>\n",
       "      <td>1.563194e-17</td>\n",
       "      <td>2.327027e-17</td>\n",
       "      <td>2.345901e-17</td>\n",
       "      <td>1.070255e-17</td>\n",
       "      <td>2.202682e-17</td>\n",
       "      <td>-1.254552e-17</td>\n",
       "      <td>1.652012e-17</td>\n",
       "      <td>1.492140e-17</td>\n",
       "      <td>3.410605e-17</td>\n",
       "      <td>2.930989e-18</td>\n",
       "      <td>5.107026e-18</td>\n",
       "      <td>1.092459e-17</td>\n",
       "      <td>-3.552714e-19</td>\n",
       "      <td>-1.598721e-17</td>\n",
       "      <td>-2.183809e-17</td>\n",
       "      <td>1.845191e-17</td>\n",
       "      <td>3.046452e-17</td>\n",
       "      <td>3.124168e-17</td>\n",
       "      <td>-9.325873e-18</td>\n",
       "      <td>-5.595524e-18</td>\n",
       "      <td>5.773160e-18</td>\n",
       "      <td>2.344791e-17</td>\n",
       "      <td>1.098233e-16</td>\n",
       "      <td>3.641532e-18</td>\n",
       "      <td>-1.483258e-17</td>\n",
       "      <td>-2.478018e-17</td>\n",
       "      <td>-2.309264e-17</td>\n",
       "      <td>-2.726708e-17</td>\n",
       "      <td>1.803002e-17</td>\n",
       "      <td>6.439294e-18</td>\n",
       "      <td>1.216804e-17</td>\n",
       "      <td>1.803002e-17</td>\n",
       "      <td>1.865175e-18</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>0.868531</td>\n",
       "      <td>0.103362</td>\n",
       "      <td>0.042256</td>\n",
       "      <td>0.183787</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.284806</td>\n",
       "      <td>0.258306</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.137119</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.015044</td>\n",
       "      <td>0.067619</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.003525</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.183931</td>\n",
       "      <td>0.174712</td>\n",
       "      <td>0.634594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>1.000003e+00</td>\n",
       "      <td>0.013228</td>\n",
       "      <td>0.164776</td>\n",
       "      <td>0.337913</td>\n",
       "      <td>0.304433</td>\n",
       "      <td>0.201174</td>\n",
       "      <td>0.387312</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>0.029568</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.451324</td>\n",
       "      <td>0.437705</td>\n",
       "      <td>0.087376</td>\n",
       "      <td>0.343974</td>\n",
       "      <td>0.013692</td>\n",
       "      <td>0.045846</td>\n",
       "      <td>0.121727</td>\n",
       "      <td>0.251091</td>\n",
       "      <td>0.013692</td>\n",
       "      <td>0.059267</td>\n",
       "      <td>0.055143</td>\n",
       "      <td>0.387429</td>\n",
       "      <td>0.379722</td>\n",
       "      <td>0.481545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.288823e+00</td>\n",
       "      <td>-4.144980e+00</td>\n",
       "      <td>-4.388522e+00</td>\n",
       "      <td>-4.396283e+00</td>\n",
       "      <td>-4.460683e+00</td>\n",
       "      <td>-4.412028e+00</td>\n",
       "      <td>-5.691441e+00</td>\n",
       "      <td>-4.231245e+00</td>\n",
       "      <td>-4.404195e+00</td>\n",
       "      <td>-4.613370e+00</td>\n",
       "      <td>-4.347593e+00</td>\n",
       "      <td>-4.262139e+00</td>\n",
       "      <td>-4.326084e+00</td>\n",
       "      <td>-4.439441e+00</td>\n",
       "      <td>-5.197628e+00</td>\n",
       "      <td>-5.226939e+00</td>\n",
       "      <td>-4.546090e+00</td>\n",
       "      <td>-4.451716e+00</td>\n",
       "      <td>-4.695258e+00</td>\n",
       "      <td>-4.646718e+00</td>\n",
       "      <td>-4.620455e+00</td>\n",
       "      <td>-4.370773e+00</td>\n",
       "      <td>-4.518880e+00</td>\n",
       "      <td>-5.036419e+00</td>\n",
       "      <td>-4.573763e+00</td>\n",
       "      <td>-4.724281e+00</td>\n",
       "      <td>-5.051167e+00</td>\n",
       "      <td>-4.438996e+00</td>\n",
       "      <td>-4.807344e+00</td>\n",
       "      <td>-4.260227e+00</td>\n",
       "      <td>-4.507126e+00</td>\n",
       "      <td>-4.206687e+00</td>\n",
       "      <td>-4.314053e+00</td>\n",
       "      <td>-4.735530e+00</td>\n",
       "      <td>-4.757674e+00</td>\n",
       "      <td>-4.305838e+00</td>\n",
       "      <td>-4.209855e+00</td>\n",
       "      <td>-5.107341e+00</td>\n",
       "      <td>-4.478939e+00</td>\n",
       "      <td>-4.317236e+00</td>\n",
       "      <td>-4.423039e+00</td>\n",
       "      <td>-5.165182e+00</td>\n",
       "      <td>-4.410353e+00</td>\n",
       "      <td>-4.386876e+00</td>\n",
       "      <td>-4.330973e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.751325e-01</td>\n",
       "      <td>-6.721857e-01</td>\n",
       "      <td>-6.791459e-01</td>\n",
       "      <td>-6.730523e-01</td>\n",
       "      <td>-6.755947e-01</td>\n",
       "      <td>-6.729232e-01</td>\n",
       "      <td>-6.426318e-01</td>\n",
       "      <td>-6.740780e-01</td>\n",
       "      <td>-6.712440e-01</td>\n",
       "      <td>-6.719356e-01</td>\n",
       "      <td>-6.765309e-01</td>\n",
       "      <td>-6.809367e-01</td>\n",
       "      <td>-6.741967e-01</td>\n",
       "      <td>-6.754506e-01</td>\n",
       "      <td>-6.749848e-01</td>\n",
       "      <td>-6.721729e-01</td>\n",
       "      <td>-6.714868e-01</td>\n",
       "      <td>-6.757744e-01</td>\n",
       "      <td>-6.747496e-01</td>\n",
       "      <td>-6.727404e-01</td>\n",
       "      <td>-6.729052e-01</td>\n",
       "      <td>-6.759158e-01</td>\n",
       "      <td>-6.699772e-01</td>\n",
       "      <td>-6.742090e-01</td>\n",
       "      <td>-6.713979e-01</td>\n",
       "      <td>-6.781400e-01</td>\n",
       "      <td>-6.740929e-01</td>\n",
       "      <td>-6.749454e-01</td>\n",
       "      <td>-9.606321e-01</td>\n",
       "      <td>-6.734678e-01</td>\n",
       "      <td>-6.738401e-01</td>\n",
       "      <td>-6.769885e-01</td>\n",
       "      <td>-6.745658e-01</td>\n",
       "      <td>-6.916378e-01</td>\n",
       "      <td>-6.694053e-01</td>\n",
       "      <td>-6.743219e-01</td>\n",
       "      <td>-6.827701e-01</td>\n",
       "      <td>-6.513220e-01</td>\n",
       "      <td>-6.759257e-01</td>\n",
       "      <td>-6.736188e-01</td>\n",
       "      <td>-6.739956e-01</td>\n",
       "      <td>-6.466441e-01</td>\n",
       "      <td>-6.774577e-01</td>\n",
       "      <td>-6.821900e-01</td>\n",
       "      <td>-6.821369e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.746371e-03</td>\n",
       "      <td>5.447652e-04</td>\n",
       "      <td>-1.437191e-02</td>\n",
       "      <td>-8.394334e-04</td>\n",
       "      <td>2.202375e-04</td>\n",
       "      <td>6.990824e-05</td>\n",
       "      <td>2.409599e-02</td>\n",
       "      <td>1.539050e-03</td>\n",
       "      <td>9.173396e-05</td>\n",
       "      <td>-2.595107e-03</td>\n",
       "      <td>-2.199643e-03</td>\n",
       "      <td>-1.966006e-02</td>\n",
       "      <td>-1.244282e-03</td>\n",
       "      <td>-8.092785e-04</td>\n",
       "      <td>7.092059e-04</td>\n",
       "      <td>1.310282e-03</td>\n",
       "      <td>1.651351e-03</td>\n",
       "      <td>7.067760e-04</td>\n",
       "      <td>2.652317e-03</td>\n",
       "      <td>2.307276e-02</td>\n",
       "      <td>2.924825e-04</td>\n",
       "      <td>5.069986e-04</td>\n",
       "      <td>2.062737e-02</td>\n",
       "      <td>-2.308286e-03</td>\n",
       "      <td>-5.161663e-04</td>\n",
       "      <td>6.095692e-03</td>\n",
       "      <td>1.472654e-02</td>\n",
       "      <td>2.875236e-03</td>\n",
       "      <td>1.045825e-03</td>\n",
       "      <td>2.796334e-04</td>\n",
       "      <td>-1.579900e-03</td>\n",
       "      <td>-1.401311e-03</td>\n",
       "      <td>-1.761759e-03</td>\n",
       "      <td>-1.413743e-02</td>\n",
       "      <td>7.416011e-03</td>\n",
       "      <td>2.885359e-03</td>\n",
       "      <td>-2.260671e-02</td>\n",
       "      <td>-1.759591e-02</td>\n",
       "      <td>-1.493971e-03</td>\n",
       "      <td>-1.016356e-03</td>\n",
       "      <td>1.917060e-03</td>\n",
       "      <td>-6.202920e-03</td>\n",
       "      <td>1.512456e-03</td>\n",
       "      <td>-6.079897e-03</td>\n",
       "      <td>6.637382e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.724050e-01</td>\n",
       "      <td>6.753186e-01</td>\n",
       "      <td>6.795468e-01</td>\n",
       "      <td>6.781186e-01</td>\n",
       "      <td>6.747798e-01</td>\n",
       "      <td>6.747967e-01</td>\n",
       "      <td>6.518074e-01</td>\n",
       "      <td>6.743741e-01</td>\n",
       "      <td>6.766323e-01</td>\n",
       "      <td>6.768308e-01</td>\n",
       "      <td>6.732292e-01</td>\n",
       "      <td>6.580844e-01</td>\n",
       "      <td>6.738479e-01</td>\n",
       "      <td>6.737908e-01</td>\n",
       "      <td>6.754615e-01</td>\n",
       "      <td>6.741725e-01</td>\n",
       "      <td>6.723684e-01</td>\n",
       "      <td>6.738985e-01</td>\n",
       "      <td>6.766587e-01</td>\n",
       "      <td>6.900914e-01</td>\n",
       "      <td>6.743317e-01</td>\n",
       "      <td>6.770309e-01</td>\n",
       "      <td>6.911330e-01</td>\n",
       "      <td>6.746406e-01</td>\n",
       "      <td>6.741390e-01</td>\n",
       "      <td>6.866950e-01</td>\n",
       "      <td>6.859042e-01</td>\n",
       "      <td>6.728554e-01</td>\n",
       "      <td>9.627237e-01</td>\n",
       "      <td>6.784543e-01</td>\n",
       "      <td>6.752713e-01</td>\n",
       "      <td>6.732924e-01</td>\n",
       "      <td>6.737657e-01</td>\n",
       "      <td>6.638180e-01</td>\n",
       "      <td>6.725604e-01</td>\n",
       "      <td>6.736552e-01</td>\n",
       "      <td>6.627977e-01</td>\n",
       "      <td>6.431158e-01</td>\n",
       "      <td>6.744227e-01</td>\n",
       "      <td>6.696912e-01</td>\n",
       "      <td>6.760229e-01</td>\n",
       "      <td>6.609153e-01</td>\n",
       "      <td>6.763977e-01</td>\n",
       "      <td>6.808362e-01</td>\n",
       "      <td>6.866128e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.316499e+00</td>\n",
       "      <td>4.414243e+00</td>\n",
       "      <td>4.874662e+00</td>\n",
       "      <td>4.827667e+00</td>\n",
       "      <td>4.113032e+00</td>\n",
       "      <td>4.633689e+00</td>\n",
       "      <td>5.135737e+00</td>\n",
       "      <td>4.390770e+00</td>\n",
       "      <td>4.307610e+00</td>\n",
       "      <td>4.821050e+00</td>\n",
       "      <td>4.143131e+00</td>\n",
       "      <td>5.058809e+00</td>\n",
       "      <td>4.734143e+00</td>\n",
       "      <td>4.672795e+00</td>\n",
       "      <td>4.212661e+00</td>\n",
       "      <td>4.405390e+00</td>\n",
       "      <td>4.894508e+00</td>\n",
       "      <td>4.325929e+00</td>\n",
       "      <td>4.409923e+00</td>\n",
       "      <td>4.739553e+00</td>\n",
       "      <td>4.917404e+00</td>\n",
       "      <td>4.592894e+00</td>\n",
       "      <td>3.875280e+00</td>\n",
       "      <td>4.206332e+00</td>\n",
       "      <td>4.687976e+00</td>\n",
       "      <td>4.229672e+00</td>\n",
       "      <td>4.690586e+00</td>\n",
       "      <td>4.439675e+00</td>\n",
       "      <td>4.809435e+00</td>\n",
       "      <td>4.459823e+00</td>\n",
       "      <td>4.348097e+00</td>\n",
       "      <td>4.157264e+00</td>\n",
       "      <td>4.389767e+00</td>\n",
       "      <td>5.547334e+00</td>\n",
       "      <td>4.997588e+00</td>\n",
       "      <td>4.195871e+00</td>\n",
       "      <td>5.348160e+00</td>\n",
       "      <td>4.794630e+00</td>\n",
       "      <td>4.354407e+00</td>\n",
       "      <td>4.581107e+00</td>\n",
       "      <td>4.206725e+00</td>\n",
       "      <td>4.469743e+00</td>\n",
       "      <td>4.346286e+00</td>\n",
       "      <td>4.250902e+00</td>\n",
       "      <td>4.492914e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 x0            x1            x2            x3            x4  \\\n",
       "count  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05   \n",
       "mean   4.707346e-18  8.082424e-18 -3.330669e-17 -2.255973e-17 -1.150191e-17   \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min   -4.288823e+00 -4.144980e+00 -4.388522e+00 -4.396283e+00 -4.460683e+00   \n",
       "25%   -6.751325e-01 -6.721857e-01 -6.791459e-01 -6.730523e-01 -6.755947e-01   \n",
       "50%   -2.746371e-03  5.447652e-04 -1.437191e-02 -8.394334e-04  2.202375e-04   \n",
       "75%    6.724050e-01  6.753186e-01  6.795468e-01  6.781186e-01  6.747798e-01   \n",
       "max    4.316499e+00  4.414243e+00  4.874662e+00  4.827667e+00  4.113032e+00   \n",
       "\n",
       "                 x5            x7            x8            x9           x10  \\\n",
       "count  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05   \n",
       "mean  -7.105427e-19 -2.340350e-17  2.646772e-17 -1.172396e-17 -2.233769e-17   \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min   -4.412028e+00 -5.691441e+00 -4.231245e+00 -4.404195e+00 -4.613370e+00   \n",
       "25%   -6.729232e-01 -6.426318e-01 -6.740780e-01 -6.712440e-01 -6.719356e-01   \n",
       "50%    6.990824e-05  2.409599e-02  1.539050e-03  9.173396e-05 -2.595107e-03   \n",
       "75%    6.747967e-01  6.518074e-01  6.743741e-01  6.766323e-01  6.768308e-01   \n",
       "max    4.633689e+00  5.135737e+00  4.390770e+00  4.307610e+00  4.821050e+00   \n",
       "\n",
       "                x11           x12           x13           x14           x15  \\\n",
       "count  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05   \n",
       "mean   4.618528e-18  1.224576e-17  1.563194e-17  2.327027e-17  2.345901e-17   \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min   -4.347593e+00 -4.262139e+00 -4.326084e+00 -4.439441e+00 -5.197628e+00   \n",
       "25%   -6.765309e-01 -6.809367e-01 -6.741967e-01 -6.754506e-01 -6.749848e-01   \n",
       "50%   -2.199643e-03 -1.966006e-02 -1.244282e-03 -8.092785e-04  7.092059e-04   \n",
       "75%    6.732292e-01  6.580844e-01  6.738479e-01  6.737908e-01  6.754615e-01   \n",
       "max    4.143131e+00  5.058809e+00  4.734143e+00  4.672795e+00  4.212661e+00   \n",
       "\n",
       "                x16           x17           x18           x19           x20  \\\n",
       "count  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05   \n",
       "mean   1.070255e-17  2.202682e-17 -1.254552e-17  1.652012e-17  1.492140e-17   \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min   -5.226939e+00 -4.546090e+00 -4.451716e+00 -4.695258e+00 -4.646718e+00   \n",
       "25%   -6.721729e-01 -6.714868e-01 -6.757744e-01 -6.747496e-01 -6.727404e-01   \n",
       "50%    1.310282e-03  1.651351e-03  7.067760e-04  2.652317e-03  2.307276e-02   \n",
       "75%    6.741725e-01  6.723684e-01  6.738985e-01  6.766587e-01  6.900914e-01   \n",
       "max    4.405390e+00  4.894508e+00  4.325929e+00  4.409923e+00  4.739553e+00   \n",
       "\n",
       "                x21           x22           x23           x25           x26  \\\n",
       "count  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05   \n",
       "mean   3.410605e-17  2.930989e-18  5.107026e-18  1.092459e-17 -3.552714e-19   \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min   -4.620455e+00 -4.370773e+00 -4.518880e+00 -5.036419e+00 -4.573763e+00   \n",
       "25%   -6.729052e-01 -6.759158e-01 -6.699772e-01 -6.742090e-01 -6.713979e-01   \n",
       "50%    2.924825e-04  5.069986e-04  2.062737e-02 -2.308286e-03 -5.161663e-04   \n",
       "75%    6.743317e-01  6.770309e-01  6.911330e-01  6.746406e-01  6.741390e-01   \n",
       "max    4.917404e+00  4.592894e+00  3.875280e+00  4.206332e+00  4.687976e+00   \n",
       "\n",
       "                x27           x28           x31           x32           x33  \\\n",
       "count  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05   \n",
       "mean  -1.598721e-17 -2.183809e-17  1.845191e-17  3.046452e-17  3.124168e-17   \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min   -4.724281e+00 -5.051167e+00 -4.438996e+00 -4.807344e+00 -4.260227e+00   \n",
       "25%   -6.781400e-01 -6.740929e-01 -6.749454e-01 -9.606321e-01 -6.734678e-01   \n",
       "50%    6.095692e-03  1.472654e-02  2.875236e-03  1.045825e-03  2.796334e-04   \n",
       "75%    6.866950e-01  6.859042e-01  6.728554e-01  9.627237e-01  6.784543e-01   \n",
       "max    4.229672e+00  4.690586e+00  4.439675e+00  4.809435e+00  4.459823e+00   \n",
       "\n",
       "                x34           x35           x36           x37           x38  \\\n",
       "count  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05   \n",
       "mean  -9.325873e-18 -5.595524e-18  5.773160e-18  2.344791e-17  1.098233e-16   \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min   -4.507126e+00 -4.206687e+00 -4.314053e+00 -4.735530e+00 -4.757674e+00   \n",
       "25%   -6.738401e-01 -6.769885e-01 -6.745658e-01 -6.916378e-01 -6.694053e-01   \n",
       "50%   -1.579900e-03 -1.401311e-03 -1.761759e-03 -1.413743e-02  7.416011e-03   \n",
       "75%    6.752713e-01  6.732924e-01  6.737657e-01  6.638180e-01  6.725604e-01   \n",
       "max    4.348097e+00  4.157264e+00  4.389767e+00  5.547334e+00  4.997588e+00   \n",
       "\n",
       "                x39           x40           x42           x43           x44  \\\n",
       "count  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05   \n",
       "mean   3.641532e-18 -1.483258e-17 -2.478018e-17 -2.309264e-17 -2.726708e-17   \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min   -4.305838e+00 -4.209855e+00 -5.107341e+00 -4.478939e+00 -4.317236e+00   \n",
       "25%   -6.743219e-01 -6.827701e-01 -6.513220e-01 -6.759257e-01 -6.736188e-01   \n",
       "50%    2.885359e-03 -2.260671e-02 -1.759591e-02 -1.493971e-03 -1.016356e-03   \n",
       "75%    6.736552e-01  6.627977e-01  6.431158e-01  6.744227e-01  6.696912e-01   \n",
       "max    4.195871e+00  5.348160e+00  4.794630e+00  4.354407e+00  4.581107e+00   \n",
       "\n",
       "                x45           x46           x47           x48           x49  \\\n",
       "count  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05  1.600000e+05   \n",
       "mean   1.803002e-17  6.439294e-18  1.216804e-17  1.803002e-17  1.865175e-18   \n",
       "std    1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
       "min   -4.423039e+00 -5.165182e+00 -4.410353e+00 -4.386876e+00 -4.330973e+00   \n",
       "25%   -6.739956e-01 -6.466441e-01 -6.774577e-01 -6.821900e-01 -6.821369e-01   \n",
       "50%    1.917060e-03 -6.202920e-03  1.512456e-03 -6.079897e-03  6.637382e-03   \n",
       "75%    6.760229e-01  6.609153e-01  6.763977e-01  6.808362e-01  6.866128e-01   \n",
       "max    4.206725e+00  4.469743e+00  4.346286e+00  4.250902e+00  4.492914e+00   \n",
       "\n",
       "         x24_Missing    x24_america       x24_asia     x24_euorpe  \\\n",
       "count  160000.000000  160000.000000  160000.000000  160000.000000   \n",
       "mean        0.000175       0.027931       0.868531       0.103362   \n",
       "std         0.013228       0.164776       0.337913       0.304433   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       1.000000       0.000000   \n",
       "50%         0.000000       0.000000       1.000000       0.000000   \n",
       "75%         0.000000       0.000000       1.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             x29_Apr        x29_Aug        x29_Dec        x29_Feb  \\\n",
       "count  160000.000000  160000.000000  160000.000000  160000.000000   \n",
       "mean        0.042256       0.183787       0.000144       0.000875   \n",
       "std         0.201174       0.387312       0.011989       0.029568   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             x29_Jan        x29_Jul        x29_Jun        x29_Mar  \\\n",
       "count  160000.000000  160000.000000  160000.000000  160000.000000   \n",
       "mean        0.000056       0.284806       0.258306       0.007694   \n",
       "std         0.007500       0.451324       0.437705       0.087376   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       1.000000       1.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             x29_May    x29_Missing        x29_Nov        x29_Oct  \\\n",
       "count  160000.000000  160000.000000  160000.000000  160000.000000   \n",
       "mean        0.137119       0.000188       0.002106       0.015044   \n",
       "std         0.343974       0.013692       0.045846       0.121727   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             x29_Sep    x30_Missing     x30_friday     x30_monday  \\\n",
       "count  160000.000000  160000.000000  160000.000000  160000.000000   \n",
       "mean        0.067619       0.000188       0.003525       0.003050   \n",
       "std         0.251091       0.013692       0.059267       0.055143   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "         x30_thurday    x30_tuesday  x30_wednesday  \n",
       "count  160000.000000  160000.000000  160000.000000  \n",
       "mean        0.183931       0.174712       0.634594  \n",
       "std         0.387429       0.379722       0.481545  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       1.000000  \n",
       "75%         0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2,y,test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4013828125"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[y_train==1])/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables that will not be tuned\n",
    "params = {\n",
    "   \"objective\": \"binary:logistic\",\n",
    "   \"booster\": \"gbtree\",\n",
    "   \"eval_metric\": \"logloss\",\n",
    "    \"tree_method\": \"hist\"\n",
    "}\n",
    "\n",
    "# create a gridsearch for the variables that will be tuned\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight, subsample, colsample, eta, gamma, alpha, lambd)\n",
    "    for max_depth in range(6,8)\n",
    "    for min_child_weight in range(5,7)\n",
    "    for subsample in [i/10. for i in range(5,7)]\n",
    "    for colsample in [i/10. for i in range(5,7)]\n",
    "    for eta in [.01, .05]\n",
    "    for gamma in [0, .1]\n",
    "    for alpha in [0]\n",
    "    for lambd in [1]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def myscore(predt: np.ndarray, dtrain: xgb.DMatrix) -> [str, float]:\n",
    "    ''' ...'''\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred = [1 if p >= 0.5 else 0 for p in predt]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    tn_score = tn * 0\n",
    "    tp_score = tp * 0\n",
    "    fp_score = fp * -25\n",
    "    fn_score = fn * -125\n",
    "    score = tn_score + tp_score + fp_score + fn_score\n",
    "    return 'CustomScore', float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.get_label()\n",
    "# predt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=6, min_child_weight=5, subsample=0.5, colsample=0.5, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.3211116 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.5, colsample=0.5, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.32109019999999994 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.5, colsample=0.5, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.21717679999999998 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.5, colsample=0.5, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2171942 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.5, colsample=0.6, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.3134284 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.5, colsample=0.6, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.3132309999999999 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.5, colsample=0.6, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2132316 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.5, colsample=0.6, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.21235880000000001 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.6, colsample=0.5, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.32158640000000005 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.6, colsample=0.5, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.3216784 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.6, colsample=0.5, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2164176 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.6, colsample=0.5, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.21612899999999996 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.6, colsample=0.6, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.3132892 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.6, colsample=0.6, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.313365 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.6, colsample=0.6, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.212594 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=5, subsample=0.6, colsample=0.6, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.21233019999999997 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.5, colsample=0.5, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.32112379999999996 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.5, colsample=0.5, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.3211828 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.5, colsample=0.5, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2177212 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.5, colsample=0.5, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.21826280000000003 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.5, colsample=0.6, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.3134654 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.5, colsample=0.6, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.31351880000000004 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.5, colsample=0.6, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.21370180000000003 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.5, colsample=0.6, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2128982 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.6, colsample=0.5, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.3217956 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.6, colsample=0.5, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.3217358 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.6, colsample=0.5, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.216554 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.6, colsample=0.5, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2167818 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.6, colsample=0.6, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.31349360000000004 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.6, colsample=0.6, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.3135612 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.6, colsample=0.6, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2129016 for 498 rounds\n",
      "CV with max_depth=6, min_child_weight=6, subsample=0.6, colsample=0.6, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.21310540000000003 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.5, colsample=0.5, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.289351 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.5, colsample=0.5, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2891578 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.5, colsample=0.5, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.20100160000000003 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.5, colsample=0.5, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2006912 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.5, colsample=0.6, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2806804 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.5, colsample=0.6, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.28063720000000003 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.5, colsample=0.6, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.19644520000000001 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.5, colsample=0.6, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.1971582 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.6, colsample=0.5, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.28931019999999996 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.6, colsample=0.5, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.289365 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.6, colsample=0.5, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.1997254 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.6, colsample=0.5, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.1992658 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.6, colsample=0.6, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2803774 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.6, colsample=0.6, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2802918 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.6, colsample=0.6, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.1948134 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=5, subsample=0.6, colsample=0.6, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.1949338 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.5, colsample=0.5, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.28979260000000007 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.5, colsample=0.5, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2898576 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.5, colsample=0.5, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2014472 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.5, colsample=0.5, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2010668 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.5, colsample=0.6, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2810038 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.5, colsample=0.6, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2809198 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.5, colsample=0.6, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.19773860000000001 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.5, colsample=0.6, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.1972486 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.6, colsample=0.5, eta=0.01, gamma=0, alpha=0, lambd=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLogLoss 0.28957 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.6, colsample=0.5, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2896592 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.6, colsample=0.5, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2004042 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.6, colsample=0.5, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2005944 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.6, colsample=0.6, eta=0.01, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.2808118 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.6, colsample=0.6, eta=0.01, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.2806392 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.6, colsample=0.6, eta=0.05, gamma=0, alpha=0, lambd=1\n",
      "\tLogLoss 0.1963032 for 498 rounds\n",
      "CV with max_depth=7, min_child_weight=6, subsample=0.6, colsample=0.6, eta=0.05, gamma=0.1, alpha=0, lambd=1\n",
      "\tLogLoss 0.19595780000000002 for 498 rounds\n",
      "Best params: 7, 5, 0.6, 0.6, 0.05, 0, 0, 1 Logloss: 0.1948134\n",
      "Wall time: 1h 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set the number of boosting rounds\n",
    "num_boost_round = 499\n",
    "\n",
    "\n",
    "# Define initial best params and MAE\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight, subsample, colsample, eta, gamma, alpha, lambd in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}, subsample={}, colsample={}, eta={}, gamma={}, alpha={}, lambd={}\".format(\n",
    "        max_depth, min_child_weight, subsample, colsample, eta, gamma, alpha, lambd))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    params['eta'] = eta\n",
    "    params['gamma'] = gamma\n",
    "    params['alpha'] = alpha\n",
    "    params['lambda'] = lambd\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'logloss'},\n",
    "#         feval=myscore,\n",
    "#         metrics={'logloss', CustomScore},\n",
    "#         evals=[(dtrain, 'dtrain'), (dtest, 'dtest')],\n",
    "        early_stopping_rounds=5\n",
    "    )\n",
    "    # Update best MAE\n",
    "    mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tLogLoss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (max_depth, min_child_weight, subsample, colsample, eta, gamma, alpha, lambd)\n",
    "print(\"Best params: {}, {}, {}, {}, {}, {}, {}, {} Logloss: {}\".format(best_params[0], best_params[1], best_params[2], best_params[3], best_params[4], best_params[5], best_params[6], best_params[7], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tdtrain-logloss:0.67568\tdtest-logloss:0.67593\tdtrain-CustomScore:-6422125.00000\tdtest-CustomScore:-1602500.00000\n",
      "[1]\tdtrain-logloss:0.66077\tdtest-logloss:0.66119\tdtrain-CustomScore:-6422125.00000\tdtest-CustomScore:-1602500.00000\n",
      "[2]\tdtrain-logloss:0.64619\tdtest-logloss:0.64660\tdtrain-CustomScore:-6422125.00000\tdtest-CustomScore:-1602500.00000\n",
      "[3]\tdtrain-logloss:0.62985\tdtest-logloss:0.63030\tdtrain-CustomScore:-6422125.00000\tdtest-CustomScore:-1602500.00000\n",
      "[4]\tdtrain-logloss:0.61426\tdtest-logloss:0.61499\tdtrain-CustomScore:-6422125.00000\tdtest-CustomScore:-1602500.00000\n",
      "[5]\tdtrain-logloss:0.59925\tdtest-logloss:0.60007\tdtrain-CustomScore:-6412750.00000\tdtest-CustomScore:-1600000.00000\n",
      "[6]\tdtrain-logloss:0.58756\tdtest-logloss:0.58849\tdtrain-CustomScore:-6123650.00000\tdtest-CustomScore:-1529250.00000\n",
      "[7]\tdtrain-logloss:0.57639\tdtest-logloss:0.57746\tdtrain-CustomScore:-5800600.00000\tdtest-CustomScore:-1450800.00000\n",
      "[8]\tdtrain-logloss:0.56710\tdtest-logloss:0.56813\tdtrain-CustomScore:-5630225.00000\tdtest-CustomScore:-1407700.00000\n",
      "[9]\tdtrain-logloss:0.55405\tdtest-logloss:0.55522\tdtrain-CustomScore:-5318300.00000\tdtest-CustomScore:-1330475.00000\n",
      "[10]\tdtrain-logloss:0.54254\tdtest-logloss:0.54371\tdtrain-CustomScore:-5046375.00000\tdtest-CustomScore:-1262675.00000\n",
      "[11]\tdtrain-logloss:0.53476\tdtest-logloss:0.53604\tdtrain-CustomScore:-4874700.00000\tdtest-CustomScore:-1223375.00000\n",
      "[12]\tdtrain-logloss:0.52678\tdtest-logloss:0.52824\tdtrain-CustomScore:-4682100.00000\tdtest-CustomScore:-1171675.00000\n",
      "[13]\tdtrain-logloss:0.51706\tdtest-logloss:0.51879\tdtrain-CustomScore:-4460325.00000\tdtest-CustomScore:-1117050.00000\n",
      "[14]\tdtrain-logloss:0.50813\tdtest-logloss:0.51003\tdtrain-CustomScore:-4274375.00000\tdtest-CustomScore:-1075025.00000\n",
      "[15]\tdtrain-logloss:0.50128\tdtest-logloss:0.50336\tdtrain-CustomScore:-4180500.00000\tdtest-CustomScore:-1054350.00000\n",
      "[16]\tdtrain-logloss:0.49418\tdtest-logloss:0.49635\tdtrain-CustomScore:-4041250.00000\tdtest-CustomScore:-1016600.00000\n",
      "[17]\tdtrain-logloss:0.48668\tdtest-logloss:0.48915\tdtrain-CustomScore:-3912225.00000\tdtest-CustomScore:-984400.00000\n",
      "[18]\tdtrain-logloss:0.47748\tdtest-logloss:0.48009\tdtrain-CustomScore:-3745975.00000\tdtest-CustomScore:-941900.00000\n",
      "[19]\tdtrain-logloss:0.47210\tdtest-logloss:0.47492\tdtrain-CustomScore:-3662650.00000\tdtest-CustomScore:-923175.00000\n",
      "[20]\tdtrain-logloss:0.46607\tdtest-logloss:0.46904\tdtrain-CustomScore:-3570500.00000\tdtest-CustomScore:-905575.00000\n",
      "[21]\tdtrain-logloss:0.45882\tdtest-logloss:0.46187\tdtrain-CustomScore:-3473750.00000\tdtest-CustomScore:-874700.00000\n",
      "[22]\tdtrain-logloss:0.45503\tdtest-logloss:0.45822\tdtrain-CustomScore:-3428450.00000\tdtest-CustomScore:-866400.00000\n",
      "[23]\tdtrain-logloss:0.44843\tdtest-logloss:0.45172\tdtrain-CustomScore:-3317350.00000\tdtest-CustomScore:-840100.00000\n",
      "[24]\tdtrain-logloss:0.44196\tdtest-logloss:0.44533\tdtrain-CustomScore:-3221000.00000\tdtest-CustomScore:-822175.00000\n",
      "[25]\tdtrain-logloss:0.43750\tdtest-logloss:0.44085\tdtrain-CustomScore:-3168200.00000\tdtest-CustomScore:-803625.00000\n",
      "[26]\tdtrain-logloss:0.43024\tdtest-logloss:0.43381\tdtrain-CustomScore:-3069250.00000\tdtest-CustomScore:-780575.00000\n",
      "[27]\tdtrain-logloss:0.42346\tdtest-logloss:0.42719\tdtrain-CustomScore:-2986150.00000\tdtest-CustomScore:-758775.00000\n",
      "[28]\tdtrain-logloss:0.41917\tdtest-logloss:0.42296\tdtrain-CustomScore:-2935100.00000\tdtest-CustomScore:-747775.00000\n",
      "[29]\tdtrain-logloss:0.41401\tdtest-logloss:0.41787\tdtrain-CustomScore:-2884150.00000\tdtest-CustomScore:-732000.00000\n",
      "[30]\tdtrain-logloss:0.41078\tdtest-logloss:0.41474\tdtrain-CustomScore:-2851500.00000\tdtest-CustomScore:-722375.00000\n",
      "[31]\tdtrain-logloss:0.40670\tdtest-logloss:0.41079\tdtrain-CustomScore:-2807750.00000\tdtest-CustomScore:-713650.00000\n",
      "[32]\tdtrain-logloss:0.40213\tdtest-logloss:0.40631\tdtrain-CustomScore:-2767150.00000\tdtest-CustomScore:-702250.00000\n",
      "[33]\tdtrain-logloss:0.39798\tdtest-logloss:0.40228\tdtrain-CustomScore:-2720475.00000\tdtest-CustomScore:-693175.00000\n",
      "[34]\tdtrain-logloss:0.39373\tdtest-logloss:0.39803\tdtrain-CustomScore:-2675950.00000\tdtest-CustomScore:-680850.00000\n",
      "[35]\tdtrain-logloss:0.38932\tdtest-logloss:0.39373\tdtrain-CustomScore:-2635175.00000\tdtest-CustomScore:-671675.00000\n",
      "[36]\tdtrain-logloss:0.38567\tdtest-logloss:0.39020\tdtrain-CustomScore:-2600875.00000\tdtest-CustomScore:-665275.00000\n",
      "[37]\tdtrain-logloss:0.38207\tdtest-logloss:0.38662\tdtrain-CustomScore:-2564550.00000\tdtest-CustomScore:-655250.00000\n",
      "[38]\tdtrain-logloss:0.37876\tdtest-logloss:0.38332\tdtrain-CustomScore:-2529500.00000\tdtest-CustomScore:-645350.00000\n",
      "[39]\tdtrain-logloss:0.37414\tdtest-logloss:0.37876\tdtrain-CustomScore:-2472775.00000\tdtest-CustomScore:-631500.00000\n",
      "[40]\tdtrain-logloss:0.37113\tdtest-logloss:0.37588\tdtrain-CustomScore:-2448525.00000\tdtest-CustomScore:-625800.00000\n",
      "[41]\tdtrain-logloss:0.36843\tdtest-logloss:0.37323\tdtrain-CustomScore:-2428375.00000\tdtest-CustomScore:-621625.00000\n",
      "[42]\tdtrain-logloss:0.36504\tdtest-logloss:0.36994\tdtrain-CustomScore:-2388775.00000\tdtest-CustomScore:-612175.00000\n",
      "[43]\tdtrain-logloss:0.36200\tdtest-logloss:0.36697\tdtrain-CustomScore:-2352875.00000\tdtest-CustomScore:-603250.00000\n",
      "[44]\tdtrain-logloss:0.35975\tdtest-logloss:0.36474\tdtrain-CustomScore:-2335525.00000\tdtest-CustomScore:-598800.00000\n",
      "[45]\tdtrain-logloss:0.35610\tdtest-logloss:0.36124\tdtrain-CustomScore:-2299150.00000\tdtest-CustomScore:-588650.00000\n",
      "[46]\tdtrain-logloss:0.35375\tdtest-logloss:0.35900\tdtrain-CustomScore:-2281100.00000\tdtest-CustomScore:-581775.00000\n",
      "[47]\tdtrain-logloss:0.35119\tdtest-logloss:0.35658\tdtrain-CustomScore:-2256300.00000\tdtest-CustomScore:-577275.00000\n",
      "[48]\tdtrain-logloss:0.34768\tdtest-logloss:0.35308\tdtrain-CustomScore:-2214425.00000\tdtest-CustomScore:-567825.00000\n",
      "[49]\tdtrain-logloss:0.34576\tdtest-logloss:0.35128\tdtrain-CustomScore:-2203300.00000\tdtest-CustomScore:-564200.00000\n",
      "[50]\tdtrain-logloss:0.34315\tdtest-logloss:0.34874\tdtrain-CustomScore:-2175000.00000\tdtest-CustomScore:-557850.00000\n",
      "[51]\tdtrain-logloss:0.34002\tdtest-logloss:0.34567\tdtrain-CustomScore:-2145825.00000\tdtest-CustomScore:-551700.00000\n",
      "[52]\tdtrain-logloss:0.33741\tdtest-logloss:0.34308\tdtrain-CustomScore:-2121900.00000\tdtest-CustomScore:-545450.00000\n",
      "[53]\tdtrain-logloss:0.33452\tdtest-logloss:0.34027\tdtrain-CustomScore:-2094675.00000\tdtest-CustomScore:-539650.00000\n",
      "[54]\tdtrain-logloss:0.33207\tdtest-logloss:0.33794\tdtrain-CustomScore:-2077350.00000\tdtest-CustomScore:-532575.00000\n",
      "[55]\tdtrain-logloss:0.33056\tdtest-logloss:0.33644\tdtrain-CustomScore:-2073125.00000\tdtest-CustomScore:-530600.00000\n",
      "[56]\tdtrain-logloss:0.32908\tdtest-logloss:0.33501\tdtrain-CustomScore:-2063700.00000\tdtest-CustomScore:-528425.00000\n",
      "[57]\tdtrain-logloss:0.32665\tdtest-logloss:0.33264\tdtrain-CustomScore:-2042525.00000\tdtest-CustomScore:-522375.00000\n",
      "[58]\tdtrain-logloss:0.32441\tdtest-logloss:0.33053\tdtrain-CustomScore:-2020850.00000\tdtest-CustomScore:-516525.00000\n",
      "[59]\tdtrain-logloss:0.32287\tdtest-logloss:0.32902\tdtrain-CustomScore:-2010375.00000\tdtest-CustomScore:-514600.00000\n",
      "[60]\tdtrain-logloss:0.32121\tdtest-logloss:0.32746\tdtrain-CustomScore:-1998000.00000\tdtest-CustomScore:-511775.00000\n",
      "[61]\tdtrain-logloss:0.32005\tdtest-logloss:0.32640\tdtrain-CustomScore:-1990050.00000\tdtest-CustomScore:-510250.00000\n",
      "[62]\tdtrain-logloss:0.31800\tdtest-logloss:0.32440\tdtrain-CustomScore:-1971475.00000\tdtest-CustomScore:-506575.00000\n",
      "[63]\tdtrain-logloss:0.31656\tdtest-logloss:0.32304\tdtrain-CustomScore:-1958200.00000\tdtest-CustomScore:-504100.00000\n",
      "[64]\tdtrain-logloss:0.31465\tdtest-logloss:0.32114\tdtrain-CustomScore:-1942300.00000\tdtest-CustomScore:-501800.00000\n",
      "[65]\tdtrain-logloss:0.31210\tdtest-logloss:0.31877\tdtrain-CustomScore:-1915000.00000\tdtest-CustomScore:-495700.00000\n",
      "[66]\tdtrain-logloss:0.31022\tdtest-logloss:0.31695\tdtrain-CustomScore:-1896375.00000\tdtest-CustomScore:-491000.00000\n",
      "[67]\tdtrain-logloss:0.30822\tdtest-logloss:0.31500\tdtrain-CustomScore:-1880800.00000\tdtest-CustomScore:-487575.00000\n",
      "[68]\tdtrain-logloss:0.30662\tdtest-logloss:0.31347\tdtrain-CustomScore:-1868425.00000\tdtest-CustomScore:-484325.00000\n",
      "[69]\tdtrain-logloss:0.30523\tdtest-logloss:0.31217\tdtrain-CustomScore:-1855375.00000\tdtest-CustomScore:-483550.00000\n",
      "[70]\tdtrain-logloss:0.30413\tdtest-logloss:0.31113\tdtrain-CustomScore:-1847850.00000\tdtest-CustomScore:-482175.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71]\tdtrain-logloss:0.30298\tdtest-logloss:0.31004\tdtrain-CustomScore:-1839175.00000\tdtest-CustomScore:-479550.00000\n",
      "[72]\tdtrain-logloss:0.30129\tdtest-logloss:0.30842\tdtrain-CustomScore:-1825750.00000\tdtest-CustomScore:-477250.00000\n",
      "[73]\tdtrain-logloss:0.29971\tdtest-logloss:0.30696\tdtrain-CustomScore:-1816075.00000\tdtest-CustomScore:-474475.00000\n",
      "[74]\tdtrain-logloss:0.29743\tdtest-logloss:0.30482\tdtrain-CustomScore:-1796775.00000\tdtest-CustomScore:-470350.00000\n",
      "[75]\tdtrain-logloss:0.29543\tdtest-logloss:0.30286\tdtrain-CustomScore:-1779300.00000\tdtest-CustomScore:-465250.00000\n",
      "[76]\tdtrain-logloss:0.29407\tdtest-logloss:0.30169\tdtrain-CustomScore:-1766275.00000\tdtest-CustomScore:-462100.00000\n",
      "[77]\tdtrain-logloss:0.29288\tdtest-logloss:0.30058\tdtrain-CustomScore:-1756975.00000\tdtest-CustomScore:-460525.00000\n",
      "[78]\tdtrain-logloss:0.29159\tdtest-logloss:0.29939\tdtrain-CustomScore:-1746175.00000\tdtest-CustomScore:-458250.00000\n",
      "[79]\tdtrain-logloss:0.29053\tdtest-logloss:0.29838\tdtrain-CustomScore:-1738250.00000\tdtest-CustomScore:-455625.00000\n",
      "[80]\tdtrain-logloss:0.28875\tdtest-logloss:0.29664\tdtrain-CustomScore:-1722625.00000\tdtest-CustomScore:-451000.00000\n",
      "[81]\tdtrain-logloss:0.28764\tdtest-logloss:0.29557\tdtrain-CustomScore:-1715600.00000\tdtest-CustomScore:-449450.00000\n",
      "[82]\tdtrain-logloss:0.28588\tdtest-logloss:0.29388\tdtrain-CustomScore:-1701900.00000\tdtest-CustomScore:-444750.00000\n",
      "[83]\tdtrain-logloss:0.28444\tdtest-logloss:0.29250\tdtrain-CustomScore:-1688575.00000\tdtest-CustomScore:-441800.00000\n",
      "[84]\tdtrain-logloss:0.28261\tdtest-logloss:0.29078\tdtrain-CustomScore:-1670975.00000\tdtest-CustomScore:-437700.00000\n",
      "[85]\tdtrain-logloss:0.28176\tdtest-logloss:0.28999\tdtrain-CustomScore:-1666500.00000\tdtest-CustomScore:-436575.00000\n",
      "[86]\tdtrain-logloss:0.28032\tdtest-logloss:0.28861\tdtrain-CustomScore:-1656125.00000\tdtest-CustomScore:-434050.00000\n",
      "[87]\tdtrain-logloss:0.27869\tdtest-logloss:0.28703\tdtrain-CustomScore:-1646050.00000\tdtest-CustomScore:-431450.00000\n",
      "[88]\tdtrain-logloss:0.27747\tdtest-logloss:0.28588\tdtrain-CustomScore:-1635050.00000\tdtest-CustomScore:-428475.00000\n",
      "[89]\tdtrain-logloss:0.27653\tdtest-logloss:0.28504\tdtrain-CustomScore:-1628425.00000\tdtest-CustomScore:-426500.00000\n",
      "[90]\tdtrain-logloss:0.27525\tdtest-logloss:0.28381\tdtrain-CustomScore:-1617425.00000\tdtest-CustomScore:-424275.00000\n",
      "[91]\tdtrain-logloss:0.27457\tdtest-logloss:0.28320\tdtrain-CustomScore:-1613450.00000\tdtest-CustomScore:-422625.00000\n",
      "[92]\tdtrain-logloss:0.27281\tdtest-logloss:0.28145\tdtrain-CustomScore:-1595925.00000\tdtest-CustomScore:-417275.00000\n",
      "[93]\tdtrain-logloss:0.27179\tdtest-logloss:0.28049\tdtrain-CustomScore:-1587575.00000\tdtest-CustomScore:-417375.00000\n",
      "[94]\tdtrain-logloss:0.27061\tdtest-logloss:0.27944\tdtrain-CustomScore:-1579675.00000\tdtest-CustomScore:-414900.00000\n",
      "[95]\tdtrain-logloss:0.26923\tdtest-logloss:0.27821\tdtrain-CustomScore:-1568875.00000\tdtest-CustomScore:-412500.00000\n",
      "[96]\tdtrain-logloss:0.26878\tdtest-logloss:0.27780\tdtrain-CustomScore:-1565275.00000\tdtest-CustomScore:-411900.00000\n",
      "[97]\tdtrain-logloss:0.26712\tdtest-logloss:0.27629\tdtrain-CustomScore:-1547700.00000\tdtest-CustomScore:-407800.00000\n",
      "[98]\tdtrain-logloss:0.26632\tdtest-logloss:0.27551\tdtrain-CustomScore:-1541600.00000\tdtest-CustomScore:-407000.00000\n",
      "[99]\tdtrain-logloss:0.26575\tdtest-logloss:0.27497\tdtrain-CustomScore:-1537475.00000\tdtest-CustomScore:-406025.00000\n",
      "[100]\tdtrain-logloss:0.26523\tdtest-logloss:0.27448\tdtrain-CustomScore:-1533125.00000\tdtest-CustomScore:-406100.00000\n",
      "[101]\tdtrain-logloss:0.26395\tdtest-logloss:0.27332\tdtrain-CustomScore:-1524125.00000\tdtest-CustomScore:-403425.00000\n",
      "[102]\tdtrain-logloss:0.26300\tdtest-logloss:0.27244\tdtrain-CustomScore:-1516550.00000\tdtest-CustomScore:-401275.00000\n",
      "[103]\tdtrain-logloss:0.26165\tdtest-logloss:0.27117\tdtrain-CustomScore:-1508150.00000\tdtest-CustomScore:-398225.00000\n",
      "[104]\tdtrain-logloss:0.26046\tdtest-logloss:0.27011\tdtrain-CustomScore:-1497050.00000\tdtest-CustomScore:-396575.00000\n",
      "[105]\tdtrain-logloss:0.25984\tdtest-logloss:0.26954\tdtrain-CustomScore:-1494875.00000\tdtest-CustomScore:-395300.00000\n",
      "[106]\tdtrain-logloss:0.25884\tdtest-logloss:0.26859\tdtrain-CustomScore:-1482775.00000\tdtest-CustomScore:-393500.00000\n",
      "[107]\tdtrain-logloss:0.25779\tdtest-logloss:0.26760\tdtrain-CustomScore:-1474550.00000\tdtest-CustomScore:-391325.00000\n",
      "[108]\tdtrain-logloss:0.25693\tdtest-logloss:0.26684\tdtrain-CustomScore:-1470200.00000\tdtest-CustomScore:-388900.00000\n",
      "[109]\tdtrain-logloss:0.25578\tdtest-logloss:0.26577\tdtrain-CustomScore:-1462275.00000\tdtest-CustomScore:-387700.00000\n",
      "[110]\tdtrain-logloss:0.25469\tdtest-logloss:0.26470\tdtrain-CustomScore:-1452900.00000\tdtest-CustomScore:-385800.00000\n",
      "[111]\tdtrain-logloss:0.25374\tdtest-logloss:0.26379\tdtrain-CustomScore:-1446025.00000\tdtest-CustomScore:-383675.00000\n",
      "[112]\tdtrain-logloss:0.25310\tdtest-logloss:0.26326\tdtrain-CustomScore:-1439850.00000\tdtest-CustomScore:-382725.00000\n",
      "[113]\tdtrain-logloss:0.25251\tdtest-logloss:0.26270\tdtrain-CustomScore:-1434775.00000\tdtest-CustomScore:-381675.00000\n",
      "[114]\tdtrain-logloss:0.25191\tdtest-logloss:0.26213\tdtrain-CustomScore:-1430425.00000\tdtest-CustomScore:-380150.00000\n",
      "[115]\tdtrain-logloss:0.25088\tdtest-logloss:0.26119\tdtrain-CustomScore:-1422925.00000\tdtest-CustomScore:-378875.00000\n",
      "[116]\tdtrain-logloss:0.25044\tdtest-logloss:0.26080\tdtrain-CustomScore:-1421450.00000\tdtest-CustomScore:-378125.00000\n",
      "[117]\tdtrain-logloss:0.24939\tdtest-logloss:0.25991\tdtrain-CustomScore:-1411575.00000\tdtest-CustomScore:-375825.00000\n",
      "[118]\tdtrain-logloss:0.24847\tdtest-logloss:0.25909\tdtrain-CustomScore:-1401000.00000\tdtest-CustomScore:-373025.00000\n",
      "[119]\tdtrain-logloss:0.24808\tdtest-logloss:0.25873\tdtrain-CustomScore:-1398475.00000\tdtest-CustomScore:-372650.00000\n",
      "[120]\tdtrain-logloss:0.24776\tdtest-logloss:0.25842\tdtrain-CustomScore:-1396025.00000\tdtest-CustomScore:-371925.00000\n",
      "[121]\tdtrain-logloss:0.24655\tdtest-logloss:0.25735\tdtrain-CustomScore:-1386325.00000\tdtest-CustomScore:-370700.00000\n",
      "[122]\tdtrain-logloss:0.24578\tdtest-logloss:0.25670\tdtrain-CustomScore:-1383275.00000\tdtest-CustomScore:-369575.00000\n",
      "[123]\tdtrain-logloss:0.24544\tdtest-logloss:0.25638\tdtrain-CustomScore:-1381000.00000\tdtest-CustomScore:-368850.00000\n",
      "[124]\tdtrain-logloss:0.24470\tdtest-logloss:0.25567\tdtrain-CustomScore:-1376925.00000\tdtest-CustomScore:-368400.00000\n",
      "[125]\tdtrain-logloss:0.24402\tdtest-logloss:0.25506\tdtrain-CustomScore:-1368925.00000\tdtest-CustomScore:-367550.00000\n",
      "[126]\tdtrain-logloss:0.24309\tdtest-logloss:0.25421\tdtrain-CustomScore:-1360775.00000\tdtest-CustomScore:-364525.00000\n",
      "[127]\tdtrain-logloss:0.24168\tdtest-logloss:0.25294\tdtrain-CustomScore:-1351150.00000\tdtest-CustomScore:-362500.00000\n",
      "[128]\tdtrain-logloss:0.24106\tdtest-logloss:0.25239\tdtrain-CustomScore:-1346750.00000\tdtest-CustomScore:-362025.00000\n",
      "[129]\tdtrain-logloss:0.24070\tdtest-logloss:0.25209\tdtrain-CustomScore:-1344575.00000\tdtest-CustomScore:-361000.00000\n",
      "[130]\tdtrain-logloss:0.24009\tdtest-logloss:0.25151\tdtrain-CustomScore:-1340875.00000\tdtest-CustomScore:-360525.00000\n",
      "[131]\tdtrain-logloss:0.23921\tdtest-logloss:0.25071\tdtrain-CustomScore:-1336475.00000\tdtest-CustomScore:-360225.00000\n",
      "[132]\tdtrain-logloss:0.23887\tdtest-logloss:0.25039\tdtrain-CustomScore:-1334925.00000\tdtest-CustomScore:-359275.00000\n",
      "[133]\tdtrain-logloss:0.23806\tdtest-logloss:0.24967\tdtrain-CustomScore:-1329200.00000\tdtest-CustomScore:-358025.00000\n",
      "[134]\tdtrain-logloss:0.23702\tdtest-logloss:0.24879\tdtrain-CustomScore:-1322600.00000\tdtest-CustomScore:-356100.00000\n",
      "[135]\tdtrain-logloss:0.23641\tdtest-logloss:0.24825\tdtrain-CustomScore:-1317750.00000\tdtest-CustomScore:-355475.00000\n",
      "[136]\tdtrain-logloss:0.23607\tdtest-logloss:0.24796\tdtrain-CustomScore:-1314850.00000\tdtest-CustomScore:-353875.00000\n",
      "[137]\tdtrain-logloss:0.23569\tdtest-logloss:0.24766\tdtrain-CustomScore:-1313225.00000\tdtest-CustomScore:-353950.00000\n",
      "[138]\tdtrain-logloss:0.23468\tdtest-logloss:0.24668\tdtrain-CustomScore:-1304350.00000\tdtest-CustomScore:-351950.00000\n",
      "[139]\tdtrain-logloss:0.23403\tdtest-logloss:0.24610\tdtrain-CustomScore:-1300875.00000\tdtest-CustomScore:-350450.00000\n",
      "[140]\tdtrain-logloss:0.23366\tdtest-logloss:0.24574\tdtrain-CustomScore:-1298675.00000\tdtest-CustomScore:-350050.00000\n",
      "[141]\tdtrain-logloss:0.23336\tdtest-logloss:0.24548\tdtrain-CustomScore:-1296900.00000\tdtest-CustomScore:-349500.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142]\tdtrain-logloss:0.23254\tdtest-logloss:0.24474\tdtrain-CustomScore:-1288150.00000\tdtest-CustomScore:-347925.00000\n",
      "[143]\tdtrain-logloss:0.23204\tdtest-logloss:0.24425\tdtrain-CustomScore:-1285225.00000\tdtest-CustomScore:-346325.00000\n",
      "[144]\tdtrain-logloss:0.23183\tdtest-logloss:0.24407\tdtrain-CustomScore:-1284625.00000\tdtest-CustomScore:-346125.00000\n",
      "[145]\tdtrain-logloss:0.23096\tdtest-logloss:0.24326\tdtrain-CustomScore:-1275750.00000\tdtest-CustomScore:-344125.00000\n",
      "[146]\tdtrain-logloss:0.23050\tdtest-logloss:0.24282\tdtrain-CustomScore:-1272675.00000\tdtest-CustomScore:-343500.00000\n",
      "[147]\tdtrain-logloss:0.23020\tdtest-logloss:0.24255\tdtrain-CustomScore:-1271150.00000\tdtest-CustomScore:-343475.00000\n",
      "[148]\tdtrain-logloss:0.22963\tdtest-logloss:0.24207\tdtrain-CustomScore:-1268325.00000\tdtest-CustomScore:-343025.00000\n",
      "[149]\tdtrain-logloss:0.22886\tdtest-logloss:0.24136\tdtrain-CustomScore:-1258250.00000\tdtest-CustomScore:-341125.00000\n",
      "[150]\tdtrain-logloss:0.22820\tdtest-logloss:0.24080\tdtrain-CustomScore:-1253475.00000\tdtest-CustomScore:-339850.00000\n",
      "[151]\tdtrain-logloss:0.22772\tdtest-logloss:0.24037\tdtrain-CustomScore:-1250300.00000\tdtest-CustomScore:-338125.00000\n",
      "[152]\tdtrain-logloss:0.22700\tdtest-logloss:0.23971\tdtrain-CustomScore:-1244750.00000\tdtest-CustomScore:-337700.00000\n",
      "[153]\tdtrain-logloss:0.22617\tdtest-logloss:0.23897\tdtrain-CustomScore:-1239325.00000\tdtest-CustomScore:-335900.00000\n",
      "[154]\tdtrain-logloss:0.22576\tdtest-logloss:0.23863\tdtrain-CustomScore:-1237850.00000\tdtest-CustomScore:-334075.00000\n",
      "[155]\tdtrain-logloss:0.22496\tdtest-logloss:0.23795\tdtrain-CustomScore:-1233250.00000\tdtest-CustomScore:-332300.00000\n",
      "[156]\tdtrain-logloss:0.22419\tdtest-logloss:0.23723\tdtrain-CustomScore:-1228050.00000\tdtest-CustomScore:-331175.00000\n",
      "[157]\tdtrain-logloss:0.22351\tdtest-logloss:0.23664\tdtrain-CustomScore:-1222175.00000\tdtest-CustomScore:-329950.00000\n",
      "[158]\tdtrain-logloss:0.22322\tdtest-logloss:0.23643\tdtrain-CustomScore:-1218475.00000\tdtest-CustomScore:-329350.00000\n",
      "[159]\tdtrain-logloss:0.22273\tdtest-logloss:0.23600\tdtrain-CustomScore:-1216025.00000\tdtest-CustomScore:-328875.00000\n",
      "[160]\tdtrain-logloss:0.22209\tdtest-logloss:0.23539\tdtrain-CustomScore:-1211375.00000\tdtest-CustomScore:-328525.00000\n",
      "[161]\tdtrain-logloss:0.22164\tdtest-logloss:0.23497\tdtrain-CustomScore:-1207525.00000\tdtest-CustomScore:-326825.00000\n",
      "[162]\tdtrain-logloss:0.22077\tdtest-logloss:0.23423\tdtrain-CustomScore:-1201250.00000\tdtest-CustomScore:-326575.00000\n",
      "[163]\tdtrain-logloss:0.22022\tdtest-logloss:0.23375\tdtrain-CustomScore:-1200875.00000\tdtest-CustomScore:-326275.00000\n",
      "[164]\tdtrain-logloss:0.22002\tdtest-logloss:0.23358\tdtrain-CustomScore:-1201600.00000\tdtest-CustomScore:-326200.00000\n",
      "[165]\tdtrain-logloss:0.21894\tdtest-logloss:0.23267\tdtrain-CustomScore:-1192325.00000\tdtest-CustomScore:-323050.00000\n",
      "[166]\tdtrain-logloss:0.21874\tdtest-logloss:0.23252\tdtrain-CustomScore:-1190825.00000\tdtest-CustomScore:-322550.00000\n",
      "[167]\tdtrain-logloss:0.21854\tdtest-logloss:0.23234\tdtrain-CustomScore:-1190000.00000\tdtest-CustomScore:-322725.00000\n",
      "[168]\tdtrain-logloss:0.21819\tdtest-logloss:0.23205\tdtrain-CustomScore:-1187525.00000\tdtest-CustomScore:-322125.00000\n",
      "[169]\tdtrain-logloss:0.21764\tdtest-logloss:0.23157\tdtrain-CustomScore:-1181950.00000\tdtest-CustomScore:-320650.00000\n",
      "[170]\tdtrain-logloss:0.21731\tdtest-logloss:0.23127\tdtrain-CustomScore:-1179575.00000\tdtest-CustomScore:-319725.00000\n",
      "[171]\tdtrain-logloss:0.21642\tdtest-logloss:0.23047\tdtrain-CustomScore:-1172525.00000\tdtest-CustomScore:-319600.00000\n",
      "[172]\tdtrain-logloss:0.21594\tdtest-logloss:0.23004\tdtrain-CustomScore:-1170025.00000\tdtest-CustomScore:-319550.00000\n",
      "[173]\tdtrain-logloss:0.21521\tdtest-logloss:0.22935\tdtrain-CustomScore:-1163625.00000\tdtest-CustomScore:-317800.00000\n",
      "[174]\tdtrain-logloss:0.21449\tdtest-logloss:0.22872\tdtrain-CustomScore:-1158300.00000\tdtest-CustomScore:-316600.00000\n",
      "[175]\tdtrain-logloss:0.21370\tdtest-logloss:0.22798\tdtrain-CustomScore:-1150275.00000\tdtest-CustomScore:-314850.00000\n",
      "[176]\tdtrain-logloss:0.21348\tdtest-logloss:0.22782\tdtrain-CustomScore:-1148925.00000\tdtest-CustomScore:-314450.00000\n",
      "[177]\tdtrain-logloss:0.21325\tdtest-logloss:0.22763\tdtrain-CustomScore:-1148700.00000\tdtest-CustomScore:-314150.00000\n",
      "[178]\tdtrain-logloss:0.21247\tdtest-logloss:0.22695\tdtrain-CustomScore:-1145025.00000\tdtest-CustomScore:-313050.00000\n",
      "[179]\tdtrain-logloss:0.21209\tdtest-logloss:0.22663\tdtrain-CustomScore:-1142275.00000\tdtest-CustomScore:-311950.00000\n",
      "[180]\tdtrain-logloss:0.21191\tdtest-logloss:0.22647\tdtrain-CustomScore:-1141400.00000\tdtest-CustomScore:-311900.00000\n",
      "[181]\tdtrain-logloss:0.21148\tdtest-logloss:0.22615\tdtrain-CustomScore:-1138075.00000\tdtest-CustomScore:-311625.00000\n",
      "[182]\tdtrain-logloss:0.21138\tdtest-logloss:0.22609\tdtrain-CustomScore:-1136775.00000\tdtest-CustomScore:-311525.00000\n",
      "[183]\tdtrain-logloss:0.21108\tdtest-logloss:0.22580\tdtrain-CustomScore:-1135950.00000\tdtest-CustomScore:-310725.00000\n",
      "[184]\tdtrain-logloss:0.21083\tdtest-logloss:0.22564\tdtrain-CustomScore:-1133700.00000\tdtest-CustomScore:-310650.00000\n",
      "[185]\tdtrain-logloss:0.21047\tdtest-logloss:0.22537\tdtrain-CustomScore:-1132350.00000\tdtest-CustomScore:-310050.00000\n",
      "[186]\tdtrain-logloss:0.20996\tdtest-logloss:0.22495\tdtrain-CustomScore:-1126750.00000\tdtest-CustomScore:-309850.00000\n",
      "[187]\tdtrain-logloss:0.20982\tdtest-logloss:0.22485\tdtrain-CustomScore:-1125575.00000\tdtest-CustomScore:-309600.00000\n",
      "[188]\tdtrain-logloss:0.20910\tdtest-logloss:0.22423\tdtrain-CustomScore:-1119200.00000\tdtest-CustomScore:-309325.00000\n",
      "[189]\tdtrain-logloss:0.20877\tdtest-logloss:0.22401\tdtrain-CustomScore:-1118350.00000\tdtest-CustomScore:-309100.00000\n",
      "[190]\tdtrain-logloss:0.20848\tdtest-logloss:0.22382\tdtrain-CustomScore:-1116650.00000\tdtest-CustomScore:-308075.00000\n",
      "[191]\tdtrain-logloss:0.20813\tdtest-logloss:0.22348\tdtrain-CustomScore:-1112875.00000\tdtest-CustomScore:-306350.00000\n",
      "[192]\tdtrain-logloss:0.20769\tdtest-logloss:0.22307\tdtrain-CustomScore:-1109225.00000\tdtest-CustomScore:-305450.00000\n",
      "[193]\tdtrain-logloss:0.20747\tdtest-logloss:0.22287\tdtrain-CustomScore:-1108600.00000\tdtest-CustomScore:-304925.00000\n",
      "[194]\tdtrain-logloss:0.20685\tdtest-logloss:0.22232\tdtrain-CustomScore:-1105775.00000\tdtest-CustomScore:-303750.00000\n",
      "[195]\tdtrain-logloss:0.20650\tdtest-logloss:0.22202\tdtrain-CustomScore:-1102800.00000\tdtest-CustomScore:-304575.00000\n",
      "[196]\tdtrain-logloss:0.20628\tdtest-logloss:0.22185\tdtrain-CustomScore:-1100925.00000\tdtest-CustomScore:-304100.00000\n",
      "[197]\tdtrain-logloss:0.20614\tdtest-logloss:0.22175\tdtrain-CustomScore:-1101100.00000\tdtest-CustomScore:-304000.00000\n",
      "[198]\tdtrain-logloss:0.20580\tdtest-logloss:0.22142\tdtrain-CustomScore:-1098675.00000\tdtest-CustomScore:-302025.00000\n",
      "[199]\tdtrain-logloss:0.20540\tdtest-logloss:0.22116\tdtrain-CustomScore:-1097450.00000\tdtest-CustomScore:-302325.00000\n",
      "[200]\tdtrain-logloss:0.20527\tdtest-logloss:0.22106\tdtrain-CustomScore:-1095325.00000\tdtest-CustomScore:-301800.00000\n",
      "[201]\tdtrain-logloss:0.20514\tdtest-logloss:0.22099\tdtrain-CustomScore:-1093450.00000\tdtest-CustomScore:-301800.00000\n",
      "[202]\tdtrain-logloss:0.20501\tdtest-logloss:0.22097\tdtrain-CustomScore:-1093200.00000\tdtest-CustomScore:-302200.00000\n",
      "[203]\tdtrain-logloss:0.20453\tdtest-logloss:0.22058\tdtrain-CustomScore:-1091850.00000\tdtest-CustomScore:-301925.00000\n",
      "[204]\tdtrain-logloss:0.20422\tdtest-logloss:0.22031\tdtrain-CustomScore:-1088050.00000\tdtest-CustomScore:-301025.00000\n",
      "[205]\tdtrain-logloss:0.20394\tdtest-logloss:0.22007\tdtrain-CustomScore:-1085325.00000\tdtest-CustomScore:-299875.00000\n",
      "[206]\tdtrain-logloss:0.20376\tdtest-logloss:0.21999\tdtrain-CustomScore:-1084725.00000\tdtest-CustomScore:-299025.00000\n",
      "[207]\tdtrain-logloss:0.20359\tdtest-logloss:0.21999\tdtrain-CustomScore:-1082600.00000\tdtest-CustomScore:-298750.00000\n",
      "[208]\tdtrain-logloss:0.20347\tdtest-logloss:0.21996\tdtrain-CustomScore:-1082150.00000\tdtest-CustomScore:-299150.00000\n",
      "[209]\tdtrain-logloss:0.20327\tdtest-logloss:0.21983\tdtrain-CustomScore:-1082675.00000\tdtest-CustomScore:-298850.00000\n",
      "[210]\tdtrain-logloss:0.20254\tdtest-logloss:0.21920\tdtrain-CustomScore:-1076150.00000\tdtest-CustomScore:-298925.00000\n",
      "[211]\tdtrain-logloss:0.20194\tdtest-logloss:0.21867\tdtrain-CustomScore:-1071950.00000\tdtest-CustomScore:-297725.00000\n",
      "[212]\tdtrain-logloss:0.20155\tdtest-logloss:0.21837\tdtrain-CustomScore:-1067800.00000\tdtest-CustomScore:-297550.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[213]\tdtrain-logloss:0.20114\tdtest-logloss:0.21803\tdtrain-CustomScore:-1067425.00000\tdtest-CustomScore:-297625.00000\n",
      "[214]\tdtrain-logloss:0.20096\tdtest-logloss:0.21789\tdtrain-CustomScore:-1067000.00000\tdtest-CustomScore:-296800.00000\n",
      "[215]\tdtrain-logloss:0.20063\tdtest-logloss:0.21765\tdtrain-CustomScore:-1062625.00000\tdtest-CustomScore:-296225.00000\n",
      "[216]\tdtrain-logloss:0.20032\tdtest-logloss:0.21739\tdtrain-CustomScore:-1060575.00000\tdtest-CustomScore:-296475.00000\n",
      "[217]\tdtrain-logloss:0.20023\tdtest-logloss:0.21730\tdtrain-CustomScore:-1059525.00000\tdtest-CustomScore:-295950.00000\n",
      "[218]\tdtrain-logloss:0.20003\tdtest-logloss:0.21718\tdtrain-CustomScore:-1060025.00000\tdtest-CustomScore:-295850.00000\n",
      "[219]\tdtrain-logloss:0.19992\tdtest-logloss:0.21711\tdtrain-CustomScore:-1058825.00000\tdtest-CustomScore:-296600.00000\n",
      "[220]\tdtrain-logloss:0.19921\tdtest-logloss:0.21651\tdtrain-CustomScore:-1054475.00000\tdtest-CustomScore:-293825.00000\n",
      "[221]\tdtrain-logloss:0.19912\tdtest-logloss:0.21644\tdtrain-CustomScore:-1053675.00000\tdtest-CustomScore:-293450.00000\n",
      "[222]\tdtrain-logloss:0.19880\tdtest-logloss:0.21615\tdtrain-CustomScore:-1052725.00000\tdtest-CustomScore:-293600.00000\n",
      "[223]\tdtrain-logloss:0.19870\tdtest-logloss:0.21613\tdtrain-CustomScore:-1051950.00000\tdtest-CustomScore:-294000.00000\n",
      "[224]\tdtrain-logloss:0.19854\tdtest-logloss:0.21614\tdtrain-CustomScore:-1051375.00000\tdtest-CustomScore:-293725.00000\n",
      "[225]\tdtrain-logloss:0.19782\tdtest-logloss:0.21554\tdtrain-CustomScore:-1047600.00000\tdtest-CustomScore:-292925.00000\n",
      "[226]\tdtrain-logloss:0.19718\tdtest-logloss:0.21497\tdtrain-CustomScore:-1044800.00000\tdtest-CustomScore:-291750.00000\n",
      "[227]\tdtrain-logloss:0.19676\tdtest-logloss:0.21468\tdtrain-CustomScore:-1044250.00000\tdtest-CustomScore:-291350.00000\n",
      "[228]\tdtrain-logloss:0.19664\tdtest-logloss:0.21462\tdtrain-CustomScore:-1043525.00000\tdtest-CustomScore:-291475.00000\n",
      "[229]\tdtrain-logloss:0.19634\tdtest-logloss:0.21437\tdtrain-CustomScore:-1041350.00000\tdtest-CustomScore:-290975.00000\n",
      "[230]\tdtrain-logloss:0.19605\tdtest-logloss:0.21417\tdtrain-CustomScore:-1036100.00000\tdtest-CustomScore:-290250.00000\n",
      "[231]\tdtrain-logloss:0.19543\tdtest-logloss:0.21367\tdtrain-CustomScore:-1031525.00000\tdtest-CustomScore:-288750.00000\n",
      "[232]\tdtrain-logloss:0.19501\tdtest-logloss:0.21340\tdtrain-CustomScore:-1031075.00000\tdtest-CustomScore:-288100.00000\n",
      "[233]\tdtrain-logloss:0.19470\tdtest-logloss:0.21313\tdtrain-CustomScore:-1029750.00000\tdtest-CustomScore:-287675.00000\n",
      "[234]\tdtrain-logloss:0.19457\tdtest-logloss:0.21303\tdtrain-CustomScore:-1029775.00000\tdtest-CustomScore:-286775.00000\n",
      "[235]\tdtrain-logloss:0.19446\tdtest-logloss:0.21300\tdtrain-CustomScore:-1028775.00000\tdtest-CustomScore:-286900.00000\n",
      "[236]\tdtrain-logloss:0.19403\tdtest-logloss:0.21265\tdtrain-CustomScore:-1023325.00000\tdtest-CustomScore:-286325.00000\n",
      "[237]\tdtrain-logloss:0.19389\tdtest-logloss:0.21256\tdtrain-CustomScore:-1023125.00000\tdtest-CustomScore:-286075.00000\n",
      "[238]\tdtrain-logloss:0.19371\tdtest-logloss:0.21252\tdtrain-CustomScore:-1022875.00000\tdtest-CustomScore:-286225.00000\n",
      "[239]\tdtrain-logloss:0.19335\tdtest-logloss:0.21221\tdtrain-CustomScore:-1020450.00000\tdtest-CustomScore:-285525.00000\n",
      "[240]\tdtrain-logloss:0.19319\tdtest-logloss:0.21215\tdtrain-CustomScore:-1017900.00000\tdtest-CustomScore:-284975.00000\n",
      "[241]\tdtrain-logloss:0.19275\tdtest-logloss:0.21176\tdtrain-CustomScore:-1014950.00000\tdtest-CustomScore:-284025.00000\n",
      "[242]\tdtrain-logloss:0.19263\tdtest-logloss:0.21172\tdtrain-CustomScore:-1014350.00000\tdtest-CustomScore:-283900.00000\n",
      "[243]\tdtrain-logloss:0.19230\tdtest-logloss:0.21142\tdtrain-CustomScore:-1012975.00000\tdtest-CustomScore:-283375.00000\n",
      "[244]\tdtrain-logloss:0.19215\tdtest-logloss:0.21130\tdtrain-CustomScore:-1012075.00000\tdtest-CustomScore:-282425.00000\n",
      "[245]\tdtrain-logloss:0.19171\tdtest-logloss:0.21096\tdtrain-CustomScore:-1009050.00000\tdtest-CustomScore:-282075.00000\n",
      "[246]\tdtrain-logloss:0.19157\tdtest-logloss:0.21089\tdtrain-CustomScore:-1007800.00000\tdtest-CustomScore:-281550.00000\n",
      "[247]\tdtrain-logloss:0.19141\tdtest-logloss:0.21087\tdtrain-CustomScore:-1004550.00000\tdtest-CustomScore:-281950.00000\n",
      "[248]\tdtrain-logloss:0.19098\tdtest-logloss:0.21056\tdtrain-CustomScore:-1003275.00000\tdtest-CustomScore:-282125.00000\n",
      "[249]\tdtrain-logloss:0.19083\tdtest-logloss:0.21056\tdtrain-CustomScore:-1001775.00000\tdtest-CustomScore:-281600.00000\n",
      "[250]\tdtrain-logloss:0.19069\tdtest-logloss:0.21050\tdtrain-CustomScore:-1002100.00000\tdtest-CustomScore:-281975.00000\n",
      "[251]\tdtrain-logloss:0.19024\tdtest-logloss:0.21016\tdtrain-CustomScore:-998700.00000\tdtest-CustomScore:-282300.00000\n",
      "[252]\tdtrain-logloss:0.19008\tdtest-logloss:0.21011\tdtrain-CustomScore:-998300.00000\tdtest-CustomScore:-282375.00000\n",
      "[253]\tdtrain-logloss:0.18976\tdtest-logloss:0.20982\tdtrain-CustomScore:-993775.00000\tdtest-CustomScore:-281500.00000\n",
      "[254]\tdtrain-logloss:0.18957\tdtest-logloss:0.20971\tdtrain-CustomScore:-991825.00000\tdtest-CustomScore:-281125.00000\n",
      "[255]\tdtrain-logloss:0.18944\tdtest-logloss:0.20968\tdtrain-CustomScore:-991600.00000\tdtest-CustomScore:-281500.00000\n",
      "[256]\tdtrain-logloss:0.18938\tdtest-logloss:0.20963\tdtrain-CustomScore:-990775.00000\tdtest-CustomScore:-281875.00000\n",
      "[257]\tdtrain-logloss:0.18930\tdtest-logloss:0.20964\tdtrain-CustomScore:-989725.00000\tdtest-CustomScore:-281375.00000\n",
      "[258]\tdtrain-logloss:0.18905\tdtest-logloss:0.20943\tdtrain-CustomScore:-987625.00000\tdtest-CustomScore:-281450.00000\n",
      "[259]\tdtrain-logloss:0.18890\tdtest-logloss:0.20933\tdtrain-CustomScore:-986825.00000\tdtest-CustomScore:-280950.00000\n",
      "[260]\tdtrain-logloss:0.18846\tdtest-logloss:0.20896\tdtrain-CustomScore:-985375.00000\tdtest-CustomScore:-280675.00000\n",
      "[261]\tdtrain-logloss:0.18813\tdtest-logloss:0.20870\tdtrain-CustomScore:-981025.00000\tdtest-CustomScore:-279225.00000\n",
      "[262]\tdtrain-logloss:0.18804\tdtest-logloss:0.20870\tdtrain-CustomScore:-980600.00000\tdtest-CustomScore:-278325.00000\n",
      "[263]\tdtrain-logloss:0.18756\tdtest-logloss:0.20836\tdtrain-CustomScore:-976400.00000\tdtest-CustomScore:-278350.00000\n",
      "[264]\tdtrain-logloss:0.18737\tdtest-logloss:0.20821\tdtrain-CustomScore:-974475.00000\tdtest-CustomScore:-278475.00000\n",
      "[265]\tdtrain-logloss:0.18730\tdtest-logloss:0.20819\tdtrain-CustomScore:-975100.00000\tdtest-CustomScore:-278875.00000\n",
      "[266]\tdtrain-logloss:0.18702\tdtest-logloss:0.20800\tdtrain-CustomScore:-974225.00000\tdtest-CustomScore:-278250.00000\n",
      "[267]\tdtrain-logloss:0.18687\tdtest-logloss:0.20792\tdtrain-CustomScore:-973350.00000\tdtest-CustomScore:-278000.00000\n",
      "[268]\tdtrain-logloss:0.18675\tdtest-logloss:0.20787\tdtrain-CustomScore:-972300.00000\tdtest-CustomScore:-279100.00000\n",
      "[269]\tdtrain-logloss:0.18653\tdtest-logloss:0.20770\tdtrain-CustomScore:-971600.00000\tdtest-CustomScore:-278975.00000\n",
      "[270]\tdtrain-logloss:0.18628\tdtest-logloss:0.20749\tdtrain-CustomScore:-971750.00000\tdtest-CustomScore:-278650.00000\n",
      "[271]\tdtrain-logloss:0.18569\tdtest-logloss:0.20701\tdtrain-CustomScore:-967325.00000\tdtest-CustomScore:-278525.00000\n",
      "[272]\tdtrain-logloss:0.18561\tdtest-logloss:0.20697\tdtrain-CustomScore:-966700.00000\tdtest-CustomScore:-277975.00000\n",
      "[273]\tdtrain-logloss:0.18541\tdtest-logloss:0.20683\tdtrain-CustomScore:-964925.00000\tdtest-CustomScore:-278075.00000\n",
      "[274]\tdtrain-logloss:0.18532\tdtest-logloss:0.20679\tdtrain-CustomScore:-964050.00000\tdtest-CustomScore:-277450.00000\n",
      "[275]\tdtrain-logloss:0.18504\tdtest-logloss:0.20655\tdtrain-CustomScore:-961950.00000\tdtest-CustomScore:-277200.00000\n",
      "[276]\tdtrain-logloss:0.18448\tdtest-logloss:0.20611\tdtrain-CustomScore:-957900.00000\tdtest-CustomScore:-276150.00000\n",
      "[277]\tdtrain-logloss:0.18375\tdtest-logloss:0.20545\tdtrain-CustomScore:-952150.00000\tdtest-CustomScore:-275450.00000\n",
      "[278]\tdtrain-logloss:0.18312\tdtest-logloss:0.20495\tdtrain-CustomScore:-946550.00000\tdtest-CustomScore:-274175.00000\n",
      "[279]\tdtrain-logloss:0.18294\tdtest-logloss:0.20485\tdtrain-CustomScore:-945975.00000\tdtest-CustomScore:-273900.00000\n",
      "[280]\tdtrain-logloss:0.18288\tdtest-logloss:0.20486\tdtrain-CustomScore:-945550.00000\tdtest-CustomScore:-274300.00000\n",
      "[281]\tdtrain-logloss:0.18267\tdtest-logloss:0.20471\tdtrain-CustomScore:-945350.00000\tdtest-CustomScore:-274175.00000\n",
      "[282]\tdtrain-logloss:0.18236\tdtest-logloss:0.20447\tdtrain-CustomScore:-944850.00000\tdtest-CustomScore:-274600.00000\n",
      "[283]\tdtrain-logloss:0.18213\tdtest-logloss:0.20431\tdtrain-CustomScore:-945625.00000\tdtest-CustomScore:-274425.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284]\tdtrain-logloss:0.18205\tdtest-logloss:0.20428\tdtrain-CustomScore:-945025.00000\tdtest-CustomScore:-274525.00000\n",
      "[285]\tdtrain-logloss:0.18167\tdtest-logloss:0.20400\tdtrain-CustomScore:-941600.00000\tdtest-CustomScore:-272975.00000\n",
      "[286]\tdtrain-logloss:0.18162\tdtest-logloss:0.20396\tdtrain-CustomScore:-941750.00000\tdtest-CustomScore:-272750.00000\n",
      "[287]\tdtrain-logloss:0.18149\tdtest-logloss:0.20392\tdtrain-CustomScore:-939500.00000\tdtest-CustomScore:-272900.00000\n",
      "[288]\tdtrain-logloss:0.18136\tdtest-logloss:0.20386\tdtrain-CustomScore:-939100.00000\tdtest-CustomScore:-272525.00000\n",
      "[289]\tdtrain-logloss:0.18111\tdtest-logloss:0.20361\tdtrain-CustomScore:-938475.00000\tdtest-CustomScore:-272200.00000\n",
      "[290]\tdtrain-logloss:0.18089\tdtest-logloss:0.20351\tdtrain-CustomScore:-937575.00000\tdtest-CustomScore:-272225.00000\n",
      "[291]\tdtrain-logloss:0.18070\tdtest-logloss:0.20339\tdtrain-CustomScore:-936200.00000\tdtest-CustomScore:-271275.00000\n",
      "[292]\tdtrain-logloss:0.18056\tdtest-logloss:0.20327\tdtrain-CustomScore:-936100.00000\tdtest-CustomScore:-270950.00000\n",
      "[293]\tdtrain-logloss:0.18028\tdtest-logloss:0.20313\tdtrain-CustomScore:-935775.00000\tdtest-CustomScore:-270425.00000\n",
      "[294]\tdtrain-logloss:0.18020\tdtest-logloss:0.20312\tdtrain-CustomScore:-934800.00000\tdtest-CustomScore:-270000.00000\n",
      "[295]\tdtrain-logloss:0.18011\tdtest-logloss:0.20310\tdtrain-CustomScore:-933825.00000\tdtest-CustomScore:-270150.00000\n",
      "[296]\tdtrain-logloss:0.17995\tdtest-logloss:0.20303\tdtrain-CustomScore:-931000.00000\tdtest-CustomScore:-269825.00000\n",
      "[297]\tdtrain-logloss:0.17976\tdtest-logloss:0.20287\tdtrain-CustomScore:-930750.00000\tdtest-CustomScore:-268850.00000\n",
      "[298]\tdtrain-logloss:0.17967\tdtest-logloss:0.20280\tdtrain-CustomScore:-929800.00000\tdtest-CustomScore:-268825.00000\n",
      "[299]\tdtrain-logloss:0.17961\tdtest-logloss:0.20278\tdtrain-CustomScore:-930300.00000\tdtest-CustomScore:-269225.00000\n",
      "[300]\tdtrain-logloss:0.17945\tdtest-logloss:0.20270\tdtrain-CustomScore:-929575.00000\tdtest-CustomScore:-268975.00000\n",
      "[301]\tdtrain-logloss:0.17936\tdtest-logloss:0.20267\tdtrain-CustomScore:-929675.00000\tdtest-CustomScore:-268625.00000\n",
      "[302]\tdtrain-logloss:0.17916\tdtest-logloss:0.20255\tdtrain-CustomScore:-927425.00000\tdtest-CustomScore:-268575.00000\n",
      "[303]\tdtrain-logloss:0.17900\tdtest-logloss:0.20253\tdtrain-CustomScore:-925875.00000\tdtest-CustomScore:-268750.00000\n",
      "[304]\tdtrain-logloss:0.17882\tdtest-logloss:0.20253\tdtrain-CustomScore:-924275.00000\tdtest-CustomScore:-270000.00000\n",
      "[305]\tdtrain-logloss:0.17838\tdtest-logloss:0.20220\tdtrain-CustomScore:-921225.00000\tdtest-CustomScore:-268600.00000\n",
      "[306]\tdtrain-logloss:0.17829\tdtest-logloss:0.20215\tdtrain-CustomScore:-919675.00000\tdtest-CustomScore:-268500.00000\n",
      "[307]\tdtrain-logloss:0.17805\tdtest-logloss:0.20203\tdtrain-CustomScore:-917125.00000\tdtest-CustomScore:-268875.00000\n",
      "[308]\tdtrain-logloss:0.17796\tdtest-logloss:0.20201\tdtrain-CustomScore:-915825.00000\tdtest-CustomScore:-269600.00000\n",
      "[309]\tdtrain-logloss:0.17781\tdtest-logloss:0.20194\tdtrain-CustomScore:-915225.00000\tdtest-CustomScore:-268450.00000\n",
      "[310]\tdtrain-logloss:0.17777\tdtest-logloss:0.20196\tdtrain-CustomScore:-914775.00000\tdtest-CustomScore:-268575.00000\n",
      "[311]\tdtrain-logloss:0.17765\tdtest-logloss:0.20193\tdtrain-CustomScore:-914700.00000\tdtest-CustomScore:-268975.00000\n",
      "[312]\tdtrain-logloss:0.17735\tdtest-logloss:0.20169\tdtrain-CustomScore:-912925.00000\tdtest-CustomScore:-268900.00000\n",
      "[313]\tdtrain-logloss:0.17724\tdtest-logloss:0.20166\tdtrain-CustomScore:-912500.00000\tdtest-CustomScore:-268300.00000\n",
      "[314]\tdtrain-logloss:0.17702\tdtest-logloss:0.20147\tdtrain-CustomScore:-910700.00000\tdtest-CustomScore:-267725.00000\n",
      "[315]\tdtrain-logloss:0.17689\tdtest-logloss:0.20144\tdtrain-CustomScore:-909475.00000\tdtest-CustomScore:-267350.00000\n",
      "[316]\tdtrain-logloss:0.17661\tdtest-logloss:0.20122\tdtrain-CustomScore:-908275.00000\tdtest-CustomScore:-266925.00000\n",
      "[317]\tdtrain-logloss:0.17651\tdtest-logloss:0.20123\tdtrain-CustomScore:-908825.00000\tdtest-CustomScore:-267450.00000\n",
      "[318]\tdtrain-logloss:0.17639\tdtest-logloss:0.20118\tdtrain-CustomScore:-907225.00000\tdtest-CustomScore:-267600.00000\n",
      "[319]\tdtrain-logloss:0.17597\tdtest-logloss:0.20086\tdtrain-CustomScore:-904675.00000\tdtest-CustomScore:-266550.00000\n",
      "[320]\tdtrain-logloss:0.17548\tdtest-logloss:0.20050\tdtrain-CustomScore:-900050.00000\tdtest-CustomScore:-266000.00000\n",
      "[321]\tdtrain-logloss:0.17536\tdtest-logloss:0.20051\tdtrain-CustomScore:-900375.00000\tdtest-CustomScore:-266650.00000\n",
      "[322]\tdtrain-logloss:0.17525\tdtest-logloss:0.20050\tdtrain-CustomScore:-898325.00000\tdtest-CustomScore:-267025.00000\n",
      "[323]\tdtrain-logloss:0.17513\tdtest-logloss:0.20047\tdtrain-CustomScore:-897550.00000\tdtest-CustomScore:-266550.00000\n",
      "[324]\tdtrain-logloss:0.17500\tdtest-logloss:0.20045\tdtrain-CustomScore:-897125.00000\tdtest-CustomScore:-266925.00000\n",
      "[325]\tdtrain-logloss:0.17491\tdtest-logloss:0.20043\tdtrain-CustomScore:-895675.00000\tdtest-CustomScore:-266950.00000\n",
      "[326]\tdtrain-logloss:0.17475\tdtest-logloss:0.20043\tdtrain-CustomScore:-893450.00000\tdtest-CustomScore:-267200.00000\n",
      "[327]\tdtrain-logloss:0.17460\tdtest-logloss:0.20043\tdtrain-CustomScore:-893275.00000\tdtest-CustomScore:-267050.00000\n",
      "[328]\tdtrain-logloss:0.17449\tdtest-logloss:0.20043\tdtrain-CustomScore:-893025.00000\tdtest-CustomScore:-267375.00000\n",
      "[329]\tdtrain-logloss:0.17435\tdtest-logloss:0.20041\tdtrain-CustomScore:-891550.00000\tdtest-CustomScore:-267650.00000\n",
      "[330]\tdtrain-logloss:0.17430\tdtest-logloss:0.20040\tdtrain-CustomScore:-892025.00000\tdtest-CustomScore:-267500.00000\n",
      "[331]\tdtrain-logloss:0.17418\tdtest-logloss:0.20037\tdtrain-CustomScore:-892150.00000\tdtest-CustomScore:-266700.00000\n",
      "[332]\tdtrain-logloss:0.17401\tdtest-logloss:0.20038\tdtrain-CustomScore:-889750.00000\tdtest-CustomScore:-266625.00000\n",
      "[333]\tdtrain-logloss:0.17391\tdtest-logloss:0.20038\tdtrain-CustomScore:-889300.00000\tdtest-CustomScore:-266775.00000\n",
      "[334]\tdtrain-logloss:0.17375\tdtest-logloss:0.20028\tdtrain-CustomScore:-888800.00000\tdtest-CustomScore:-266775.00000\n",
      "[335]\tdtrain-logloss:0.17365\tdtest-logloss:0.20028\tdtrain-CustomScore:-887000.00000\tdtest-CustomScore:-266800.00000\n",
      "[336]\tdtrain-logloss:0.17356\tdtest-logloss:0.20024\tdtrain-CustomScore:-885775.00000\tdtest-CustomScore:-266175.00000\n",
      "[337]\tdtrain-logloss:0.17349\tdtest-logloss:0.20024\tdtrain-CustomScore:-885775.00000\tdtest-CustomScore:-266275.00000\n",
      "[338]\tdtrain-logloss:0.17321\tdtest-logloss:0.20003\tdtrain-CustomScore:-883725.00000\tdtest-CustomScore:-265575.00000\n",
      "[339]\tdtrain-logloss:0.17295\tdtest-logloss:0.19991\tdtrain-CustomScore:-884175.00000\tdtest-CustomScore:-264725.00000\n",
      "[340]\tdtrain-logloss:0.17279\tdtest-logloss:0.19986\tdtrain-CustomScore:-882400.00000\tdtest-CustomScore:-265200.00000\n",
      "[341]\tdtrain-logloss:0.17239\tdtest-logloss:0.19954\tdtrain-CustomScore:-880950.00000\tdtest-CustomScore:-264525.00000\n",
      "[342]\tdtrain-logloss:0.17208\tdtest-logloss:0.19926\tdtrain-CustomScore:-878400.00000\tdtest-CustomScore:-264050.00000\n",
      "[343]\tdtrain-logloss:0.17195\tdtest-logloss:0.19924\tdtrain-CustomScore:-877850.00000\tdtest-CustomScore:-264000.00000\n",
      "[344]\tdtrain-logloss:0.17187\tdtest-logloss:0.19926\tdtrain-CustomScore:-877050.00000\tdtest-CustomScore:-263975.00000\n",
      "[345]\tdtrain-logloss:0.17180\tdtest-logloss:0.19922\tdtrain-CustomScore:-876900.00000\tdtest-CustomScore:-263825.00000\n",
      "[346]\tdtrain-logloss:0.17155\tdtest-logloss:0.19898\tdtrain-CustomScore:-875950.00000\tdtest-CustomScore:-263300.00000\n",
      "[347]\tdtrain-logloss:0.17141\tdtest-logloss:0.19894\tdtrain-CustomScore:-875100.00000\tdtest-CustomScore:-263075.00000\n",
      "[348]\tdtrain-logloss:0.17104\tdtest-logloss:0.19865\tdtrain-CustomScore:-873075.00000\tdtest-CustomScore:-262650.00000\n",
      "[349]\tdtrain-logloss:0.17102\tdtest-logloss:0.19865\tdtrain-CustomScore:-872525.00000\tdtest-CustomScore:-262925.00000\n",
      "[350]\tdtrain-logloss:0.17098\tdtest-logloss:0.19864\tdtrain-CustomScore:-872550.00000\tdtest-CustomScore:-262925.00000\n",
      "[351]\tdtrain-logloss:0.17066\tdtest-logloss:0.19837\tdtrain-CustomScore:-869650.00000\tdtest-CustomScore:-261850.00000\n",
      "[352]\tdtrain-logloss:0.17044\tdtest-logloss:0.19818\tdtrain-CustomScore:-867425.00000\tdtest-CustomScore:-260950.00000\n",
      "[353]\tdtrain-logloss:0.17033\tdtest-logloss:0.19812\tdtrain-CustomScore:-866775.00000\tdtest-CustomScore:-260950.00000\n",
      "[354]\tdtrain-logloss:0.17020\tdtest-logloss:0.19812\tdtrain-CustomScore:-866300.00000\tdtest-CustomScore:-261375.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[355]\tdtrain-logloss:0.17005\tdtest-logloss:0.19808\tdtrain-CustomScore:-863875.00000\tdtest-CustomScore:-260575.00000\n",
      "[356]\tdtrain-logloss:0.16996\tdtest-logloss:0.19805\tdtrain-CustomScore:-863825.00000\tdtest-CustomScore:-260225.00000\n",
      "[357]\tdtrain-logloss:0.16984\tdtest-logloss:0.19806\tdtrain-CustomScore:-863400.00000\tdtest-CustomScore:-260775.00000\n",
      "[358]\tdtrain-logloss:0.16975\tdtest-logloss:0.19806\tdtrain-CustomScore:-862825.00000\tdtest-CustomScore:-260750.00000\n",
      "[359]\tdtrain-logloss:0.16959\tdtest-logloss:0.19806\tdtrain-CustomScore:-862200.00000\tdtest-CustomScore:-260675.00000\n",
      "[360]\tdtrain-logloss:0.16939\tdtest-logloss:0.19795\tdtrain-CustomScore:-859600.00000\tdtest-CustomScore:-260550.00000\n",
      "[361]\tdtrain-logloss:0.16898\tdtest-logloss:0.19768\tdtrain-CustomScore:-856175.00000\tdtest-CustomScore:-260225.00000\n",
      "[362]\tdtrain-logloss:0.16845\tdtest-logloss:0.19723\tdtrain-CustomScore:-851725.00000\tdtest-CustomScore:-258925.00000\n",
      "[363]\tdtrain-logloss:0.16831\tdtest-logloss:0.19712\tdtrain-CustomScore:-850800.00000\tdtest-CustomScore:-258775.00000\n",
      "[364]\tdtrain-logloss:0.16825\tdtest-logloss:0.19709\tdtrain-CustomScore:-850875.00000\tdtest-CustomScore:-258900.00000\n",
      "[365]\tdtrain-logloss:0.16816\tdtest-logloss:0.19708\tdtrain-CustomScore:-851175.00000\tdtest-CustomScore:-259025.00000\n",
      "[366]\tdtrain-logloss:0.16781\tdtest-logloss:0.19676\tdtrain-CustomScore:-849350.00000\tdtest-CustomScore:-259100.00000\n",
      "[367]\tdtrain-logloss:0.16742\tdtest-logloss:0.19645\tdtrain-CustomScore:-844925.00000\tdtest-CustomScore:-258175.00000\n",
      "[368]\tdtrain-logloss:0.16730\tdtest-logloss:0.19645\tdtrain-CustomScore:-846350.00000\tdtest-CustomScore:-257700.00000\n",
      "[369]\tdtrain-logloss:0.16697\tdtest-logloss:0.19617\tdtrain-CustomScore:-842725.00000\tdtest-CustomScore:-256875.00000\n",
      "[370]\tdtrain-logloss:0.16645\tdtest-logloss:0.19577\tdtrain-CustomScore:-839975.00000\tdtest-CustomScore:-257575.00000\n",
      "[371]\tdtrain-logloss:0.16596\tdtest-logloss:0.19534\tdtrain-CustomScore:-835925.00000\tdtest-CustomScore:-256625.00000\n",
      "[372]\tdtrain-logloss:0.16556\tdtest-logloss:0.19502\tdtrain-CustomScore:-831825.00000\tdtest-CustomScore:-255900.00000\n",
      "[373]\tdtrain-logloss:0.16551\tdtest-logloss:0.19503\tdtrain-CustomScore:-832150.00000\tdtest-CustomScore:-255400.00000\n",
      "[374]\tdtrain-logloss:0.16538\tdtest-logloss:0.19494\tdtrain-CustomScore:-831200.00000\tdtest-CustomScore:-255475.00000\n",
      "[375]\tdtrain-logloss:0.16491\tdtest-logloss:0.19452\tdtrain-CustomScore:-829425.00000\tdtest-CustomScore:-255575.00000\n",
      "[376]\tdtrain-logloss:0.16479\tdtest-logloss:0.19443\tdtrain-CustomScore:-828850.00000\tdtest-CustomScore:-254475.00000\n",
      "[377]\tdtrain-logloss:0.16466\tdtest-logloss:0.19444\tdtrain-CustomScore:-827600.00000\tdtest-CustomScore:-255200.00000\n",
      "[378]\tdtrain-logloss:0.16452\tdtest-logloss:0.19435\tdtrain-CustomScore:-828150.00000\tdtest-CustomScore:-255075.00000\n",
      "[379]\tdtrain-logloss:0.16446\tdtest-logloss:0.19434\tdtrain-CustomScore:-827250.00000\tdtest-CustomScore:-255150.00000\n",
      "[380]\tdtrain-logloss:0.16431\tdtest-logloss:0.19427\tdtrain-CustomScore:-826050.00000\tdtest-CustomScore:-254950.00000\n",
      "[381]\tdtrain-logloss:0.16412\tdtest-logloss:0.19419\tdtrain-CustomScore:-825025.00000\tdtest-CustomScore:-254950.00000\n",
      "[382]\tdtrain-logloss:0.16368\tdtest-logloss:0.19386\tdtrain-CustomScore:-820925.00000\tdtest-CustomScore:-254175.00000\n",
      "[383]\tdtrain-logloss:0.16356\tdtest-logloss:0.19377\tdtrain-CustomScore:-820250.00000\tdtest-CustomScore:-254600.00000\n",
      "[384]\tdtrain-logloss:0.16342\tdtest-logloss:0.19378\tdtrain-CustomScore:-819100.00000\tdtest-CustomScore:-254600.00000\n",
      "[385]\tdtrain-logloss:0.16336\tdtest-logloss:0.19378\tdtrain-CustomScore:-819300.00000\tdtest-CustomScore:-253825.00000\n",
      "[386]\tdtrain-logloss:0.16319\tdtest-logloss:0.19368\tdtrain-CustomScore:-818775.00000\tdtest-CustomScore:-253500.00000\n",
      "[387]\tdtrain-logloss:0.16311\tdtest-logloss:0.19366\tdtrain-CustomScore:-817950.00000\tdtest-CustomScore:-253375.00000\n",
      "[388]\tdtrain-logloss:0.16296\tdtest-logloss:0.19364\tdtrain-CustomScore:-816675.00000\tdtest-CustomScore:-253000.00000\n",
      "[389]\tdtrain-logloss:0.16293\tdtest-logloss:0.19364\tdtrain-CustomScore:-816400.00000\tdtest-CustomScore:-253000.00000\n",
      "[390]\tdtrain-logloss:0.16286\tdtest-logloss:0.19363\tdtrain-CustomScore:-816400.00000\tdtest-CustomScore:-253550.00000\n",
      "[391]\tdtrain-logloss:0.16275\tdtest-logloss:0.19363\tdtrain-CustomScore:-817125.00000\tdtest-CustomScore:-253475.00000\n",
      "[392]\tdtrain-logloss:0.16248\tdtest-logloss:0.19347\tdtrain-CustomScore:-814675.00000\tdtest-CustomScore:-252800.00000\n",
      "[393]\tdtrain-logloss:0.16234\tdtest-logloss:0.19345\tdtrain-CustomScore:-814150.00000\tdtest-CustomScore:-252700.00000\n",
      "[394]\tdtrain-logloss:0.16181\tdtest-logloss:0.19307\tdtrain-CustomScore:-809325.00000\tdtest-CustomScore:-252850.00000\n",
      "[395]\tdtrain-logloss:0.16146\tdtest-logloss:0.19278\tdtrain-CustomScore:-807200.00000\tdtest-CustomScore:-252300.00000\n",
      "[396]\tdtrain-logloss:0.16125\tdtest-logloss:0.19262\tdtrain-CustomScore:-806650.00000\tdtest-CustomScore:-252175.00000\n",
      "[397]\tdtrain-logloss:0.16085\tdtest-logloss:0.19231\tdtrain-CustomScore:-804800.00000\tdtest-CustomScore:-250950.00000\n",
      "[398]\tdtrain-logloss:0.16070\tdtest-logloss:0.19220\tdtrain-CustomScore:-803475.00000\tdtest-CustomScore:-250950.00000\n",
      "[399]\tdtrain-logloss:0.16056\tdtest-logloss:0.19215\tdtrain-CustomScore:-803350.00000\tdtest-CustomScore:-250850.00000\n",
      "[400]\tdtrain-logloss:0.16043\tdtest-logloss:0.19213\tdtrain-CustomScore:-803675.00000\tdtest-CustomScore:-250750.00000\n",
      "[401]\tdtrain-logloss:0.16034\tdtest-logloss:0.19207\tdtrain-CustomScore:-802750.00000\tdtest-CustomScore:-250600.00000\n",
      "[402]\tdtrain-logloss:0.16016\tdtest-logloss:0.19195\tdtrain-CustomScore:-801550.00000\tdtest-CustomScore:-251225.00000\n",
      "[403]\tdtrain-logloss:0.15994\tdtest-logloss:0.19180\tdtrain-CustomScore:-798550.00000\tdtest-CustomScore:-251350.00000\n",
      "[404]\tdtrain-logloss:0.15981\tdtest-logloss:0.19178\tdtrain-CustomScore:-796925.00000\tdtest-CustomScore:-251800.00000\n",
      "[405]\tdtrain-logloss:0.15971\tdtest-logloss:0.19178\tdtrain-CustomScore:-797000.00000\tdtest-CustomScore:-251550.00000\n",
      "[406]\tdtrain-logloss:0.15964\tdtest-logloss:0.19178\tdtrain-CustomScore:-796150.00000\tdtest-CustomScore:-251725.00000\n",
      "[407]\tdtrain-logloss:0.15956\tdtest-logloss:0.19178\tdtrain-CustomScore:-794475.00000\tdtest-CustomScore:-251625.00000\n",
      "[408]\tdtrain-logloss:0.15944\tdtest-logloss:0.19176\tdtrain-CustomScore:-794475.00000\tdtest-CustomScore:-251775.00000\n",
      "[409]\tdtrain-logloss:0.15934\tdtest-logloss:0.19175\tdtrain-CustomScore:-794025.00000\tdtest-CustomScore:-252125.00000\n",
      "[410]\tdtrain-logloss:0.15925\tdtest-logloss:0.19172\tdtrain-CustomScore:-793000.00000\tdtest-CustomScore:-252150.00000\n",
      "[411]\tdtrain-logloss:0.15905\tdtest-logloss:0.19154\tdtrain-CustomScore:-792725.00000\tdtest-CustomScore:-252225.00000\n",
      "[412]\tdtrain-logloss:0.15899\tdtest-logloss:0.19154\tdtrain-CustomScore:-791975.00000\tdtest-CustomScore:-251875.00000\n",
      "[413]\tdtrain-logloss:0.15880\tdtest-logloss:0.19148\tdtrain-CustomScore:-790150.00000\tdtest-CustomScore:-251950.00000\n",
      "[414]\tdtrain-logloss:0.15871\tdtest-logloss:0.19149\tdtrain-CustomScore:-789400.00000\tdtest-CustomScore:-251200.00000\n",
      "[415]\tdtrain-logloss:0.15860\tdtest-logloss:0.19150\tdtrain-CustomScore:-787875.00000\tdtest-CustomScore:-250875.00000\n",
      "[416]\tdtrain-logloss:0.15850\tdtest-logloss:0.19150\tdtrain-CustomScore:-787000.00000\tdtest-CustomScore:-251225.00000\n",
      "[417]\tdtrain-logloss:0.15823\tdtest-logloss:0.19131\tdtrain-CustomScore:-786675.00000\tdtest-CustomScore:-250550.00000\n",
      "[418]\tdtrain-logloss:0.15776\tdtest-logloss:0.19091\tdtrain-CustomScore:-785700.00000\tdtest-CustomScore:-249725.00000\n",
      "[419]\tdtrain-logloss:0.15774\tdtest-logloss:0.19091\tdtrain-CustomScore:-785150.00000\tdtest-CustomScore:-249475.00000\n",
      "[420]\tdtrain-logloss:0.15749\tdtest-logloss:0.19076\tdtrain-CustomScore:-783650.00000\tdtest-CustomScore:-249625.00000\n",
      "[421]\tdtrain-logloss:0.15737\tdtest-logloss:0.19072\tdtrain-CustomScore:-781450.00000\tdtest-CustomScore:-249050.00000\n",
      "[422]\tdtrain-logloss:0.15724\tdtest-logloss:0.19072\tdtrain-CustomScore:-780675.00000\tdtest-CustomScore:-249625.00000\n",
      "[423]\tdtrain-logloss:0.15699\tdtest-logloss:0.19049\tdtrain-CustomScore:-778675.00000\tdtest-CustomScore:-249025.00000\n",
      "[424]\tdtrain-logloss:0.15684\tdtest-logloss:0.19044\tdtrain-CustomScore:-777575.00000\tdtest-CustomScore:-248975.00000\n",
      "[425]\tdtrain-logloss:0.15682\tdtest-logloss:0.19045\tdtrain-CustomScore:-777675.00000\tdtest-CustomScore:-249200.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426]\tdtrain-logloss:0.15667\tdtest-logloss:0.19043\tdtrain-CustomScore:-777475.00000\tdtest-CustomScore:-249250.00000\n",
      "[427]\tdtrain-logloss:0.15658\tdtest-logloss:0.19043\tdtrain-CustomScore:-777150.00000\tdtest-CustomScore:-248900.00000\n",
      "[428]\tdtrain-logloss:0.15635\tdtest-logloss:0.19029\tdtrain-CustomScore:-773925.00000\tdtest-CustomScore:-249000.00000\n",
      "[429]\tdtrain-logloss:0.15607\tdtest-logloss:0.19013\tdtrain-CustomScore:-771250.00000\tdtest-CustomScore:-249225.00000\n",
      "[430]\tdtrain-logloss:0.15599\tdtest-logloss:0.19013\tdtrain-CustomScore:-770250.00000\tdtest-CustomScore:-248850.00000\n",
      "[431]\tdtrain-logloss:0.15580\tdtest-logloss:0.19003\tdtrain-CustomScore:-769450.00000\tdtest-CustomScore:-248475.00000\n",
      "[432]\tdtrain-logloss:0.15564\tdtest-logloss:0.18995\tdtrain-CustomScore:-768025.00000\tdtest-CustomScore:-248000.00000\n",
      "[433]\tdtrain-logloss:0.15552\tdtest-logloss:0.18987\tdtrain-CustomScore:-768750.00000\tdtest-CustomScore:-247950.00000\n",
      "[434]\tdtrain-logloss:0.15543\tdtest-logloss:0.18985\tdtrain-CustomScore:-768075.00000\tdtest-CustomScore:-248000.00000\n",
      "[435]\tdtrain-logloss:0.15536\tdtest-logloss:0.18983\tdtrain-CustomScore:-767825.00000\tdtest-CustomScore:-248350.00000\n",
      "[436]\tdtrain-logloss:0.15523\tdtest-logloss:0.18985\tdtrain-CustomScore:-767900.00000\tdtest-CustomScore:-248225.00000\n",
      "[437]\tdtrain-logloss:0.15487\tdtest-logloss:0.18957\tdtrain-CustomScore:-763725.00000\tdtest-CustomScore:-247800.00000\n",
      "[438]\tdtrain-logloss:0.15474\tdtest-logloss:0.18960\tdtrain-CustomScore:-762775.00000\tdtest-CustomScore:-247525.00000\n",
      "[439]\tdtrain-logloss:0.15467\tdtest-logloss:0.18961\tdtrain-CustomScore:-763675.00000\tdtest-CustomScore:-247200.00000\n",
      "[440]\tdtrain-logloss:0.15455\tdtest-logloss:0.18960\tdtrain-CustomScore:-762675.00000\tdtest-CustomScore:-247250.00000\n",
      "[441]\tdtrain-logloss:0.15413\tdtest-logloss:0.18925\tdtrain-CustomScore:-759700.00000\tdtest-CustomScore:-246450.00000\n",
      "[442]\tdtrain-logloss:0.15402\tdtest-logloss:0.18926\tdtrain-CustomScore:-758625.00000\tdtest-CustomScore:-246600.00000\n",
      "[443]\tdtrain-logloss:0.15374\tdtest-logloss:0.18906\tdtrain-CustomScore:-756550.00000\tdtest-CustomScore:-246125.00000\n",
      "[444]\tdtrain-logloss:0.15361\tdtest-logloss:0.18902\tdtrain-CustomScore:-756325.00000\tdtest-CustomScore:-246075.00000\n",
      "[445]\tdtrain-logloss:0.15342\tdtest-logloss:0.18892\tdtrain-CustomScore:-756525.00000\tdtest-CustomScore:-245725.00000\n",
      "[446]\tdtrain-logloss:0.15331\tdtest-logloss:0.18893\tdtrain-CustomScore:-755575.00000\tdtest-CustomScore:-245475.00000\n",
      "[447]\tdtrain-logloss:0.15326\tdtest-logloss:0.18890\tdtrain-CustomScore:-755850.00000\tdtest-CustomScore:-245600.00000\n",
      "[448]\tdtrain-logloss:0.15320\tdtest-logloss:0.18889\tdtrain-CustomScore:-754725.00000\tdtest-CustomScore:-245650.00000\n",
      "[449]\tdtrain-logloss:0.15277\tdtest-logloss:0.18857\tdtrain-CustomScore:-754100.00000\tdtest-CustomScore:-244550.00000\n",
      "[450]\tdtrain-logloss:0.15255\tdtest-logloss:0.18841\tdtrain-CustomScore:-754150.00000\tdtest-CustomScore:-243600.00000\n",
      "[451]\tdtrain-logloss:0.15238\tdtest-logloss:0.18832\tdtrain-CustomScore:-750950.00000\tdtest-CustomScore:-244125.00000\n",
      "[452]\tdtrain-logloss:0.15224\tdtest-logloss:0.18832\tdtrain-CustomScore:-750925.00000\tdtest-CustomScore:-243900.00000\n",
      "[453]\tdtrain-logloss:0.15218\tdtest-logloss:0.18832\tdtrain-CustomScore:-750200.00000\tdtest-CustomScore:-244225.00000\n",
      "[454]\tdtrain-logloss:0.15209\tdtest-logloss:0.18829\tdtrain-CustomScore:-749250.00000\tdtest-CustomScore:-243800.00000\n",
      "[455]\tdtrain-logloss:0.15199\tdtest-logloss:0.18826\tdtrain-CustomScore:-748250.00000\tdtest-CustomScore:-243525.00000\n",
      "[456]\tdtrain-logloss:0.15188\tdtest-logloss:0.18824\tdtrain-CustomScore:-748350.00000\tdtest-CustomScore:-243675.00000\n",
      "[457]\tdtrain-logloss:0.15174\tdtest-logloss:0.18814\tdtrain-CustomScore:-746550.00000\tdtest-CustomScore:-243800.00000\n",
      "[458]\tdtrain-logloss:0.15164\tdtest-logloss:0.18814\tdtrain-CustomScore:-745700.00000\tdtest-CustomScore:-243925.00000\n",
      "[459]\tdtrain-logloss:0.15153\tdtest-logloss:0.18814\tdtrain-CustomScore:-745000.00000\tdtest-CustomScore:-244650.00000\n",
      "[460]\tdtrain-logloss:0.15139\tdtest-logloss:0.18811\tdtrain-CustomScore:-744275.00000\tdtest-CustomScore:-244400.00000\n",
      "[461]\tdtrain-logloss:0.15129\tdtest-logloss:0.18806\tdtrain-CustomScore:-743775.00000\tdtest-CustomScore:-243850.00000\n",
      "[462]\tdtrain-logloss:0.15123\tdtest-logloss:0.18806\tdtrain-CustomScore:-743650.00000\tdtest-CustomScore:-243825.00000\n",
      "[463]\tdtrain-logloss:0.15109\tdtest-logloss:0.18800\tdtrain-CustomScore:-743100.00000\tdtest-CustomScore:-244100.00000\n",
      "[464]\tdtrain-logloss:0.15093\tdtest-logloss:0.18795\tdtrain-CustomScore:-740950.00000\tdtest-CustomScore:-243750.00000\n",
      "[465]\tdtrain-logloss:0.15063\tdtest-logloss:0.18771\tdtrain-CustomScore:-739350.00000\tdtest-CustomScore:-242825.00000\n",
      "[466]\tdtrain-logloss:0.15056\tdtest-logloss:0.18773\tdtrain-CustomScore:-739950.00000\tdtest-CustomScore:-242750.00000\n",
      "[467]\tdtrain-logloss:0.15048\tdtest-logloss:0.18771\tdtrain-CustomScore:-739000.00000\tdtest-CustomScore:-242975.00000\n",
      "[468]\tdtrain-logloss:0.15037\tdtest-logloss:0.18770\tdtrain-CustomScore:-738350.00000\tdtest-CustomScore:-243125.00000\n",
      "[469]\tdtrain-logloss:0.15019\tdtest-logloss:0.18757\tdtrain-CustomScore:-736450.00000\tdtest-CustomScore:-242225.00000\n",
      "[470]\tdtrain-logloss:0.15009\tdtest-logloss:0.18758\tdtrain-CustomScore:-734775.00000\tdtest-CustomScore:-242125.00000\n",
      "[471]\tdtrain-logloss:0.14994\tdtest-logloss:0.18749\tdtrain-CustomScore:-734150.00000\tdtest-CustomScore:-241650.00000\n",
      "[472]\tdtrain-logloss:0.14987\tdtest-logloss:0.18747\tdtrain-CustomScore:-733625.00000\tdtest-CustomScore:-241525.00000\n",
      "[473]\tdtrain-logloss:0.14958\tdtest-logloss:0.18721\tdtrain-CustomScore:-730650.00000\tdtest-CustomScore:-241925.00000\n",
      "[474]\tdtrain-logloss:0.14946\tdtest-logloss:0.18711\tdtrain-CustomScore:-730350.00000\tdtest-CustomScore:-242075.00000\n",
      "[475]\tdtrain-logloss:0.14936\tdtest-logloss:0.18710\tdtrain-CustomScore:-729175.00000\tdtest-CustomScore:-241400.00000\n",
      "[476]\tdtrain-logloss:0.14927\tdtest-logloss:0.18708\tdtrain-CustomScore:-729350.00000\tdtest-CustomScore:-241100.00000\n",
      "[477]\tdtrain-logloss:0.14890\tdtest-logloss:0.18680\tdtrain-CustomScore:-727850.00000\tdtest-CustomScore:-240825.00000\n",
      "[478]\tdtrain-logloss:0.14863\tdtest-logloss:0.18659\tdtrain-CustomScore:-726200.00000\tdtest-CustomScore:-239900.00000\n",
      "[479]\tdtrain-logloss:0.14848\tdtest-logloss:0.18648\tdtrain-CustomScore:-725425.00000\tdtest-CustomScore:-239800.00000\n",
      "[480]\tdtrain-logloss:0.14839\tdtest-logloss:0.18646\tdtrain-CustomScore:-724425.00000\tdtest-CustomScore:-240100.00000\n",
      "[481]\tdtrain-logloss:0.14829\tdtest-logloss:0.18638\tdtrain-CustomScore:-723225.00000\tdtest-CustomScore:-239800.00000\n",
      "[482]\tdtrain-logloss:0.14820\tdtest-logloss:0.18639\tdtrain-CustomScore:-723075.00000\tdtest-CustomScore:-239600.00000\n",
      "[483]\tdtrain-logloss:0.14803\tdtest-logloss:0.18630\tdtrain-CustomScore:-721950.00000\tdtest-CustomScore:-238900.00000\n",
      "[484]\tdtrain-logloss:0.14795\tdtest-logloss:0.18627\tdtrain-CustomScore:-721150.00000\tdtest-CustomScore:-239250.00000\n",
      "[485]\tdtrain-logloss:0.14789\tdtest-logloss:0.18626\tdtrain-CustomScore:-720350.00000\tdtest-CustomScore:-239325.00000\n",
      "[486]\tdtrain-logloss:0.14780\tdtest-logloss:0.18624\tdtrain-CustomScore:-719700.00000\tdtest-CustomScore:-239200.00000\n",
      "[487]\tdtrain-logloss:0.14769\tdtest-logloss:0.18622\tdtrain-CustomScore:-720125.00000\tdtest-CustomScore:-239050.00000\n",
      "[488]\tdtrain-logloss:0.14758\tdtest-logloss:0.18620\tdtrain-CustomScore:-718500.00000\tdtest-CustomScore:-239100.00000\n",
      "[489]\tdtrain-logloss:0.14747\tdtest-logloss:0.18620\tdtrain-CustomScore:-717700.00000\tdtest-CustomScore:-238250.00000\n",
      "[490]\tdtrain-logloss:0.14739\tdtest-logloss:0.18621\tdtrain-CustomScore:-717500.00000\tdtest-CustomScore:-238825.00000\n",
      "[491]\tdtrain-logloss:0.14731\tdtest-logloss:0.18623\tdtrain-CustomScore:-716975.00000\tdtest-CustomScore:-238975.00000\n",
      "[492]\tdtrain-logloss:0.14726\tdtest-logloss:0.18622\tdtrain-CustomScore:-716475.00000\tdtest-CustomScore:-238850.00000\n",
      "[493]\tdtrain-logloss:0.14706\tdtest-logloss:0.18615\tdtrain-CustomScore:-715325.00000\tdtest-CustomScore:-239200.00000\n",
      "[494]\tdtrain-logloss:0.14694\tdtest-logloss:0.18608\tdtrain-CustomScore:-714550.00000\tdtest-CustomScore:-238675.00000\n",
      "[495]\tdtrain-logloss:0.14687\tdtest-logloss:0.18607\tdtrain-CustomScore:-713650.00000\tdtest-CustomScore:-238575.00000\n",
      "[496]\tdtrain-logloss:0.14682\tdtest-logloss:0.18608\tdtrain-CustomScore:-713475.00000\tdtest-CustomScore:-238800.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[497]\tdtrain-logloss:0.14673\tdtest-logloss:0.18609\tdtrain-CustomScore:-713625.00000\tdtest-CustomScore:-238325.00000\n",
      "[498]\tdtrain-logloss:0.14643\tdtest-logloss:0.18587\tdtrain-CustomScore:-710875.00000\tdtest-CustomScore:-238225.00000\n",
      "Wall time: 5min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x1e2a2e2af88>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "new_params = {'objective': 'binary:logistic',\n",
    " 'booster': 'gbtree',\n",
    " 'eval_metric': 'logloss',\n",
    " 'tree_method': 'hist',\n",
    " 'max_depth': 7,\n",
    " 'min_child_weight': 5,\n",
    " 'subsample': 0.6,\n",
    " 'colsample_bytree': 0.6,\n",
    " 'eta': 0.05,\n",
    " 'gamma': 0,\n",
    " 'alpha': 0,\n",
    " 'lambda': 1}\n",
    "\n",
    "xgb.train(new_params,\n",
    "          dtrain=dtrain,\n",
    "          num_boost_round=num_boost_round,\n",
    "#           obj=squared_log,\n",
    "          feval=myscore,\n",
    "          evals=[(dtrain, 'dtrain'), (dtest, 'dtest')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 68)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# input\n",
    "model.add(tf.keras.Input(shape=(68,)))\n",
    "# hidden\n",
    "model.add(layers.Dense(100, activation='relu') ) \n",
    "model.add(layers.Dense(100, activation='relu')  )\n",
    "model.add(layers.Dense(1,\n",
    "                       activation='sigmoid')  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myscore(predt: np.ndarray, dtrain: xgb.DMatrix) -> [str, float]:\n",
    "#     ''' ...'''\n",
    "#     y_true = dtrain.get_label()\n",
    "#     y_pred = [1 if p >= 0.5 else 0 for p in predt]\n",
    "#     tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "#     tn_score = tn * 0\n",
    "#     tp_score = tp * 0\n",
    "#     fp_score = fp * -25\n",
    "#     fn_score = fn * -125\n",
    "#     score = tn_score + tp_score + fp_score + fn_score\n",
    "#     return 'CustomScore', float(score)\n",
    "\n",
    "# https://medium.com/analytics-vidhya/custom-metrics-for-keras-tensorflow-ae7036654e05\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# def myscoretwo(y_true, y_pred):\n",
    "# #     tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "#     tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "#     fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "#     tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "#     tn_score = tn * 0\n",
    "#     tp_score = tp * 0\n",
    "#     fp_score = fp * -25\n",
    "#     fn_score = fn * -125\n",
    "#     score = tn_score + tp_score + fp_score + fn_score\n",
    "# #     return float(score)\n",
    "#     return tp\n",
    "\n",
    "def myscoretwo(y_true, y_pred):\n",
    "    tn, fp, fn, tp = tf.math.confusion_matrix(y_true, y_pred).numpy().ravel()\n",
    "#     tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "#     fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "#     tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    tn_score = tn * 0\n",
    "    tp_score = tp * 0\n",
    "    fp_score = fp * -25\n",
    "    fn_score = fn * -125\n",
    "    score = tn_score + tp_score + fp_score + fn_score\n",
    "    return float(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change learning rate and add stopping criteria\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy', myscoretwo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-103-7b5c00741f21>:32 myscoretwo  *\n        tn, fp, fn, tp = tf.math.confusion_matrix(y_true, y_pred).numpy().ravel()\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\confusion_matrix.py:200 confusion_matrix\n        dense_shape=math_ops.cast(shape, dtypes.int64))\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:144 __init__\n        indices_shape = indices.shape.with_rank(2)\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1034 with_rank\n        raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\n\n    ValueError: Shape (100, 2, 1) must have rank 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-a477d6dabd83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-103-7b5c00741f21>:32 myscoretwo  *\n        tn, fp, fn, tp = tf.math.confusion_matrix(y_true, y_pred).numpy().ravel()\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\confusion_matrix.py:200 confusion_matrix\n        dense_shape=math_ops.cast(shape, dtypes.int64))\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:144 __init__\n        indices_shape = indices.shape.with_rank(2)\n    c:\\users\\allro\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1034 with_rank\n        raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\n\n    ValueError: Shape (100, 2, 1) must have rank 2\n"
     ]
    }
   ],
   "source": [
    "training_history = model.fit(X_train, y_train, epochs=15, validation_data=(X_test,y_test), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18607,   573],\n",
       "       [  581, 12239]], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test, batch_size=100)\n",
    "type(preds)\n",
    "preds = [1 if p >= 0.5 else 0 for p in preds]\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(preds)\n",
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18607,   573,   581, 12239])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "tf_preds = torch.from_numpy(np.array(preds))\n",
    "tf_y_test = torch.from_numpy(np.array(y_test))\n",
    "\n",
    "tf.math.confusion_matrix(tf_y_test, tf_preds).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-86950"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def willscore(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    tn_score = tn * 0\n",
    "    tp_score = tp * 0\n",
    "    fp_score = fp * -25\n",
    "    fn_score = fn * -125\n",
    "    score = tn_score + tp_score + fp_score + fn_score\n",
    "    return score\n",
    "\n",
    "willscore(y_test, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
